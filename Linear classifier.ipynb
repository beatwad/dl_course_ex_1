{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer\n",
    "import itertools  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([[-10, 0, 10]]))\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([[1000, 0, 0]]))\n",
    "assert np.isclose(probs[0, 0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5046407076661206"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([[-5, 0, 5], [-4, 0, 6]]))\n",
    "linear_classifer.cross_entropy_loss(probs, [1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([[1, 0, 0]]), [[1]])\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, [[1]]), np.array([[1, 0, 0]], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(17)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# # Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.608786\n",
      "Epoch 1, loss: 2.578784\n",
      "Epoch 2, loss: 2.550954\n",
      "Epoch 3, loss: 2.526245\n",
      "Epoch 4, loss: 2.505394\n",
      "Epoch 5, loss: 2.483122\n",
      "Epoch 6, loss: 2.467298\n",
      "Epoch 7, loss: 2.451356\n",
      "Epoch 8, loss: 2.435842\n",
      "Epoch 9, loss: 2.424889\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f548ca4d780>]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.115\n",
      "Epoch 0, loss: 2.411435\n",
      "Epoch 1, loss: 2.402383\n",
      "Epoch 2, loss: 2.392385\n",
      "Epoch 3, loss: 2.384350\n",
      "Epoch 4, loss: 2.376266\n",
      "Epoch 5, loss: 2.370571\n",
      "Epoch 6, loss: 2.363494\n",
      "Epoch 7, loss: 2.358460\n",
      "Epoch 8, loss: 2.353469\n",
      "Epoch 9, loss: 2.348225\n",
      "Epoch 10, loss: 2.343689\n",
      "Epoch 11, loss: 2.340393\n",
      "Epoch 12, loss: 2.336975\n",
      "Epoch 13, loss: 2.334680\n",
      "Epoch 14, loss: 2.330778\n",
      "Epoch 15, loss: 2.328536\n",
      "Epoch 16, loss: 2.325855\n",
      "Epoch 17, loss: 2.323803\n",
      "Epoch 18, loss: 2.321549\n",
      "Epoch 19, loss: 2.320268\n",
      "Epoch 20, loss: 2.318411\n",
      "Epoch 21, loss: 2.316635\n",
      "Epoch 22, loss: 2.314989\n",
      "Epoch 23, loss: 2.314042\n",
      "Epoch 24, loss: 2.314574\n",
      "Epoch 25, loss: 2.312237\n",
      "Epoch 26, loss: 2.310462\n",
      "Epoch 27, loss: 2.311160\n",
      "Epoch 28, loss: 2.310537\n",
      "Epoch 29, loss: 2.309730\n",
      "Epoch 30, loss: 2.309156\n",
      "Epoch 31, loss: 2.308272\n",
      "Epoch 32, loss: 2.307523\n",
      "Epoch 33, loss: 2.307544\n",
      "Epoch 34, loss: 2.307470\n",
      "Epoch 35, loss: 2.305915\n",
      "Epoch 36, loss: 2.305841\n",
      "Epoch 37, loss: 2.306043\n",
      "Epoch 38, loss: 2.305413\n",
      "Epoch 39, loss: 2.304049\n",
      "Epoch 40, loss: 2.304210\n",
      "Epoch 41, loss: 2.305435\n",
      "Epoch 42, loss: 2.304918\n",
      "Epoch 43, loss: 2.303742\n",
      "Epoch 44, loss: 2.304399\n",
      "Epoch 45, loss: 2.304424\n",
      "Epoch 46, loss: 2.304119\n",
      "Epoch 47, loss: 2.303987\n",
      "Epoch 48, loss: 2.303519\n",
      "Epoch 49, loss: 2.303328\n",
      "Epoch 50, loss: 2.303856\n",
      "Epoch 51, loss: 2.302821\n",
      "Epoch 52, loss: 2.303799\n",
      "Epoch 53, loss: 2.303374\n",
      "Epoch 54, loss: 2.302641\n",
      "Epoch 55, loss: 2.303469\n",
      "Epoch 56, loss: 2.302970\n",
      "Epoch 57, loss: 2.303289\n",
      "Epoch 58, loss: 2.302713\n",
      "Epoch 59, loss: 2.303074\n",
      "Epoch 60, loss: 2.303329\n",
      "Epoch 61, loss: 2.303090\n",
      "Epoch 62, loss: 2.302834\n",
      "Epoch 63, loss: 2.302505\n",
      "Epoch 64, loss: 2.302710\n",
      "Epoch 65, loss: 2.302430\n",
      "Epoch 66, loss: 2.303355\n",
      "Epoch 67, loss: 2.303159\n",
      "Epoch 68, loss: 2.302894\n",
      "Epoch 69, loss: 2.302487\n",
      "Epoch 70, loss: 2.302503\n",
      "Epoch 71, loss: 2.302982\n",
      "Epoch 72, loss: 2.302529\n",
      "Epoch 73, loss: 2.302272\n",
      "Epoch 74, loss: 2.302781\n",
      "Epoch 75, loss: 2.302474\n",
      "Epoch 76, loss: 2.302235\n",
      "Epoch 77, loss: 2.302601\n",
      "Epoch 78, loss: 2.302057\n",
      "Epoch 79, loss: 2.302418\n",
      "Epoch 80, loss: 2.302225\n",
      "Epoch 81, loss: 2.303092\n",
      "Epoch 82, loss: 2.302851\n",
      "Epoch 83, loss: 2.302235\n",
      "Epoch 84, loss: 2.302633\n",
      "Epoch 85, loss: 2.302329\n",
      "Epoch 86, loss: 2.302941\n",
      "Epoch 87, loss: 2.302676\n",
      "Epoch 88, loss: 2.302393\n",
      "Epoch 89, loss: 2.302554\n",
      "Epoch 90, loss: 2.302069\n",
      "Epoch 91, loss: 2.302260\n",
      "Epoch 92, loss: 2.303130\n",
      "Epoch 93, loss: 2.302801\n",
      "Epoch 94, loss: 2.302465\n",
      "Epoch 95, loss: 2.302971\n",
      "Epoch 96, loss: 2.302868\n",
      "Epoch 97, loss: 2.302421\n",
      "Epoch 98, loss: 2.302081\n",
      "Epoch 99, loss: 2.302757\n",
      "Accuracy after training for 100 epochs:  0.149\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.304183\n",
      "Epoch 1, loss: 2.302271\n",
      "Epoch 2, loss: 2.302436\n",
      "Epoch 3, loss: 2.302558\n",
      "Epoch 4, loss: 2.302484\n",
      "Epoch 5, loss: 2.301739\n",
      "Epoch 6, loss: 2.302759\n",
      "Epoch 7, loss: 2.301724\n",
      "Epoch 8, loss: 2.302420\n",
      "Epoch 9, loss: 2.303226\n",
      "Epoch 10, loss: 2.301546\n",
      "Epoch 11, loss: 2.301844\n",
      "Epoch 12, loss: 2.302825\n",
      "Epoch 13, loss: 2.301024\n",
      "Epoch 14, loss: 2.300590\n",
      "Epoch 15, loss: 2.301954\n",
      "Epoch 16, loss: 2.300839\n",
      "Epoch 17, loss: 2.301073\n",
      "Epoch 18, loss: 2.303006\n",
      "Epoch 19, loss: 2.302228\n",
      "Epoch 20, loss: 2.301842\n",
      "Epoch 21, loss: 2.300845\n",
      "Epoch 22, loss: 2.301458\n",
      "Epoch 23, loss: 2.301472\n",
      "Epoch 24, loss: 2.301259\n",
      "Epoch 25, loss: 2.300893\n",
      "Epoch 26, loss: 2.301708\n",
      "Epoch 27, loss: 2.301744\n",
      "Epoch 28, loss: 2.299288\n",
      "Epoch 29, loss: 2.302162\n",
      "Epoch 30, loss: 2.301664\n",
      "Epoch 31, loss: 2.301639\n",
      "Epoch 32, loss: 2.301528\n",
      "Epoch 33, loss: 2.302741\n",
      "Epoch 34, loss: 2.300535\n",
      "Epoch 35, loss: 2.301246\n",
      "Epoch 36, loss: 2.301235\n",
      "Epoch 37, loss: 2.302232\n",
      "Epoch 38, loss: 2.299834\n",
      "Epoch 39, loss: 2.301980\n",
      "Epoch 40, loss: 2.302995\n",
      "Epoch 41, loss: 2.297705\n",
      "Epoch 42, loss: 2.301497\n",
      "Epoch 43, loss: 2.302411\n",
      "Epoch 44, loss: 2.301648\n",
      "Epoch 45, loss: 2.300964\n",
      "Epoch 46, loss: 2.299093\n",
      "Epoch 47, loss: 2.299966\n",
      "Epoch 48, loss: 2.301436\n",
      "Epoch 49, loss: 2.300244\n",
      "Epoch 50, loss: 2.301692\n",
      "Epoch 51, loss: 2.301860\n",
      "Epoch 52, loss: 2.301243\n",
      "Epoch 53, loss: 2.301474\n",
      "Epoch 54, loss: 2.300627\n",
      "Epoch 55, loss: 2.299764\n",
      "Epoch 56, loss: 2.300899\n",
      "Epoch 57, loss: 2.301234\n",
      "Epoch 58, loss: 2.300536\n",
      "Epoch 59, loss: 2.301451\n",
      "Epoch 60, loss: 2.301596\n",
      "Epoch 61, loss: 2.300633\n",
      "Epoch 62, loss: 2.299459\n",
      "Epoch 63, loss: 2.299432\n",
      "Epoch 64, loss: 2.299665\n",
      "Epoch 65, loss: 2.299394\n",
      "Epoch 66, loss: 2.299467\n",
      "Epoch 67, loss: 2.300448\n",
      "Epoch 68, loss: 2.299647\n",
      "Epoch 69, loss: 2.299744\n",
      "Epoch 70, loss: 2.300490\n",
      "Epoch 71, loss: 2.299543\n",
      "Epoch 72, loss: 2.299528\n",
      "Epoch 73, loss: 2.299719\n",
      "Epoch 74, loss: 2.300096\n",
      "Epoch 75, loss: 2.299557\n",
      "Epoch 76, loss: 2.299864\n",
      "Epoch 77, loss: 2.300248\n",
      "Epoch 78, loss: 2.298672\n",
      "Epoch 79, loss: 2.299229\n",
      "Epoch 80, loss: 2.300639\n",
      "Epoch 81, loss: 2.299400\n",
      "Epoch 82, loss: 2.298854\n",
      "Epoch 83, loss: 2.298023\n",
      "Epoch 84, loss: 2.300390\n",
      "Epoch 85, loss: 2.298821\n",
      "Epoch 86, loss: 2.299259\n",
      "Epoch 87, loss: 2.299081\n",
      "Epoch 88, loss: 2.298189\n",
      "Epoch 89, loss: 2.299035\n",
      "Epoch 90, loss: 2.298762\n",
      "Epoch 91, loss: 2.298428\n",
      "Epoch 92, loss: 2.300473\n",
      "Epoch 93, loss: 2.298607\n",
      "Epoch 94, loss: 2.297876\n",
      "Epoch 95, loss: 2.299125\n",
      "Epoch 96, loss: 2.299133\n",
      "Epoch 97, loss: 2.298664\n",
      "Epoch 98, loss: 2.298037\n",
      "Epoch 99, loss: 2.297684\n",
      "Epoch 100, loss: 2.298788\n",
      "Epoch 101, loss: 2.298370\n",
      "Epoch 102, loss: 2.298055\n",
      "Epoch 103, loss: 2.298795\n",
      "Epoch 104, loss: 2.297613\n",
      "Epoch 105, loss: 2.297908\n",
      "Epoch 106, loss: 2.296935\n",
      "Epoch 107, loss: 2.297613\n",
      "Epoch 108, loss: 2.296787\n",
      "Epoch 109, loss: 2.298063\n",
      "Epoch 110, loss: 2.298646\n",
      "Epoch 111, loss: 2.296495\n",
      "Epoch 112, loss: 2.298498\n",
      "Epoch 113, loss: 2.298471\n",
      "Epoch 114, loss: 2.298843\n",
      "Epoch 115, loss: 2.298860\n",
      "Epoch 116, loss: 2.298842\n",
      "Epoch 117, loss: 2.297409\n",
      "Epoch 118, loss: 2.297156\n",
      "Epoch 119, loss: 2.298955\n",
      "Epoch 120, loss: 2.299156\n",
      "Epoch 121, loss: 2.297144\n",
      "Epoch 122, loss: 2.296772\n",
      "Epoch 123, loss: 2.296718\n",
      "Epoch 124, loss: 2.297329\n",
      "Epoch 125, loss: 2.298145\n",
      "Epoch 126, loss: 2.298534\n",
      "Epoch 127, loss: 2.297108\n",
      "Epoch 128, loss: 2.296853\n",
      "Epoch 129, loss: 2.297558\n",
      "Epoch 130, loss: 2.297115\n",
      "Epoch 131, loss: 2.298406\n",
      "Epoch 132, loss: 2.296255\n",
      "Epoch 133, loss: 2.296933\n",
      "Epoch 134, loss: 2.297461\n",
      "Epoch 135, loss: 2.298114\n",
      "Epoch 136, loss: 2.297873\n",
      "Epoch 137, loss: 2.296987\n",
      "Epoch 138, loss: 2.297497\n",
      "Epoch 139, loss: 2.296631\n",
      "Epoch 140, loss: 2.296276\n",
      "Epoch 141, loss: 2.297518\n",
      "Epoch 142, loss: 2.297090\n",
      "Epoch 143, loss: 2.295767\n",
      "Epoch 144, loss: 2.296061\n",
      "Epoch 145, loss: 2.295850\n",
      "Epoch 146, loss: 2.296182\n",
      "Epoch 147, loss: 2.296658\n",
      "Epoch 148, loss: 2.296866\n",
      "Epoch 149, loss: 2.296525\n",
      "Epoch 150, loss: 2.295796\n",
      "Epoch 151, loss: 2.295694\n",
      "Epoch 152, loss: 2.295652\n",
      "Epoch 153, loss: 2.296268\n",
      "Epoch 154, loss: 2.296299\n",
      "Epoch 155, loss: 2.296305\n",
      "Epoch 156, loss: 2.296750\n",
      "Epoch 157, loss: 2.296859\n",
      "Epoch 158, loss: 2.296357\n",
      "Epoch 159, loss: 2.294727\n",
      "Epoch 160, loss: 2.296786\n",
      "Epoch 161, loss: 2.295388\n",
      "Epoch 162, loss: 2.297890\n",
      "Epoch 163, loss: 2.297297\n",
      "Epoch 164, loss: 2.295957\n",
      "Epoch 165, loss: 2.295387\n",
      "Epoch 166, loss: 2.295195\n",
      "Epoch 167, loss: 2.295884\n",
      "Epoch 168, loss: 2.296545\n",
      "Epoch 169, loss: 2.295329\n",
      "Epoch 170, loss: 2.294036\n",
      "Epoch 171, loss: 2.296464\n",
      "Epoch 172, loss: 2.295213\n",
      "Epoch 173, loss: 2.297636\n",
      "Epoch 174, loss: 2.295242\n",
      "Epoch 175, loss: 2.294509\n",
      "Epoch 176, loss: 2.296389\n",
      "Epoch 177, loss: 2.294844\n",
      "Epoch 178, loss: 2.296428\n",
      "Epoch 179, loss: 2.295664\n",
      "Epoch 180, loss: 2.294366\n",
      "Epoch 181, loss: 2.295476\n",
      "Epoch 182, loss: 2.293842\n",
      "Epoch 183, loss: 2.297204\n",
      "Epoch 184, loss: 2.295076\n",
      "Epoch 185, loss: 2.295080\n",
      "Epoch 186, loss: 2.295785\n",
      "Epoch 187, loss: 2.293877\n",
      "Epoch 188, loss: 2.294925\n",
      "Epoch 189, loss: 2.295308\n",
      "Epoch 190, loss: 2.294513\n",
      "Epoch 191, loss: 2.292809\n",
      "Epoch 192, loss: 2.295760\n",
      "Epoch 193, loss: 2.293865\n",
      "Epoch 194, loss: 2.294559\n",
      "Epoch 195, loss: 2.294543\n",
      "Epoch 196, loss: 2.293452\n",
      "Epoch 197, loss: 2.294099\n",
      "Epoch 198, loss: 2.294372\n",
      "Epoch 199, loss: 2.296033\n",
      "Epoch 200, loss: 2.295307\n",
      "Epoch 201, loss: 2.295930\n",
      "Epoch 202, loss: 2.293535\n",
      "Epoch 203, loss: 2.294832\n",
      "Epoch 204, loss: 2.295538\n",
      "Epoch 205, loss: 2.295182\n",
      "Epoch 206, loss: 2.294195\n",
      "Epoch 207, loss: 2.295044\n",
      "Epoch 208, loss: 2.294750\n",
      "Epoch 209, loss: 2.295102\n",
      "Epoch 210, loss: 2.294005\n",
      "Epoch 211, loss: 2.292920\n",
      "Epoch 212, loss: 2.294147\n",
      "Epoch 213, loss: 2.293863\n",
      "Epoch 214, loss: 2.293523\n",
      "Epoch 215, loss: 2.293937\n",
      "Epoch 216, loss: 2.294685\n",
      "Epoch 217, loss: 2.294206\n",
      "Epoch 218, loss: 2.293635\n",
      "Epoch 219, loss: 2.294662\n",
      "Epoch 220, loss: 2.294772\n",
      "Epoch 221, loss: 2.294254\n",
      "Epoch 222, loss: 2.291902\n",
      "Epoch 223, loss: 2.293526\n",
      "Epoch 224, loss: 2.292798\n",
      "Epoch 225, loss: 2.292725\n",
      "Epoch 226, loss: 2.292844\n",
      "Epoch 227, loss: 2.291234\n",
      "Epoch 228, loss: 2.293890\n",
      "Epoch 229, loss: 2.291708\n",
      "Epoch 230, loss: 2.294013\n",
      "Epoch 231, loss: 2.293585\n",
      "Epoch 232, loss: 2.293107\n",
      "Epoch 233, loss: 2.294290\n",
      "Epoch 234, loss: 2.293165\n",
      "Epoch 235, loss: 2.292130\n",
      "Epoch 236, loss: 2.293526\n",
      "Epoch 237, loss: 2.291256\n",
      "Epoch 238, loss: 2.292490\n",
      "Epoch 239, loss: 2.290938\n",
      "Epoch 240, loss: 2.292843\n",
      "Epoch 241, loss: 2.292909\n",
      "Epoch 242, loss: 2.292958\n",
      "Epoch 243, loss: 2.295013\n",
      "Epoch 244, loss: 2.292500\n",
      "Epoch 245, loss: 2.293862\n",
      "Epoch 246, loss: 2.291295\n",
      "Epoch 247, loss: 2.291567\n",
      "Epoch 248, loss: 2.293237\n",
      "Epoch 249, loss: 2.293936\n",
      "Epoch 250, loss: 2.289458\n",
      "Epoch 251, loss: 2.295022\n",
      "Epoch 252, loss: 2.294983\n",
      "Epoch 253, loss: 2.293377\n",
      "Epoch 254, loss: 2.291059\n",
      "Epoch 255, loss: 2.295279\n",
      "Epoch 256, loss: 2.290957\n",
      "Epoch 257, loss: 2.292626\n",
      "Epoch 258, loss: 2.294045\n",
      "Epoch 259, loss: 2.292291\n",
      "Epoch 260, loss: 2.294835\n",
      "Epoch 261, loss: 2.292745\n",
      "Epoch 262, loss: 2.292397\n",
      "Epoch 263, loss: 2.291902\n",
      "Epoch 264, loss: 2.292784\n",
      "Epoch 265, loss: 2.291826\n",
      "Epoch 266, loss: 2.291229\n",
      "Epoch 267, loss: 2.291145\n",
      "Epoch 268, loss: 2.290129\n",
      "Epoch 269, loss: 2.292881\n",
      "Epoch 270, loss: 2.291375\n",
      "Epoch 271, loss: 2.290379\n",
      "Epoch 272, loss: 2.291839\n",
      "Epoch 273, loss: 2.292621\n",
      "Epoch 274, loss: 2.291327\n",
      "Epoch 275, loss: 2.292773\n",
      "Epoch 276, loss: 2.291752\n",
      "Epoch 277, loss: 2.289632\n",
      "Epoch 278, loss: 2.292651\n",
      "Epoch 279, loss: 2.291936\n",
      "Epoch 280, loss: 2.290779\n",
      "Epoch 281, loss: 2.291950\n",
      "Epoch 282, loss: 2.290842\n",
      "Epoch 283, loss: 2.292512\n",
      "Epoch 284, loss: 2.292830\n",
      "Epoch 285, loss: 2.292673\n",
      "Epoch 286, loss: 2.291912\n",
      "Epoch 287, loss: 2.288221\n",
      "Epoch 288, loss: 2.291398\n",
      "Epoch 289, loss: 2.291956\n",
      "Epoch 290, loss: 2.293372\n",
      "Epoch 291, loss: 2.291021\n",
      "Epoch 292, loss: 2.290570\n",
      "Epoch 293, loss: 2.290366\n",
      "Epoch 294, loss: 2.289123\n",
      "Epoch 295, loss: 2.290061\n",
      "Epoch 296, loss: 2.292004\n",
      "Epoch 297, loss: 2.291348\n",
      "Epoch 298, loss: 2.291702\n",
      "Epoch 299, loss: 2.289948\n",
      "Epoch 0, loss: 2.304180\n",
      "Epoch 1, loss: 2.302268\n",
      "Epoch 2, loss: 2.302434\n",
      "Epoch 3, loss: 2.302555\n",
      "Epoch 4, loss: 2.302481\n",
      "Epoch 5, loss: 2.301737\n",
      "Epoch 6, loss: 2.302756\n",
      "Epoch 7, loss: 2.301721\n",
      "Epoch 8, loss: 2.302417\n",
      "Epoch 9, loss: 2.303223\n",
      "Epoch 10, loss: 2.301543\n",
      "Epoch 11, loss: 2.301841\n",
      "Epoch 12, loss: 2.302822\n",
      "Epoch 13, loss: 2.301021\n",
      "Epoch 14, loss: 2.300587\n",
      "Epoch 15, loss: 2.301951\n",
      "Epoch 16, loss: 2.300837\n",
      "Epoch 17, loss: 2.301070\n",
      "Epoch 18, loss: 2.303003\n",
      "Epoch 19, loss: 2.302226\n",
      "Epoch 20, loss: 2.301840\n",
      "Epoch 21, loss: 2.300842\n",
      "Epoch 22, loss: 2.301455\n",
      "Epoch 23, loss: 2.301469\n",
      "Epoch 24, loss: 2.301256\n",
      "Epoch 25, loss: 2.300891\n",
      "Epoch 26, loss: 2.301705\n",
      "Epoch 27, loss: 2.301741\n",
      "Epoch 28, loss: 2.299285\n",
      "Epoch 29, loss: 2.302159\n",
      "Epoch 30, loss: 2.301661\n",
      "Epoch 31, loss: 2.301637\n",
      "Epoch 32, loss: 2.301525\n",
      "Epoch 33, loss: 2.302738\n",
      "Epoch 34, loss: 2.300532\n",
      "Epoch 35, loss: 2.301243\n",
      "Epoch 36, loss: 2.301232\n",
      "Epoch 37, loss: 2.302229\n",
      "Epoch 38, loss: 2.299832\n",
      "Epoch 39, loss: 2.301977\n",
      "Epoch 40, loss: 2.302993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, loss: 2.297702\n",
      "Epoch 42, loss: 2.301494\n",
      "Epoch 43, loss: 2.302408\n",
      "Epoch 44, loss: 2.301646\n",
      "Epoch 45, loss: 2.300961\n",
      "Epoch 46, loss: 2.299090\n",
      "Epoch 47, loss: 2.299964\n",
      "Epoch 48, loss: 2.301434\n",
      "Epoch 49, loss: 2.300241\n",
      "Epoch 50, loss: 2.301689\n",
      "Epoch 51, loss: 2.301857\n",
      "Epoch 52, loss: 2.301240\n",
      "Epoch 53, loss: 2.301471\n",
      "Epoch 54, loss: 2.300624\n",
      "Epoch 55, loss: 2.299761\n",
      "Epoch 56, loss: 2.300896\n",
      "Epoch 57, loss: 2.301231\n",
      "Epoch 58, loss: 2.300533\n",
      "Epoch 59, loss: 2.301448\n",
      "Epoch 60, loss: 2.301593\n",
      "Epoch 61, loss: 2.300631\n",
      "Epoch 62, loss: 2.299456\n",
      "Epoch 63, loss: 2.299430\n",
      "Epoch 64, loss: 2.299662\n",
      "Epoch 65, loss: 2.299391\n",
      "Epoch 66, loss: 2.299465\n",
      "Epoch 67, loss: 2.300445\n",
      "Epoch 68, loss: 2.299644\n",
      "Epoch 69, loss: 2.299741\n",
      "Epoch 70, loss: 2.300487\n",
      "Epoch 71, loss: 2.299540\n",
      "Epoch 72, loss: 2.299525\n",
      "Epoch 73, loss: 2.299716\n",
      "Epoch 74, loss: 2.300093\n",
      "Epoch 75, loss: 2.299554\n",
      "Epoch 76, loss: 2.299861\n",
      "Epoch 77, loss: 2.300245\n",
      "Epoch 78, loss: 2.298669\n",
      "Epoch 79, loss: 2.299227\n",
      "Epoch 80, loss: 2.300636\n",
      "Epoch 81, loss: 2.299397\n",
      "Epoch 82, loss: 2.298851\n",
      "Epoch 83, loss: 2.298020\n",
      "Epoch 84, loss: 2.300387\n",
      "Epoch 85, loss: 2.298819\n",
      "Epoch 86, loss: 2.299256\n",
      "Epoch 87, loss: 2.299078\n",
      "Epoch 88, loss: 2.298186\n",
      "Epoch 89, loss: 2.299032\n",
      "Epoch 90, loss: 2.298759\n",
      "Epoch 91, loss: 2.298425\n",
      "Epoch 92, loss: 2.300471\n",
      "Epoch 93, loss: 2.298604\n",
      "Epoch 94, loss: 2.297873\n",
      "Epoch 95, loss: 2.299122\n",
      "Epoch 96, loss: 2.299130\n",
      "Epoch 97, loss: 2.298661\n",
      "Epoch 98, loss: 2.298034\n",
      "Epoch 99, loss: 2.297681\n",
      "Epoch 100, loss: 2.298785\n",
      "Epoch 101, loss: 2.298367\n",
      "Epoch 102, loss: 2.298052\n",
      "Epoch 103, loss: 2.298792\n",
      "Epoch 104, loss: 2.297611\n",
      "Epoch 105, loss: 2.297905\n",
      "Epoch 106, loss: 2.296932\n",
      "Epoch 107, loss: 2.297610\n",
      "Epoch 108, loss: 2.296784\n",
      "Epoch 109, loss: 2.298060\n",
      "Epoch 110, loss: 2.298643\n",
      "Epoch 111, loss: 2.296492\n",
      "Epoch 112, loss: 2.298495\n",
      "Epoch 113, loss: 2.298468\n",
      "Epoch 114, loss: 2.298840\n",
      "Epoch 115, loss: 2.298857\n",
      "Epoch 116, loss: 2.298839\n",
      "Epoch 117, loss: 2.297406\n",
      "Epoch 118, loss: 2.297153\n",
      "Epoch 119, loss: 2.298952\n",
      "Epoch 120, loss: 2.299153\n",
      "Epoch 121, loss: 2.297141\n",
      "Epoch 122, loss: 2.296769\n",
      "Epoch 123, loss: 2.296715\n",
      "Epoch 124, loss: 2.297326\n",
      "Epoch 125, loss: 2.298142\n",
      "Epoch 126, loss: 2.298531\n",
      "Epoch 127, loss: 2.297105\n",
      "Epoch 128, loss: 2.296850\n",
      "Epoch 129, loss: 2.297555\n",
      "Epoch 130, loss: 2.297112\n",
      "Epoch 131, loss: 2.298403\n",
      "Epoch 132, loss: 2.296251\n",
      "Epoch 133, loss: 2.296930\n",
      "Epoch 134, loss: 2.297458\n",
      "Epoch 135, loss: 2.298111\n",
      "Epoch 136, loss: 2.297870\n",
      "Epoch 137, loss: 2.296983\n",
      "Epoch 138, loss: 2.297494\n",
      "Epoch 139, loss: 2.296628\n",
      "Epoch 140, loss: 2.296273\n",
      "Epoch 141, loss: 2.297515\n",
      "Epoch 142, loss: 2.297087\n",
      "Epoch 143, loss: 2.295764\n",
      "Epoch 144, loss: 2.296058\n",
      "Epoch 145, loss: 2.295846\n",
      "Epoch 146, loss: 2.296179\n",
      "Epoch 147, loss: 2.296655\n",
      "Epoch 148, loss: 2.296863\n",
      "Epoch 149, loss: 2.296522\n",
      "Epoch 150, loss: 2.295793\n",
      "Epoch 151, loss: 2.295691\n",
      "Epoch 152, loss: 2.295649\n",
      "Epoch 153, loss: 2.296264\n",
      "Epoch 154, loss: 2.296296\n",
      "Epoch 155, loss: 2.296302\n",
      "Epoch 156, loss: 2.296747\n",
      "Epoch 157, loss: 2.296856\n",
      "Epoch 158, loss: 2.296353\n",
      "Epoch 159, loss: 2.294723\n",
      "Epoch 160, loss: 2.296783\n",
      "Epoch 161, loss: 2.295385\n",
      "Epoch 162, loss: 2.297886\n",
      "Epoch 163, loss: 2.297294\n",
      "Epoch 164, loss: 2.295954\n",
      "Epoch 165, loss: 2.295383\n",
      "Epoch 166, loss: 2.295192\n",
      "Epoch 167, loss: 2.295881\n",
      "Epoch 168, loss: 2.296542\n",
      "Epoch 169, loss: 2.295325\n",
      "Epoch 170, loss: 2.294033\n",
      "Epoch 171, loss: 2.296461\n",
      "Epoch 172, loss: 2.295210\n",
      "Epoch 173, loss: 2.297633\n",
      "Epoch 174, loss: 2.295239\n",
      "Epoch 175, loss: 2.294506\n",
      "Epoch 176, loss: 2.296386\n",
      "Epoch 177, loss: 2.294841\n",
      "Epoch 178, loss: 2.296425\n",
      "Epoch 179, loss: 2.295661\n",
      "Epoch 180, loss: 2.294363\n",
      "Epoch 181, loss: 2.295472\n",
      "Epoch 182, loss: 2.293838\n",
      "Epoch 183, loss: 2.297200\n",
      "Epoch 184, loss: 2.295072\n",
      "Epoch 185, loss: 2.295076\n",
      "Epoch 186, loss: 2.295782\n",
      "Epoch 187, loss: 2.293873\n",
      "Epoch 188, loss: 2.294922\n",
      "Epoch 189, loss: 2.295305\n",
      "Epoch 190, loss: 2.294509\n",
      "Epoch 191, loss: 2.292805\n",
      "Epoch 192, loss: 2.295757\n",
      "Epoch 193, loss: 2.293862\n",
      "Epoch 194, loss: 2.294556\n",
      "Epoch 195, loss: 2.294540\n",
      "Epoch 196, loss: 2.293448\n",
      "Epoch 197, loss: 2.294096\n",
      "Epoch 198, loss: 2.294368\n",
      "Epoch 199, loss: 2.296029\n",
      "Epoch 200, loss: 2.295304\n",
      "Epoch 201, loss: 2.295926\n",
      "Epoch 202, loss: 2.293531\n",
      "Epoch 203, loss: 2.294828\n",
      "Epoch 204, loss: 2.295535\n",
      "Epoch 205, loss: 2.295179\n",
      "Epoch 206, loss: 2.294191\n",
      "Epoch 207, loss: 2.295040\n",
      "Epoch 208, loss: 2.294747\n",
      "Epoch 209, loss: 2.295099\n",
      "Epoch 210, loss: 2.294001\n",
      "Epoch 211, loss: 2.292916\n",
      "Epoch 212, loss: 2.294144\n",
      "Epoch 213, loss: 2.293860\n",
      "Epoch 214, loss: 2.293520\n",
      "Epoch 215, loss: 2.293934\n",
      "Epoch 216, loss: 2.294681\n",
      "Epoch 217, loss: 2.294203\n",
      "Epoch 218, loss: 2.293631\n",
      "Epoch 219, loss: 2.294658\n",
      "Epoch 220, loss: 2.294769\n",
      "Epoch 221, loss: 2.294251\n",
      "Epoch 222, loss: 2.291898\n",
      "Epoch 223, loss: 2.293522\n",
      "Epoch 224, loss: 2.292794\n",
      "Epoch 225, loss: 2.292721\n",
      "Epoch 226, loss: 2.292841\n",
      "Epoch 227, loss: 2.291231\n",
      "Epoch 228, loss: 2.293887\n",
      "Epoch 229, loss: 2.291704\n",
      "Epoch 230, loss: 2.294010\n",
      "Epoch 231, loss: 2.293581\n",
      "Epoch 232, loss: 2.293103\n",
      "Epoch 233, loss: 2.294287\n",
      "Epoch 234, loss: 2.293161\n",
      "Epoch 235, loss: 2.292127\n",
      "Epoch 236, loss: 2.293522\n",
      "Epoch 237, loss: 2.291252\n",
      "Epoch 238, loss: 2.292486\n",
      "Epoch 239, loss: 2.290934\n",
      "Epoch 240, loss: 2.292839\n",
      "Epoch 241, loss: 2.292905\n",
      "Epoch 242, loss: 2.292955\n",
      "Epoch 243, loss: 2.295009\n",
      "Epoch 244, loss: 2.292496\n",
      "Epoch 245, loss: 2.293859\n",
      "Epoch 246, loss: 2.291291\n",
      "Epoch 247, loss: 2.291563\n",
      "Epoch 248, loss: 2.293234\n",
      "Epoch 249, loss: 2.293932\n",
      "Epoch 250, loss: 2.289454\n",
      "Epoch 251, loss: 2.295018\n",
      "Epoch 252, loss: 2.294979\n",
      "Epoch 253, loss: 2.293373\n",
      "Epoch 254, loss: 2.291055\n",
      "Epoch 255, loss: 2.295275\n",
      "Epoch 256, loss: 2.290953\n",
      "Epoch 257, loss: 2.292622\n",
      "Epoch 258, loss: 2.294041\n",
      "Epoch 259, loss: 2.292287\n",
      "Epoch 260, loss: 2.294831\n",
      "Epoch 261, loss: 2.292741\n",
      "Epoch 262, loss: 2.292394\n",
      "Epoch 263, loss: 2.291898\n",
      "Epoch 264, loss: 2.292780\n",
      "Epoch 265, loss: 2.291822\n",
      "Epoch 266, loss: 2.291225\n",
      "Epoch 267, loss: 2.291141\n",
      "Epoch 268, loss: 2.290125\n",
      "Epoch 269, loss: 2.292877\n",
      "Epoch 270, loss: 2.291371\n",
      "Epoch 271, loss: 2.290375\n",
      "Epoch 272, loss: 2.291835\n",
      "Epoch 273, loss: 2.292617\n",
      "Epoch 274, loss: 2.291323\n",
      "Epoch 275, loss: 2.292769\n",
      "Epoch 276, loss: 2.291748\n",
      "Epoch 277, loss: 2.289628\n",
      "Epoch 278, loss: 2.292647\n",
      "Epoch 279, loss: 2.291932\n",
      "Epoch 280, loss: 2.290775\n",
      "Epoch 281, loss: 2.291946\n",
      "Epoch 282, loss: 2.290838\n",
      "Epoch 283, loss: 2.292508\n",
      "Epoch 284, loss: 2.292826\n",
      "Epoch 285, loss: 2.292669\n",
      "Epoch 286, loss: 2.291908\n",
      "Epoch 287, loss: 2.288217\n",
      "Epoch 288, loss: 2.291394\n",
      "Epoch 289, loss: 2.291952\n",
      "Epoch 290, loss: 2.293368\n",
      "Epoch 291, loss: 2.291017\n",
      "Epoch 292, loss: 2.290566\n",
      "Epoch 293, loss: 2.290362\n",
      "Epoch 294, loss: 2.289118\n",
      "Epoch 295, loss: 2.290057\n",
      "Epoch 296, loss: 2.292000\n",
      "Epoch 297, loss: 2.291344\n",
      "Epoch 298, loss: 2.291698\n",
      "Epoch 299, loss: 2.289943\n",
      "Epoch 0, loss: 2.304180\n",
      "Epoch 1, loss: 2.302268\n",
      "Epoch 2, loss: 2.302433\n",
      "Epoch 3, loss: 2.302555\n",
      "Epoch 4, loss: 2.302481\n",
      "Epoch 5, loss: 2.301736\n",
      "Epoch 6, loss: 2.302756\n",
      "Epoch 7, loss: 2.301721\n",
      "Epoch 8, loss: 2.302417\n",
      "Epoch 9, loss: 2.303223\n",
      "Epoch 10, loss: 2.301543\n",
      "Epoch 11, loss: 2.301841\n",
      "Epoch 12, loss: 2.302822\n",
      "Epoch 13, loss: 2.301021\n",
      "Epoch 14, loss: 2.300587\n",
      "Epoch 15, loss: 2.301951\n",
      "Epoch 16, loss: 2.300836\n",
      "Epoch 17, loss: 2.301070\n",
      "Epoch 18, loss: 2.303002\n",
      "Epoch 19, loss: 2.302225\n",
      "Epoch 20, loss: 2.301839\n",
      "Epoch 21, loss: 2.300842\n",
      "Epoch 22, loss: 2.301455\n",
      "Epoch 23, loss: 2.301469\n",
      "Epoch 24, loss: 2.301255\n",
      "Epoch 25, loss: 2.300890\n",
      "Epoch 26, loss: 2.301704\n",
      "Epoch 27, loss: 2.301741\n",
      "Epoch 28, loss: 2.299285\n",
      "Epoch 29, loss: 2.302159\n",
      "Epoch 30, loss: 2.301661\n",
      "Epoch 31, loss: 2.301636\n",
      "Epoch 32, loss: 2.301525\n",
      "Epoch 33, loss: 2.302738\n",
      "Epoch 34, loss: 2.300532\n",
      "Epoch 35, loss: 2.301243\n",
      "Epoch 36, loss: 2.301232\n",
      "Epoch 37, loss: 2.302229\n",
      "Epoch 38, loss: 2.299831\n",
      "Epoch 39, loss: 2.301977\n",
      "Epoch 40, loss: 2.302992\n",
      "Epoch 41, loss: 2.297702\n",
      "Epoch 42, loss: 2.301494\n",
      "Epoch 43, loss: 2.302408\n",
      "Epoch 44, loss: 2.301645\n",
      "Epoch 45, loss: 2.300961\n",
      "Epoch 46, loss: 2.299090\n",
      "Epoch 47, loss: 2.299963\n",
      "Epoch 48, loss: 2.301433\n",
      "Epoch 49, loss: 2.300241\n",
      "Epoch 50, loss: 2.301689\n",
      "Epoch 51, loss: 2.301857\n",
      "Epoch 52, loss: 2.301240\n",
      "Epoch 53, loss: 2.301471\n",
      "Epoch 54, loss: 2.300624\n",
      "Epoch 55, loss: 2.299761\n",
      "Epoch 56, loss: 2.300896\n",
      "Epoch 57, loss: 2.301231\n",
      "Epoch 58, loss: 2.300533\n",
      "Epoch 59, loss: 2.301448\n",
      "Epoch 60, loss: 2.301592\n",
      "Epoch 61, loss: 2.300630\n",
      "Epoch 62, loss: 2.299456\n",
      "Epoch 63, loss: 2.299429\n",
      "Epoch 64, loss: 2.299662\n",
      "Epoch 65, loss: 2.299391\n",
      "Epoch 66, loss: 2.299464\n",
      "Epoch 67, loss: 2.300445\n",
      "Epoch 68, loss: 2.299644\n",
      "Epoch 69, loss: 2.299741\n",
      "Epoch 70, loss: 2.300486\n",
      "Epoch 71, loss: 2.299540\n",
      "Epoch 72, loss: 2.299525\n",
      "Epoch 73, loss: 2.299716\n",
      "Epoch 74, loss: 2.300093\n",
      "Epoch 75, loss: 2.299554\n",
      "Epoch 76, loss: 2.299861\n",
      "Epoch 77, loss: 2.300245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78, loss: 2.298669\n",
      "Epoch 79, loss: 2.299226\n",
      "Epoch 80, loss: 2.300636\n",
      "Epoch 81, loss: 2.299397\n",
      "Epoch 82, loss: 2.298851\n",
      "Epoch 83, loss: 2.298020\n",
      "Epoch 84, loss: 2.300387\n",
      "Epoch 85, loss: 2.298818\n",
      "Epoch 86, loss: 2.299256\n",
      "Epoch 87, loss: 2.299078\n",
      "Epoch 88, loss: 2.298186\n",
      "Epoch 89, loss: 2.299032\n",
      "Epoch 90, loss: 2.298759\n",
      "Epoch 91, loss: 2.298424\n",
      "Epoch 92, loss: 2.300470\n",
      "Epoch 93, loss: 2.298604\n",
      "Epoch 94, loss: 2.297873\n",
      "Epoch 95, loss: 2.299122\n",
      "Epoch 96, loss: 2.299129\n",
      "Epoch 97, loss: 2.298661\n",
      "Epoch 98, loss: 2.298033\n",
      "Epoch 99, loss: 2.297681\n",
      "Epoch 100, loss: 2.298784\n",
      "Epoch 101, loss: 2.298367\n",
      "Epoch 102, loss: 2.298052\n",
      "Epoch 103, loss: 2.298792\n",
      "Epoch 104, loss: 2.297610\n",
      "Epoch 105, loss: 2.297905\n",
      "Epoch 106, loss: 2.296932\n",
      "Epoch 107, loss: 2.297610\n",
      "Epoch 108, loss: 2.296783\n",
      "Epoch 109, loss: 2.298059\n",
      "Epoch 110, loss: 2.298643\n",
      "Epoch 111, loss: 2.296491\n",
      "Epoch 112, loss: 2.298495\n",
      "Epoch 113, loss: 2.298468\n",
      "Epoch 114, loss: 2.298840\n",
      "Epoch 115, loss: 2.298857\n",
      "Epoch 116, loss: 2.298839\n",
      "Epoch 117, loss: 2.297406\n",
      "Epoch 118, loss: 2.297153\n",
      "Epoch 119, loss: 2.298952\n",
      "Epoch 120, loss: 2.299152\n",
      "Epoch 121, loss: 2.297140\n",
      "Epoch 122, loss: 2.296768\n",
      "Epoch 123, loss: 2.296715\n",
      "Epoch 124, loss: 2.297325\n",
      "Epoch 125, loss: 2.298142\n",
      "Epoch 126, loss: 2.298531\n",
      "Epoch 127, loss: 2.297105\n",
      "Epoch 128, loss: 2.296850\n",
      "Epoch 129, loss: 2.297555\n",
      "Epoch 130, loss: 2.297112\n",
      "Epoch 131, loss: 2.298403\n",
      "Epoch 132, loss: 2.296251\n",
      "Epoch 133, loss: 2.296929\n",
      "Epoch 134, loss: 2.297457\n",
      "Epoch 135, loss: 2.298111\n",
      "Epoch 136, loss: 2.297869\n",
      "Epoch 137, loss: 2.296983\n",
      "Epoch 138, loss: 2.297493\n",
      "Epoch 139, loss: 2.296628\n",
      "Epoch 140, loss: 2.296272\n",
      "Epoch 141, loss: 2.297515\n",
      "Epoch 142, loss: 2.297086\n",
      "Epoch 143, loss: 2.295764\n",
      "Epoch 144, loss: 2.296058\n",
      "Epoch 145, loss: 2.295846\n",
      "Epoch 146, loss: 2.296179\n",
      "Epoch 147, loss: 2.296655\n",
      "Epoch 148, loss: 2.296862\n",
      "Epoch 149, loss: 2.296521\n",
      "Epoch 150, loss: 2.295793\n",
      "Epoch 151, loss: 2.295691\n",
      "Epoch 152, loss: 2.295649\n",
      "Epoch 153, loss: 2.296264\n",
      "Epoch 154, loss: 2.296296\n",
      "Epoch 155, loss: 2.296301\n",
      "Epoch 156, loss: 2.296747\n",
      "Epoch 157, loss: 2.296855\n",
      "Epoch 158, loss: 2.296353\n",
      "Epoch 159, loss: 2.294723\n",
      "Epoch 160, loss: 2.296782\n",
      "Epoch 161, loss: 2.295384\n",
      "Epoch 162, loss: 2.297886\n",
      "Epoch 163, loss: 2.297294\n",
      "Epoch 164, loss: 2.295954\n",
      "Epoch 165, loss: 2.295383\n",
      "Epoch 166, loss: 2.295192\n",
      "Epoch 167, loss: 2.295881\n",
      "Epoch 168, loss: 2.296541\n",
      "Epoch 169, loss: 2.295325\n",
      "Epoch 170, loss: 2.294033\n",
      "Epoch 171, loss: 2.296461\n",
      "Epoch 172, loss: 2.295210\n",
      "Epoch 173, loss: 2.297633\n",
      "Epoch 174, loss: 2.295239\n",
      "Epoch 175, loss: 2.294506\n",
      "Epoch 176, loss: 2.296385\n",
      "Epoch 177, loss: 2.294840\n",
      "Epoch 178, loss: 2.296425\n",
      "Epoch 179, loss: 2.295660\n",
      "Epoch 180, loss: 2.294363\n",
      "Epoch 181, loss: 2.295472\n",
      "Epoch 182, loss: 2.293838\n",
      "Epoch 183, loss: 2.297200\n",
      "Epoch 184, loss: 2.295072\n",
      "Epoch 185, loss: 2.295076\n",
      "Epoch 186, loss: 2.295781\n",
      "Epoch 187, loss: 2.293873\n",
      "Epoch 188, loss: 2.294921\n",
      "Epoch 189, loss: 2.295305\n",
      "Epoch 190, loss: 2.294509\n",
      "Epoch 191, loss: 2.292805\n",
      "Epoch 192, loss: 2.295757\n",
      "Epoch 193, loss: 2.293862\n",
      "Epoch 194, loss: 2.294555\n",
      "Epoch 195, loss: 2.294539\n",
      "Epoch 196, loss: 2.293448\n",
      "Epoch 197, loss: 2.294095\n",
      "Epoch 198, loss: 2.294368\n",
      "Epoch 199, loss: 2.296029\n",
      "Epoch 200, loss: 2.295303\n",
      "Epoch 201, loss: 2.295926\n",
      "Epoch 202, loss: 2.293531\n",
      "Epoch 203, loss: 2.294828\n",
      "Epoch 204, loss: 2.295534\n",
      "Epoch 205, loss: 2.295179\n",
      "Epoch 206, loss: 2.294191\n",
      "Epoch 207, loss: 2.295040\n",
      "Epoch 208, loss: 2.294746\n",
      "Epoch 209, loss: 2.295098\n",
      "Epoch 210, loss: 2.294001\n",
      "Epoch 211, loss: 2.292916\n",
      "Epoch 212, loss: 2.294143\n",
      "Epoch 213, loss: 2.293860\n",
      "Epoch 214, loss: 2.293519\n",
      "Epoch 215, loss: 2.293933\n",
      "Epoch 216, loss: 2.294681\n",
      "Epoch 217, loss: 2.294202\n",
      "Epoch 218, loss: 2.293631\n",
      "Epoch 219, loss: 2.294658\n",
      "Epoch 220, loss: 2.294768\n",
      "Epoch 221, loss: 2.294250\n",
      "Epoch 222, loss: 2.291898\n",
      "Epoch 223, loss: 2.293522\n",
      "Epoch 224, loss: 2.292794\n",
      "Epoch 225, loss: 2.292721\n",
      "Epoch 226, loss: 2.292840\n",
      "Epoch 227, loss: 2.291230\n",
      "Epoch 228, loss: 2.293886\n",
      "Epoch 229, loss: 2.291704\n",
      "Epoch 230, loss: 2.294009\n",
      "Epoch 231, loss: 2.293581\n",
      "Epoch 232, loss: 2.293103\n",
      "Epoch 233, loss: 2.294286\n",
      "Epoch 234, loss: 2.293161\n",
      "Epoch 235, loss: 2.292126\n",
      "Epoch 236, loss: 2.293522\n",
      "Epoch 237, loss: 2.291252\n",
      "Epoch 238, loss: 2.292486\n",
      "Epoch 239, loss: 2.290934\n",
      "Epoch 240, loss: 2.292839\n",
      "Epoch 241, loss: 2.292905\n",
      "Epoch 242, loss: 2.292954\n",
      "Epoch 243, loss: 2.295009\n",
      "Epoch 244, loss: 2.292496\n",
      "Epoch 245, loss: 2.293858\n",
      "Epoch 246, loss: 2.291291\n",
      "Epoch 247, loss: 2.291562\n",
      "Epoch 248, loss: 2.293233\n",
      "Epoch 249, loss: 2.293932\n",
      "Epoch 250, loss: 2.289454\n",
      "Epoch 251, loss: 2.295018\n",
      "Epoch 252, loss: 2.294979\n",
      "Epoch 253, loss: 2.293373\n",
      "Epoch 254, loss: 2.291055\n",
      "Epoch 255, loss: 2.295275\n",
      "Epoch 256, loss: 2.290953\n",
      "Epoch 257, loss: 2.292621\n",
      "Epoch 258, loss: 2.294040\n",
      "Epoch 259, loss: 2.292287\n",
      "Epoch 260, loss: 2.294830\n",
      "Epoch 261, loss: 2.292741\n",
      "Epoch 262, loss: 2.292393\n",
      "Epoch 263, loss: 2.291898\n",
      "Epoch 264, loss: 2.292779\n",
      "Epoch 265, loss: 2.291821\n",
      "Epoch 266, loss: 2.291224\n",
      "Epoch 267, loss: 2.291140\n",
      "Epoch 268, loss: 2.290125\n",
      "Epoch 269, loss: 2.292877\n",
      "Epoch 270, loss: 2.291371\n",
      "Epoch 271, loss: 2.290375\n",
      "Epoch 272, loss: 2.291834\n",
      "Epoch 273, loss: 2.292616\n",
      "Epoch 274, loss: 2.291322\n",
      "Epoch 275, loss: 2.292768\n",
      "Epoch 276, loss: 2.291747\n",
      "Epoch 277, loss: 2.289627\n",
      "Epoch 278, loss: 2.292647\n",
      "Epoch 279, loss: 2.291931\n",
      "Epoch 280, loss: 2.290775\n",
      "Epoch 281, loss: 2.291946\n",
      "Epoch 282, loss: 2.290838\n",
      "Epoch 283, loss: 2.292507\n",
      "Epoch 284, loss: 2.292826\n",
      "Epoch 285, loss: 2.292668\n",
      "Epoch 286, loss: 2.291907\n",
      "Epoch 287, loss: 2.288216\n",
      "Epoch 288, loss: 2.291394\n",
      "Epoch 289, loss: 2.291952\n",
      "Epoch 290, loss: 2.293367\n",
      "Epoch 291, loss: 2.291016\n",
      "Epoch 292, loss: 2.290566\n",
      "Epoch 293, loss: 2.290362\n",
      "Epoch 294, loss: 2.289118\n",
      "Epoch 295, loss: 2.290057\n",
      "Epoch 296, loss: 2.291999\n",
      "Epoch 297, loss: 2.291343\n",
      "Epoch 298, loss: 2.291697\n",
      "Epoch 299, loss: 2.289943\n",
      "Epoch 0, loss: 2.304183\n",
      "Epoch 1, loss: 2.302548\n",
      "Epoch 2, loss: 2.302381\n",
      "Epoch 3, loss: 2.302486\n",
      "Epoch 4, loss: 2.302795\n",
      "Epoch 5, loss: 2.301952\n",
      "Epoch 6, loss: 2.303881\n",
      "Epoch 7, loss: 2.302815\n",
      "Epoch 8, loss: 2.303042\n",
      "Epoch 9, loss: 2.302708\n",
      "Epoch 10, loss: 2.302757\n",
      "Epoch 11, loss: 2.301877\n",
      "Epoch 12, loss: 2.302305\n",
      "Epoch 13, loss: 2.302861\n",
      "Epoch 14, loss: 2.303043\n",
      "Epoch 15, loss: 2.302223\n",
      "Epoch 16, loss: 2.302591\n",
      "Epoch 17, loss: 2.302828\n",
      "Epoch 18, loss: 2.302818\n",
      "Epoch 19, loss: 2.302300\n",
      "Epoch 20, loss: 2.302423\n",
      "Epoch 21, loss: 2.302295\n",
      "Epoch 22, loss: 2.302042\n",
      "Epoch 23, loss: 2.301671\n",
      "Epoch 24, loss: 2.301922\n",
      "Epoch 25, loss: 2.302792\n",
      "Epoch 26, loss: 2.302807\n",
      "Epoch 27, loss: 2.302894\n",
      "Epoch 28, loss: 2.302374\n",
      "Epoch 29, loss: 2.301476\n",
      "Epoch 30, loss: 2.302386\n",
      "Epoch 31, loss: 2.302344\n",
      "Epoch 32, loss: 2.302279\n",
      "Epoch 33, loss: 2.302350\n",
      "Epoch 34, loss: 2.302021\n",
      "Epoch 35, loss: 2.302854\n",
      "Epoch 36, loss: 2.302255\n",
      "Epoch 37, loss: 2.302890\n",
      "Epoch 38, loss: 2.302371\n",
      "Epoch 39, loss: 2.303278\n",
      "Epoch 40, loss: 2.302646\n",
      "Epoch 41, loss: 2.301325\n",
      "Epoch 42, loss: 2.303069\n",
      "Epoch 43, loss: 2.302919\n",
      "Epoch 44, loss: 2.302355\n",
      "Epoch 45, loss: 2.301610\n",
      "Epoch 46, loss: 2.302366\n",
      "Epoch 47, loss: 2.302647\n",
      "Epoch 48, loss: 2.302770\n",
      "Epoch 49, loss: 2.302550\n",
      "Epoch 50, loss: 2.302411\n",
      "Epoch 51, loss: 2.303050\n",
      "Epoch 52, loss: 2.302705\n",
      "Epoch 53, loss: 2.301607\n",
      "Epoch 54, loss: 2.301520\n",
      "Epoch 55, loss: 2.301752\n",
      "Epoch 56, loss: 2.302437\n",
      "Epoch 57, loss: 2.302426\n",
      "Epoch 58, loss: 2.302078\n",
      "Epoch 59, loss: 2.303164\n",
      "Epoch 60, loss: 2.303842\n",
      "Epoch 61, loss: 2.302561\n",
      "Epoch 62, loss: 2.302359\n",
      "Epoch 63, loss: 2.302086\n",
      "Epoch 64, loss: 2.301675\n",
      "Epoch 65, loss: 2.301929\n",
      "Epoch 66, loss: 2.301573\n",
      "Epoch 67, loss: 2.302176\n",
      "Epoch 68, loss: 2.302552\n",
      "Epoch 69, loss: 2.302527\n",
      "Epoch 70, loss: 2.301725\n",
      "Epoch 71, loss: 2.302133\n",
      "Epoch 72, loss: 2.302507\n",
      "Epoch 73, loss: 2.302533\n",
      "Epoch 74, loss: 2.302677\n",
      "Epoch 75, loss: 2.301914\n",
      "Epoch 76, loss: 2.301964\n",
      "Epoch 77, loss: 2.302870\n",
      "Epoch 78, loss: 2.301841\n",
      "Epoch 79, loss: 2.302350\n",
      "Epoch 80, loss: 2.302880\n",
      "Epoch 81, loss: 2.302243\n",
      "Epoch 82, loss: 2.301768\n",
      "Epoch 83, loss: 2.301372\n",
      "Epoch 84, loss: 2.302784\n",
      "Epoch 85, loss: 2.301716\n",
      "Epoch 86, loss: 2.302387\n",
      "Epoch 87, loss: 2.301763\n",
      "Epoch 88, loss: 2.301796\n",
      "Epoch 89, loss: 2.302779\n",
      "Epoch 90, loss: 2.302011\n",
      "Epoch 91, loss: 2.301986\n",
      "Epoch 92, loss: 2.302931\n",
      "Epoch 93, loss: 2.302679\n",
      "Epoch 94, loss: 2.301791\n",
      "Epoch 95, loss: 2.301949\n",
      "Epoch 96, loss: 2.301485\n",
      "Epoch 97, loss: 2.301615\n",
      "Epoch 98, loss: 2.302076\n",
      "Epoch 99, loss: 2.302136\n",
      "Epoch 100, loss: 2.302307\n",
      "Epoch 101, loss: 2.302251\n",
      "Epoch 102, loss: 2.302584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103, loss: 2.303234\n",
      "Epoch 104, loss: 2.302031\n",
      "Epoch 105, loss: 2.301533\n",
      "Epoch 106, loss: 2.301999\n",
      "Epoch 107, loss: 2.301510\n",
      "Epoch 108, loss: 2.301599\n",
      "Epoch 109, loss: 2.301820\n",
      "Epoch 110, loss: 2.302059\n",
      "Epoch 111, loss: 2.301609\n",
      "Epoch 112, loss: 2.302297\n",
      "Epoch 113, loss: 2.302430\n",
      "Epoch 114, loss: 2.301386\n",
      "Epoch 115, loss: 2.302298\n",
      "Epoch 116, loss: 2.302734\n",
      "Epoch 117, loss: 2.301538\n",
      "Epoch 118, loss: 2.302359\n",
      "Epoch 119, loss: 2.302616\n",
      "Epoch 120, loss: 2.302482\n",
      "Epoch 121, loss: 2.301276\n",
      "Epoch 122, loss: 2.301261\n",
      "Epoch 123, loss: 2.301974\n",
      "Epoch 124, loss: 2.301881\n",
      "Epoch 125, loss: 2.302801\n",
      "Epoch 126, loss: 2.301701\n",
      "Epoch 127, loss: 2.301994\n",
      "Epoch 128, loss: 2.301747\n",
      "Epoch 129, loss: 2.301705\n",
      "Epoch 130, loss: 2.301873\n",
      "Epoch 131, loss: 2.302815\n",
      "Epoch 132, loss: 2.302004\n",
      "Epoch 133, loss: 2.302730\n",
      "Epoch 134, loss: 2.301992\n",
      "Epoch 135, loss: 2.302302\n",
      "Epoch 136, loss: 2.301528\n",
      "Epoch 137, loss: 2.302239\n",
      "Epoch 138, loss: 2.301375\n",
      "Epoch 139, loss: 2.302044\n",
      "Epoch 140, loss: 2.301035\n",
      "Epoch 141, loss: 2.301670\n",
      "Epoch 142, loss: 2.302067\n",
      "Epoch 143, loss: 2.301345\n",
      "Epoch 144, loss: 2.301673\n",
      "Epoch 145, loss: 2.301524\n",
      "Epoch 146, loss: 2.301663\n",
      "Epoch 147, loss: 2.302629\n",
      "Epoch 148, loss: 2.301835\n",
      "Epoch 149, loss: 2.302427\n",
      "Epoch 150, loss: 2.301982\n",
      "Epoch 151, loss: 2.301107\n",
      "Epoch 152, loss: 2.301966\n",
      "Epoch 153, loss: 2.302164\n",
      "Epoch 154, loss: 2.301957\n",
      "Epoch 155, loss: 2.301707\n",
      "Epoch 156, loss: 2.302029\n",
      "Epoch 157, loss: 2.302034\n",
      "Epoch 158, loss: 2.301691\n",
      "Epoch 159, loss: 2.301378\n",
      "Epoch 160, loss: 2.302428\n",
      "Epoch 161, loss: 2.302339\n",
      "Epoch 162, loss: 2.302501\n",
      "Epoch 163, loss: 2.302341\n",
      "Epoch 164, loss: 2.301371\n",
      "Epoch 165, loss: 2.301620\n",
      "Epoch 166, loss: 2.302155\n",
      "Epoch 167, loss: 2.301755\n",
      "Epoch 168, loss: 2.301960\n",
      "Epoch 169, loss: 2.301465\n",
      "Epoch 170, loss: 2.301119\n",
      "Epoch 171, loss: 2.302019\n",
      "Epoch 172, loss: 2.301882\n",
      "Epoch 173, loss: 2.301788\n",
      "Epoch 174, loss: 2.302271\n",
      "Epoch 175, loss: 2.302191\n",
      "Epoch 176, loss: 2.301601\n",
      "Epoch 177, loss: 2.301869\n",
      "Epoch 178, loss: 2.301892\n",
      "Epoch 179, loss: 2.302458\n",
      "Epoch 180, loss: 2.301738\n",
      "Epoch 181, loss: 2.302200\n",
      "Epoch 182, loss: 2.302066\n",
      "Epoch 183, loss: 2.302043\n",
      "Epoch 184, loss: 2.302106\n",
      "Epoch 185, loss: 2.301726\n",
      "Epoch 186, loss: 2.300729\n",
      "Epoch 187, loss: 2.301697\n",
      "Epoch 188, loss: 2.301555\n",
      "Epoch 189, loss: 2.301807\n",
      "Epoch 190, loss: 2.301711\n",
      "Epoch 191, loss: 2.301346\n",
      "Epoch 192, loss: 2.301423\n",
      "Epoch 193, loss: 2.301843\n",
      "Epoch 194, loss: 2.301906\n",
      "Epoch 195, loss: 2.300605\n",
      "Epoch 196, loss: 2.301961\n",
      "Epoch 197, loss: 2.301554\n",
      "Epoch 198, loss: 2.301603\n",
      "Epoch 199, loss: 2.301989\n",
      "Epoch 200, loss: 2.302216\n",
      "Epoch 201, loss: 2.301639\n",
      "Epoch 202, loss: 2.301473\n",
      "Epoch 203, loss: 2.301351\n",
      "Epoch 204, loss: 2.302026\n",
      "Epoch 205, loss: 2.301598\n",
      "Epoch 206, loss: 2.301572\n",
      "Epoch 207, loss: 2.301496\n",
      "Epoch 208, loss: 2.302138\n",
      "Epoch 209, loss: 2.302296\n",
      "Epoch 210, loss: 2.301561\n",
      "Epoch 211, loss: 2.301852\n",
      "Epoch 212, loss: 2.301738\n",
      "Epoch 213, loss: 2.301725\n",
      "Epoch 214, loss: 2.301423\n",
      "Epoch 215, loss: 2.301425\n",
      "Epoch 216, loss: 2.301734\n",
      "Epoch 217, loss: 2.301343\n",
      "Epoch 218, loss: 2.301455\n",
      "Epoch 219, loss: 2.302166\n",
      "Epoch 220, loss: 2.301743\n",
      "Epoch 221, loss: 2.301458\n",
      "Epoch 222, loss: 2.300279\n",
      "Epoch 223, loss: 2.301439\n",
      "Epoch 224, loss: 2.301504\n",
      "Epoch 225, loss: 2.301231\n",
      "Epoch 226, loss: 2.301159\n",
      "Epoch 227, loss: 2.300696\n",
      "Epoch 228, loss: 2.300878\n",
      "Epoch 229, loss: 2.300852\n",
      "Epoch 230, loss: 2.301912\n",
      "Epoch 231, loss: 2.301385\n",
      "Epoch 232, loss: 2.301629\n",
      "Epoch 233, loss: 2.301866\n",
      "Epoch 234, loss: 2.301477\n",
      "Epoch 235, loss: 2.301583\n",
      "Epoch 236, loss: 2.301562\n",
      "Epoch 237, loss: 2.300879\n",
      "Epoch 238, loss: 2.301448\n",
      "Epoch 239, loss: 2.300804\n",
      "Epoch 240, loss: 2.301583\n",
      "Epoch 241, loss: 2.301758\n",
      "Epoch 242, loss: 2.301652\n",
      "Epoch 243, loss: 2.301199\n",
      "Epoch 244, loss: 2.301392\n",
      "Epoch 245, loss: 2.301772\n",
      "Epoch 246, loss: 2.301504\n",
      "Epoch 247, loss: 2.301076\n",
      "Epoch 248, loss: 2.301296\n",
      "Epoch 249, loss: 2.302423\n",
      "Epoch 250, loss: 2.300746\n",
      "Epoch 251, loss: 2.302160\n",
      "Epoch 252, loss: 2.302617\n",
      "Epoch 253, loss: 2.302153\n",
      "Epoch 254, loss: 2.300924\n",
      "Epoch 255, loss: 2.302298\n",
      "Epoch 256, loss: 2.301293\n",
      "Epoch 257, loss: 2.301754\n",
      "Epoch 258, loss: 2.301628\n",
      "Epoch 259, loss: 2.301593\n",
      "Epoch 260, loss: 2.301979\n",
      "Epoch 261, loss: 2.301968\n",
      "Epoch 262, loss: 2.301772\n",
      "Epoch 263, loss: 2.300935\n",
      "Epoch 264, loss: 2.301569\n",
      "Epoch 265, loss: 2.301109\n",
      "Epoch 266, loss: 2.300909\n",
      "Epoch 267, loss: 2.301741\n",
      "Epoch 268, loss: 2.301479\n",
      "Epoch 269, loss: 2.301951\n",
      "Epoch 270, loss: 2.300699\n",
      "Epoch 271, loss: 2.300704\n",
      "Epoch 272, loss: 2.302095\n",
      "Epoch 273, loss: 2.301520\n",
      "Epoch 274, loss: 2.300917\n",
      "Epoch 275, loss: 2.301785\n",
      "Epoch 276, loss: 2.302187\n",
      "Epoch 277, loss: 2.300823\n",
      "Epoch 278, loss: 2.301665\n",
      "Epoch 279, loss: 2.301086\n",
      "Epoch 280, loss: 2.301646\n",
      "Epoch 281, loss: 2.301532\n",
      "Epoch 282, loss: 2.301699\n",
      "Epoch 283, loss: 2.300477\n",
      "Epoch 284, loss: 2.301606\n",
      "Epoch 285, loss: 2.301087\n",
      "Epoch 286, loss: 2.300920\n",
      "Epoch 287, loss: 2.300024\n",
      "Epoch 288, loss: 2.300942\n",
      "Epoch 289, loss: 2.301553\n",
      "Epoch 290, loss: 2.301678\n",
      "Epoch 291, loss: 2.301096\n",
      "Epoch 292, loss: 2.301179\n",
      "Epoch 293, loss: 2.301076\n",
      "Epoch 294, loss: 2.300587\n",
      "Epoch 295, loss: 2.301010\n",
      "Epoch 296, loss: 2.300920\n",
      "Epoch 297, loss: 2.301728\n",
      "Epoch 298, loss: 2.301184\n",
      "Epoch 299, loss: 2.300946\n",
      "Epoch 0, loss: 2.304180\n",
      "Epoch 1, loss: 2.302545\n",
      "Epoch 2, loss: 2.302378\n",
      "Epoch 3, loss: 2.302483\n",
      "Epoch 4, loss: 2.302792\n",
      "Epoch 5, loss: 2.301949\n",
      "Epoch 6, loss: 2.303878\n",
      "Epoch 7, loss: 2.302812\n",
      "Epoch 8, loss: 2.303039\n",
      "Epoch 9, loss: 2.302705\n",
      "Epoch 10, loss: 2.302755\n",
      "Epoch 11, loss: 2.301874\n",
      "Epoch 12, loss: 2.302303\n",
      "Epoch 13, loss: 2.302858\n",
      "Epoch 14, loss: 2.303040\n",
      "Epoch 15, loss: 2.302220\n",
      "Epoch 16, loss: 2.302588\n",
      "Epoch 17, loss: 2.302826\n",
      "Epoch 18, loss: 2.302815\n",
      "Epoch 19, loss: 2.302297\n",
      "Epoch 20, loss: 2.302420\n",
      "Epoch 21, loss: 2.302292\n",
      "Epoch 22, loss: 2.302039\n",
      "Epoch 23, loss: 2.301668\n",
      "Epoch 24, loss: 2.301919\n",
      "Epoch 25, loss: 2.302789\n",
      "Epoch 26, loss: 2.302805\n",
      "Epoch 27, loss: 2.302891\n",
      "Epoch 28, loss: 2.302372\n",
      "Epoch 29, loss: 2.301473\n",
      "Epoch 30, loss: 2.302383\n",
      "Epoch 31, loss: 2.302341\n",
      "Epoch 32, loss: 2.302276\n",
      "Epoch 33, loss: 2.302348\n",
      "Epoch 34, loss: 2.302018\n",
      "Epoch 35, loss: 2.302851\n",
      "Epoch 36, loss: 2.302252\n",
      "Epoch 37, loss: 2.302887\n",
      "Epoch 38, loss: 2.302368\n",
      "Epoch 39, loss: 2.303275\n",
      "Epoch 40, loss: 2.302644\n",
      "Epoch 41, loss: 2.301322\n",
      "Epoch 42, loss: 2.303066\n",
      "Epoch 43, loss: 2.302916\n",
      "Epoch 44, loss: 2.302353\n",
      "Epoch 45, loss: 2.301608\n",
      "Epoch 46, loss: 2.302364\n",
      "Epoch 47, loss: 2.302644\n",
      "Epoch 48, loss: 2.302767\n",
      "Epoch 49, loss: 2.302548\n",
      "Epoch 50, loss: 2.302408\n",
      "Epoch 51, loss: 2.303047\n",
      "Epoch 52, loss: 2.302703\n",
      "Epoch 53, loss: 2.301604\n",
      "Epoch 54, loss: 2.301517\n",
      "Epoch 55, loss: 2.301749\n",
      "Epoch 56, loss: 2.302434\n",
      "Epoch 57, loss: 2.302423\n",
      "Epoch 58, loss: 2.302075\n",
      "Epoch 59, loss: 2.303161\n",
      "Epoch 60, loss: 2.303839\n",
      "Epoch 61, loss: 2.302558\n",
      "Epoch 62, loss: 2.302356\n",
      "Epoch 63, loss: 2.302083\n",
      "Epoch 64, loss: 2.301672\n",
      "Epoch 65, loss: 2.301927\n",
      "Epoch 66, loss: 2.301571\n",
      "Epoch 67, loss: 2.302173\n",
      "Epoch 68, loss: 2.302550\n",
      "Epoch 69, loss: 2.302524\n",
      "Epoch 70, loss: 2.301722\n",
      "Epoch 71, loss: 2.302131\n",
      "Epoch 72, loss: 2.302504\n",
      "Epoch 73, loss: 2.302530\n",
      "Epoch 74, loss: 2.302674\n",
      "Epoch 75, loss: 2.301911\n",
      "Epoch 76, loss: 2.301961\n",
      "Epoch 77, loss: 2.302867\n",
      "Epoch 78, loss: 2.301838\n",
      "Epoch 79, loss: 2.302347\n",
      "Epoch 80, loss: 2.302877\n",
      "Epoch 81, loss: 2.302240\n",
      "Epoch 82, loss: 2.301765\n",
      "Epoch 83, loss: 2.301369\n",
      "Epoch 84, loss: 2.302782\n",
      "Epoch 85, loss: 2.301713\n",
      "Epoch 86, loss: 2.302384\n",
      "Epoch 87, loss: 2.301760\n",
      "Epoch 88, loss: 2.301793\n",
      "Epoch 89, loss: 2.302776\n",
      "Epoch 90, loss: 2.302009\n",
      "Epoch 91, loss: 2.301984\n",
      "Epoch 92, loss: 2.302928\n",
      "Epoch 93, loss: 2.302677\n",
      "Epoch 94, loss: 2.301788\n",
      "Epoch 95, loss: 2.301947\n",
      "Epoch 96, loss: 2.301482\n",
      "Epoch 97, loss: 2.301613\n",
      "Epoch 98, loss: 2.302074\n",
      "Epoch 99, loss: 2.302133\n",
      "Epoch 100, loss: 2.302304\n",
      "Epoch 101, loss: 2.302249\n",
      "Epoch 102, loss: 2.302581\n",
      "Epoch 103, loss: 2.303231\n",
      "Epoch 104, loss: 2.302028\n",
      "Epoch 105, loss: 2.301530\n",
      "Epoch 106, loss: 2.301996\n",
      "Epoch 107, loss: 2.301507\n",
      "Epoch 108, loss: 2.301597\n",
      "Epoch 109, loss: 2.301817\n",
      "Epoch 110, loss: 2.302056\n",
      "Epoch 111, loss: 2.301606\n",
      "Epoch 112, loss: 2.302294\n",
      "Epoch 113, loss: 2.302428\n",
      "Epoch 114, loss: 2.301383\n",
      "Epoch 115, loss: 2.302295\n",
      "Epoch 116, loss: 2.302731\n",
      "Epoch 117, loss: 2.301535\n",
      "Epoch 118, loss: 2.302356\n",
      "Epoch 119, loss: 2.302613\n",
      "Epoch 120, loss: 2.302479\n",
      "Epoch 121, loss: 2.301273\n",
      "Epoch 122, loss: 2.301259\n",
      "Epoch 123, loss: 2.301971\n",
      "Epoch 124, loss: 2.301878\n",
      "Epoch 125, loss: 2.302798\n",
      "Epoch 126, loss: 2.301698\n",
      "Epoch 127, loss: 2.301992\n",
      "Epoch 128, loss: 2.301744\n",
      "Epoch 129, loss: 2.301702\n",
      "Epoch 130, loss: 2.301871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131, loss: 2.302813\n",
      "Epoch 132, loss: 2.302001\n",
      "Epoch 133, loss: 2.302727\n",
      "Epoch 134, loss: 2.301989\n",
      "Epoch 135, loss: 2.302299\n",
      "Epoch 136, loss: 2.301526\n",
      "Epoch 137, loss: 2.302236\n",
      "Epoch 138, loss: 2.301372\n",
      "Epoch 139, loss: 2.302041\n",
      "Epoch 140, loss: 2.301032\n",
      "Epoch 141, loss: 2.301668\n",
      "Epoch 142, loss: 2.302065\n",
      "Epoch 143, loss: 2.301343\n",
      "Epoch 144, loss: 2.301670\n",
      "Epoch 145, loss: 2.301522\n",
      "Epoch 146, loss: 2.301661\n",
      "Epoch 147, loss: 2.302626\n",
      "Epoch 148, loss: 2.301832\n",
      "Epoch 149, loss: 2.302424\n",
      "Epoch 150, loss: 2.301979\n",
      "Epoch 151, loss: 2.301104\n",
      "Epoch 152, loss: 2.301963\n",
      "Epoch 153, loss: 2.302161\n",
      "Epoch 154, loss: 2.301954\n",
      "Epoch 155, loss: 2.301704\n",
      "Epoch 156, loss: 2.302026\n",
      "Epoch 157, loss: 2.302031\n",
      "Epoch 158, loss: 2.301688\n",
      "Epoch 159, loss: 2.301376\n",
      "Epoch 160, loss: 2.302425\n",
      "Epoch 161, loss: 2.302336\n",
      "Epoch 162, loss: 2.302498\n",
      "Epoch 163, loss: 2.302338\n",
      "Epoch 164, loss: 2.301369\n",
      "Epoch 165, loss: 2.301617\n",
      "Epoch 166, loss: 2.302152\n",
      "Epoch 167, loss: 2.301752\n",
      "Epoch 168, loss: 2.301958\n",
      "Epoch 169, loss: 2.301462\n",
      "Epoch 170, loss: 2.301116\n",
      "Epoch 171, loss: 2.302016\n",
      "Epoch 172, loss: 2.301879\n",
      "Epoch 173, loss: 2.301785\n",
      "Epoch 174, loss: 2.302268\n",
      "Epoch 175, loss: 2.302188\n",
      "Epoch 176, loss: 2.301598\n",
      "Epoch 177, loss: 2.301866\n",
      "Epoch 178, loss: 2.301889\n",
      "Epoch 179, loss: 2.302456\n",
      "Epoch 180, loss: 2.301735\n",
      "Epoch 181, loss: 2.302197\n",
      "Epoch 182, loss: 2.302063\n",
      "Epoch 183, loss: 2.302040\n",
      "Epoch 184, loss: 2.302103\n",
      "Epoch 185, loss: 2.301723\n",
      "Epoch 186, loss: 2.300726\n",
      "Epoch 187, loss: 2.301694\n",
      "Epoch 188, loss: 2.301552\n",
      "Epoch 189, loss: 2.301804\n",
      "Epoch 190, loss: 2.301708\n",
      "Epoch 191, loss: 2.301343\n",
      "Epoch 192, loss: 2.301420\n",
      "Epoch 193, loss: 2.301841\n",
      "Epoch 194, loss: 2.301903\n",
      "Epoch 195, loss: 2.300602\n",
      "Epoch 196, loss: 2.301958\n",
      "Epoch 197, loss: 2.301551\n",
      "Epoch 198, loss: 2.301600\n",
      "Epoch 199, loss: 2.301986\n",
      "Epoch 200, loss: 2.302213\n",
      "Epoch 201, loss: 2.301636\n",
      "Epoch 202, loss: 2.301470\n",
      "Epoch 203, loss: 2.301348\n",
      "Epoch 204, loss: 2.302024\n",
      "Epoch 205, loss: 2.301595\n",
      "Epoch 206, loss: 2.301569\n",
      "Epoch 207, loss: 2.301493\n",
      "Epoch 208, loss: 2.302135\n",
      "Epoch 209, loss: 2.302293\n",
      "Epoch 210, loss: 2.301558\n",
      "Epoch 211, loss: 2.301849\n",
      "Epoch 212, loss: 2.301735\n",
      "Epoch 213, loss: 2.301723\n",
      "Epoch 214, loss: 2.301421\n",
      "Epoch 215, loss: 2.301422\n",
      "Epoch 216, loss: 2.301731\n",
      "Epoch 217, loss: 2.301340\n",
      "Epoch 218, loss: 2.301452\n",
      "Epoch 219, loss: 2.302163\n",
      "Epoch 220, loss: 2.301740\n",
      "Epoch 221, loss: 2.301456\n",
      "Epoch 222, loss: 2.300276\n",
      "Epoch 223, loss: 2.301436\n",
      "Epoch 224, loss: 2.301501\n",
      "Epoch 225, loss: 2.301228\n",
      "Epoch 226, loss: 2.301156\n",
      "Epoch 227, loss: 2.300693\n",
      "Epoch 228, loss: 2.300875\n",
      "Epoch 229, loss: 2.300849\n",
      "Epoch 230, loss: 2.301910\n",
      "Epoch 231, loss: 2.301382\n",
      "Epoch 232, loss: 2.301626\n",
      "Epoch 233, loss: 2.301863\n",
      "Epoch 234, loss: 2.301474\n",
      "Epoch 235, loss: 2.301580\n",
      "Epoch 236, loss: 2.301559\n",
      "Epoch 237, loss: 2.300876\n",
      "Epoch 238, loss: 2.301445\n",
      "Epoch 239, loss: 2.300801\n",
      "Epoch 240, loss: 2.301581\n",
      "Epoch 241, loss: 2.301755\n",
      "Epoch 242, loss: 2.301649\n",
      "Epoch 243, loss: 2.301197\n",
      "Epoch 244, loss: 2.301390\n",
      "Epoch 245, loss: 2.301770\n",
      "Epoch 246, loss: 2.301502\n",
      "Epoch 247, loss: 2.301074\n",
      "Epoch 248, loss: 2.301293\n",
      "Epoch 249, loss: 2.302420\n",
      "Epoch 250, loss: 2.300743\n",
      "Epoch 251, loss: 2.302157\n",
      "Epoch 252, loss: 2.302614\n",
      "Epoch 253, loss: 2.302150\n",
      "Epoch 254, loss: 2.300921\n",
      "Epoch 255, loss: 2.302296\n",
      "Epoch 256, loss: 2.301291\n",
      "Epoch 257, loss: 2.301751\n",
      "Epoch 258, loss: 2.301625\n",
      "Epoch 259, loss: 2.301590\n",
      "Epoch 260, loss: 2.301976\n",
      "Epoch 261, loss: 2.301965\n",
      "Epoch 262, loss: 2.301769\n",
      "Epoch 263, loss: 2.300932\n",
      "Epoch 264, loss: 2.301566\n",
      "Epoch 265, loss: 2.301106\n",
      "Epoch 266, loss: 2.300906\n",
      "Epoch 267, loss: 2.301738\n",
      "Epoch 268, loss: 2.301476\n",
      "Epoch 269, loss: 2.301948\n",
      "Epoch 270, loss: 2.300696\n",
      "Epoch 271, loss: 2.300701\n",
      "Epoch 272, loss: 2.302093\n",
      "Epoch 273, loss: 2.301517\n",
      "Epoch 274, loss: 2.300914\n",
      "Epoch 275, loss: 2.301782\n",
      "Epoch 276, loss: 2.302184\n",
      "Epoch 277, loss: 2.300821\n",
      "Epoch 278, loss: 2.301663\n",
      "Epoch 279, loss: 2.301084\n",
      "Epoch 280, loss: 2.301643\n",
      "Epoch 281, loss: 2.301529\n",
      "Epoch 282, loss: 2.301696\n",
      "Epoch 283, loss: 2.300474\n",
      "Epoch 284, loss: 2.301603\n",
      "Epoch 285, loss: 2.301084\n",
      "Epoch 286, loss: 2.300917\n",
      "Epoch 287, loss: 2.300021\n",
      "Epoch 288, loss: 2.300940\n",
      "Epoch 289, loss: 2.301550\n",
      "Epoch 290, loss: 2.301675\n",
      "Epoch 291, loss: 2.301094\n",
      "Epoch 292, loss: 2.301176\n",
      "Epoch 293, loss: 2.301073\n",
      "Epoch 294, loss: 2.300584\n",
      "Epoch 295, loss: 2.301007\n",
      "Epoch 296, loss: 2.300917\n",
      "Epoch 297, loss: 2.301725\n",
      "Epoch 298, loss: 2.301181\n",
      "Epoch 299, loss: 2.300943\n",
      "Epoch 0, loss: 2.304180\n",
      "Epoch 1, loss: 2.302545\n",
      "Epoch 2, loss: 2.302378\n",
      "Epoch 3, loss: 2.302483\n",
      "Epoch 4, loss: 2.302792\n",
      "Epoch 5, loss: 2.301949\n",
      "Epoch 6, loss: 2.303878\n",
      "Epoch 7, loss: 2.302812\n",
      "Epoch 8, loss: 2.303039\n",
      "Epoch 9, loss: 2.302705\n",
      "Epoch 10, loss: 2.302754\n",
      "Epoch 11, loss: 2.301874\n",
      "Epoch 12, loss: 2.302302\n",
      "Epoch 13, loss: 2.302857\n",
      "Epoch 14, loss: 2.303040\n",
      "Epoch 15, loss: 2.302220\n",
      "Epoch 16, loss: 2.302588\n",
      "Epoch 17, loss: 2.302825\n",
      "Epoch 18, loss: 2.302815\n",
      "Epoch 19, loss: 2.302297\n",
      "Epoch 20, loss: 2.302420\n",
      "Epoch 21, loss: 2.302292\n",
      "Epoch 22, loss: 2.302039\n",
      "Epoch 23, loss: 2.301668\n",
      "Epoch 24, loss: 2.301919\n",
      "Epoch 25, loss: 2.302789\n",
      "Epoch 26, loss: 2.302804\n",
      "Epoch 27, loss: 2.302891\n",
      "Epoch 28, loss: 2.302371\n",
      "Epoch 29, loss: 2.301472\n",
      "Epoch 30, loss: 2.302382\n",
      "Epoch 31, loss: 2.302341\n",
      "Epoch 32, loss: 2.302276\n",
      "Epoch 33, loss: 2.302347\n",
      "Epoch 34, loss: 2.302018\n",
      "Epoch 35, loss: 2.302851\n",
      "Epoch 36, loss: 2.302252\n",
      "Epoch 37, loss: 2.302887\n",
      "Epoch 38, loss: 2.302368\n",
      "Epoch 39, loss: 2.303274\n",
      "Epoch 40, loss: 2.302643\n",
      "Epoch 41, loss: 2.301322\n",
      "Epoch 42, loss: 2.303066\n",
      "Epoch 43, loss: 2.302916\n",
      "Epoch 44, loss: 2.302352\n",
      "Epoch 45, loss: 2.301607\n",
      "Epoch 46, loss: 2.302363\n",
      "Epoch 47, loss: 2.302644\n",
      "Epoch 48, loss: 2.302767\n",
      "Epoch 49, loss: 2.302547\n",
      "Epoch 50, loss: 2.302408\n",
      "Epoch 51, loss: 2.303047\n",
      "Epoch 52, loss: 2.302702\n",
      "Epoch 53, loss: 2.301604\n",
      "Epoch 54, loss: 2.301517\n",
      "Epoch 55, loss: 2.301749\n",
      "Epoch 56, loss: 2.302434\n",
      "Epoch 57, loss: 2.302423\n",
      "Epoch 58, loss: 2.302075\n",
      "Epoch 59, loss: 2.303161\n",
      "Epoch 60, loss: 2.303839\n",
      "Epoch 61, loss: 2.302558\n",
      "Epoch 62, loss: 2.302356\n",
      "Epoch 63, loss: 2.302083\n",
      "Epoch 64, loss: 2.301671\n",
      "Epoch 65, loss: 2.301926\n",
      "Epoch 66, loss: 2.301570\n",
      "Epoch 67, loss: 2.302173\n",
      "Epoch 68, loss: 2.302549\n",
      "Epoch 69, loss: 2.302524\n",
      "Epoch 70, loss: 2.301722\n",
      "Epoch 71, loss: 2.302130\n",
      "Epoch 72, loss: 2.302504\n",
      "Epoch 73, loss: 2.302530\n",
      "Epoch 74, loss: 2.302673\n",
      "Epoch 75, loss: 2.301911\n",
      "Epoch 76, loss: 2.301961\n",
      "Epoch 77, loss: 2.302867\n",
      "Epoch 78, loss: 2.301838\n",
      "Epoch 79, loss: 2.302347\n",
      "Epoch 80, loss: 2.302877\n",
      "Epoch 81, loss: 2.302240\n",
      "Epoch 82, loss: 2.301765\n",
      "Epoch 83, loss: 2.301369\n",
      "Epoch 84, loss: 2.302781\n",
      "Epoch 85, loss: 2.301712\n",
      "Epoch 86, loss: 2.302384\n",
      "Epoch 87, loss: 2.301760\n",
      "Epoch 88, loss: 2.301793\n",
      "Epoch 89, loss: 2.302776\n",
      "Epoch 90, loss: 2.302008\n",
      "Epoch 91, loss: 2.301983\n",
      "Epoch 92, loss: 2.302928\n",
      "Epoch 93, loss: 2.302676\n",
      "Epoch 94, loss: 2.301788\n",
      "Epoch 95, loss: 2.301946\n",
      "Epoch 96, loss: 2.301481\n",
      "Epoch 97, loss: 2.301612\n",
      "Epoch 98, loss: 2.302073\n",
      "Epoch 99, loss: 2.302133\n",
      "Epoch 100, loss: 2.302304\n",
      "Epoch 101, loss: 2.302248\n",
      "Epoch 102, loss: 2.302581\n",
      "Epoch 103, loss: 2.303231\n",
      "Epoch 104, loss: 2.302028\n",
      "Epoch 105, loss: 2.301530\n",
      "Epoch 106, loss: 2.301996\n",
      "Epoch 107, loss: 2.301507\n",
      "Epoch 108, loss: 2.301596\n",
      "Epoch 109, loss: 2.301817\n",
      "Epoch 110, loss: 2.302056\n",
      "Epoch 111, loss: 2.301606\n",
      "Epoch 112, loss: 2.302294\n",
      "Epoch 113, loss: 2.302427\n",
      "Epoch 114, loss: 2.301382\n",
      "Epoch 115, loss: 2.302295\n",
      "Epoch 116, loss: 2.302731\n",
      "Epoch 117, loss: 2.301535\n",
      "Epoch 118, loss: 2.302356\n",
      "Epoch 119, loss: 2.302612\n",
      "Epoch 120, loss: 2.302479\n",
      "Epoch 121, loss: 2.301273\n",
      "Epoch 122, loss: 2.301258\n",
      "Epoch 123, loss: 2.301971\n",
      "Epoch 124, loss: 2.301878\n",
      "Epoch 125, loss: 2.302798\n",
      "Epoch 126, loss: 2.301698\n",
      "Epoch 127, loss: 2.301991\n",
      "Epoch 128, loss: 2.301744\n",
      "Epoch 129, loss: 2.301702\n",
      "Epoch 130, loss: 2.301870\n",
      "Epoch 131, loss: 2.302812\n",
      "Epoch 132, loss: 2.302001\n",
      "Epoch 133, loss: 2.302727\n",
      "Epoch 134, loss: 2.301989\n",
      "Epoch 135, loss: 2.302299\n",
      "Epoch 136, loss: 2.301525\n",
      "Epoch 137, loss: 2.302236\n",
      "Epoch 138, loss: 2.301372\n",
      "Epoch 139, loss: 2.302041\n",
      "Epoch 140, loss: 2.301031\n",
      "Epoch 141, loss: 2.301667\n",
      "Epoch 142, loss: 2.302064\n",
      "Epoch 143, loss: 2.301342\n",
      "Epoch 144, loss: 2.301670\n",
      "Epoch 145, loss: 2.301521\n",
      "Epoch 146, loss: 2.301660\n",
      "Epoch 147, loss: 2.302626\n",
      "Epoch 148, loss: 2.301832\n",
      "Epoch 149, loss: 2.302424\n",
      "Epoch 150, loss: 2.301979\n",
      "Epoch 151, loss: 2.301103\n",
      "Epoch 152, loss: 2.301963\n",
      "Epoch 153, loss: 2.302161\n",
      "Epoch 154, loss: 2.301954\n",
      "Epoch 155, loss: 2.301704\n",
      "Epoch 156, loss: 2.302026\n",
      "Epoch 157, loss: 2.302030\n",
      "Epoch 158, loss: 2.301687\n",
      "Epoch 159, loss: 2.301375\n",
      "Epoch 160, loss: 2.302425\n",
      "Epoch 161, loss: 2.302336\n",
      "Epoch 162, loss: 2.302498\n",
      "Epoch 163, loss: 2.302338\n",
      "Epoch 164, loss: 2.301368\n",
      "Epoch 165, loss: 2.301617\n",
      "Epoch 166, loss: 2.302152\n",
      "Epoch 167, loss: 2.301752\n",
      "Epoch 168, loss: 2.301957\n",
      "Epoch 169, loss: 2.301462\n",
      "Epoch 170, loss: 2.301116\n",
      "Epoch 171, loss: 2.302016\n",
      "Epoch 172, loss: 2.301879\n",
      "Epoch 173, loss: 2.301785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174, loss: 2.302268\n",
      "Epoch 175, loss: 2.302188\n",
      "Epoch 176, loss: 2.301598\n",
      "Epoch 177, loss: 2.301866\n",
      "Epoch 178, loss: 2.301889\n",
      "Epoch 179, loss: 2.302455\n",
      "Epoch 180, loss: 2.301735\n",
      "Epoch 181, loss: 2.302197\n",
      "Epoch 182, loss: 2.302063\n",
      "Epoch 183, loss: 2.302040\n",
      "Epoch 184, loss: 2.302103\n",
      "Epoch 185, loss: 2.301723\n",
      "Epoch 186, loss: 2.300726\n",
      "Epoch 187, loss: 2.301694\n",
      "Epoch 188, loss: 2.301551\n",
      "Epoch 189, loss: 2.301804\n",
      "Epoch 190, loss: 2.301708\n",
      "Epoch 191, loss: 2.301342\n",
      "Epoch 192, loss: 2.301420\n",
      "Epoch 193, loss: 2.301840\n",
      "Epoch 194, loss: 2.301903\n",
      "Epoch 195, loss: 2.300602\n",
      "Epoch 196, loss: 2.301958\n",
      "Epoch 197, loss: 2.301551\n",
      "Epoch 198, loss: 2.301600\n",
      "Epoch 199, loss: 2.301986\n",
      "Epoch 200, loss: 2.302212\n",
      "Epoch 201, loss: 2.301636\n",
      "Epoch 202, loss: 2.301470\n",
      "Epoch 203, loss: 2.301347\n",
      "Epoch 204, loss: 2.302023\n",
      "Epoch 205, loss: 2.301595\n",
      "Epoch 206, loss: 2.301569\n",
      "Epoch 207, loss: 2.301493\n",
      "Epoch 208, loss: 2.302135\n",
      "Epoch 209, loss: 2.302293\n",
      "Epoch 210, loss: 2.301557\n",
      "Epoch 211, loss: 2.301849\n",
      "Epoch 212, loss: 2.301735\n",
      "Epoch 213, loss: 2.301722\n",
      "Epoch 214, loss: 2.301420\n",
      "Epoch 215, loss: 2.301422\n",
      "Epoch 216, loss: 2.301730\n",
      "Epoch 217, loss: 2.301340\n",
      "Epoch 218, loss: 2.301452\n",
      "Epoch 219, loss: 2.302163\n",
      "Epoch 220, loss: 2.301740\n",
      "Epoch 221, loss: 2.301455\n",
      "Epoch 222, loss: 2.300276\n",
      "Epoch 223, loss: 2.301436\n",
      "Epoch 224, loss: 2.301501\n",
      "Epoch 225, loss: 2.301228\n",
      "Epoch 226, loss: 2.301156\n",
      "Epoch 227, loss: 2.300693\n",
      "Epoch 228, loss: 2.300875\n",
      "Epoch 229, loss: 2.300849\n",
      "Epoch 230, loss: 2.301909\n",
      "Epoch 231, loss: 2.301382\n",
      "Epoch 232, loss: 2.301626\n",
      "Epoch 233, loss: 2.301863\n",
      "Epoch 234, loss: 2.301474\n",
      "Epoch 235, loss: 2.301580\n",
      "Epoch 236, loss: 2.301559\n",
      "Epoch 237, loss: 2.300876\n",
      "Epoch 238, loss: 2.301445\n",
      "Epoch 239, loss: 2.300801\n",
      "Epoch 240, loss: 2.301580\n",
      "Epoch 241, loss: 2.301755\n",
      "Epoch 242, loss: 2.301649\n",
      "Epoch 243, loss: 2.301196\n",
      "Epoch 244, loss: 2.301389\n",
      "Epoch 245, loss: 2.301769\n",
      "Epoch 246, loss: 2.301501\n",
      "Epoch 247, loss: 2.301073\n",
      "Epoch 248, loss: 2.301293\n",
      "Epoch 249, loss: 2.302420\n",
      "Epoch 250, loss: 2.300743\n",
      "Epoch 251, loss: 2.302156\n",
      "Epoch 252, loss: 2.302614\n",
      "Epoch 253, loss: 2.302150\n",
      "Epoch 254, loss: 2.300921\n",
      "Epoch 255, loss: 2.302295\n",
      "Epoch 256, loss: 2.301290\n",
      "Epoch 257, loss: 2.301751\n",
      "Epoch 258, loss: 2.301625\n",
      "Epoch 259, loss: 2.301590\n",
      "Epoch 260, loss: 2.301976\n",
      "Epoch 261, loss: 2.301965\n",
      "Epoch 262, loss: 2.301769\n",
      "Epoch 263, loss: 2.300932\n",
      "Epoch 264, loss: 2.301566\n",
      "Epoch 265, loss: 2.301106\n",
      "Epoch 266, loss: 2.300906\n",
      "Epoch 267, loss: 2.301738\n",
      "Epoch 268, loss: 2.301476\n",
      "Epoch 269, loss: 2.301948\n",
      "Epoch 270, loss: 2.300696\n",
      "Epoch 271, loss: 2.300701\n",
      "Epoch 272, loss: 2.302092\n",
      "Epoch 273, loss: 2.301517\n",
      "Epoch 274, loss: 2.300914\n",
      "Epoch 275, loss: 2.301782\n",
      "Epoch 276, loss: 2.302184\n",
      "Epoch 277, loss: 2.300820\n",
      "Epoch 278, loss: 2.301662\n",
      "Epoch 279, loss: 2.301083\n",
      "Epoch 280, loss: 2.301643\n",
      "Epoch 281, loss: 2.301529\n",
      "Epoch 282, loss: 2.301696\n",
      "Epoch 283, loss: 2.300474\n",
      "Epoch 284, loss: 2.301603\n",
      "Epoch 285, loss: 2.301084\n",
      "Epoch 286, loss: 2.300917\n",
      "Epoch 287, loss: 2.300021\n",
      "Epoch 288, loss: 2.300939\n",
      "Epoch 289, loss: 2.301550\n",
      "Epoch 290, loss: 2.301675\n",
      "Epoch 291, loss: 2.301093\n",
      "Epoch 292, loss: 2.301176\n",
      "Epoch 293, loss: 2.301073\n",
      "Epoch 294, loss: 2.300584\n",
      "Epoch 295, loss: 2.301007\n",
      "Epoch 296, loss: 2.300917\n",
      "Epoch 297, loss: 2.301725\n",
      "Epoch 298, loss: 2.301181\n",
      "Epoch 299, loss: 2.300943\n",
      "Epoch 0, loss: 2.304183\n",
      "Epoch 1, loss: 2.302576\n",
      "Epoch 2, loss: 2.302377\n",
      "Epoch 3, loss: 2.302480\n",
      "Epoch 4, loss: 2.302829\n",
      "Epoch 5, loss: 2.301976\n",
      "Epoch 6, loss: 2.304000\n",
      "Epoch 7, loss: 2.302930\n",
      "Epoch 8, loss: 2.303112\n",
      "Epoch 9, loss: 2.302656\n",
      "Epoch 10, loss: 2.302890\n",
      "Epoch 11, loss: 2.301887\n",
      "Epoch 12, loss: 2.302257\n",
      "Epoch 13, loss: 2.303069\n",
      "Epoch 14, loss: 2.303326\n",
      "Epoch 15, loss: 2.302262\n",
      "Epoch 16, loss: 2.302793\n",
      "Epoch 17, loss: 2.303038\n",
      "Epoch 18, loss: 2.302809\n",
      "Epoch 19, loss: 2.302314\n",
      "Epoch 20, loss: 2.302498\n",
      "Epoch 21, loss: 2.302466\n",
      "Epoch 22, loss: 2.302120\n",
      "Epoch 23, loss: 2.301691\n",
      "Epoch 24, loss: 2.301999\n",
      "Epoch 25, loss: 2.303031\n",
      "Epoch 26, loss: 2.302948\n",
      "Epoch 27, loss: 2.303029\n",
      "Epoch 28, loss: 2.302753\n",
      "Epoch 29, loss: 2.301380\n",
      "Epoch 30, loss: 2.302470\n",
      "Epoch 31, loss: 2.302410\n",
      "Epoch 32, loss: 2.302396\n",
      "Epoch 33, loss: 2.302251\n",
      "Epoch 34, loss: 2.302193\n",
      "Epoch 35, loss: 2.303063\n",
      "Epoch 36, loss: 2.302359\n",
      "Epoch 37, loss: 2.302959\n",
      "Epoch 38, loss: 2.302715\n",
      "Epoch 39, loss: 2.303433\n",
      "Epoch 40, loss: 2.302574\n",
      "Epoch 41, loss: 2.301833\n",
      "Epoch 42, loss: 2.303244\n",
      "Epoch 43, loss: 2.302936\n",
      "Epoch 44, loss: 2.302393\n",
      "Epoch 45, loss: 2.301573\n",
      "Epoch 46, loss: 2.302806\n",
      "Epoch 47, loss: 2.303016\n",
      "Epoch 48, loss: 2.302939\n",
      "Epoch 49, loss: 2.302856\n",
      "Epoch 50, loss: 2.302456\n",
      "Epoch 51, loss: 2.303135\n",
      "Epoch 52, loss: 2.302819\n",
      "Epoch 53, loss: 2.301435\n",
      "Epoch 54, loss: 2.301526\n",
      "Epoch 55, loss: 2.301876\n",
      "Epoch 56, loss: 2.302566\n",
      "Epoch 57, loss: 2.302522\n",
      "Epoch 58, loss: 2.302170\n",
      "Epoch 59, loss: 2.303360\n",
      "Epoch 60, loss: 2.304146\n",
      "Epoch 61, loss: 2.302761\n",
      "Epoch 62, loss: 2.302661\n",
      "Epoch 63, loss: 2.302489\n",
      "Epoch 64, loss: 2.301773\n",
      "Epoch 65, loss: 2.302167\n",
      "Epoch 66, loss: 2.301710\n",
      "Epoch 67, loss: 2.302342\n",
      "Epoch 68, loss: 2.302866\n",
      "Epoch 69, loss: 2.302853\n",
      "Epoch 70, loss: 2.301827\n",
      "Epoch 71, loss: 2.302337\n",
      "Epoch 72, loss: 2.302977\n",
      "Epoch 73, loss: 2.302862\n",
      "Epoch 74, loss: 2.302879\n",
      "Epoch 75, loss: 2.302138\n",
      "Epoch 76, loss: 2.302164\n",
      "Epoch 77, loss: 2.303209\n",
      "Epoch 78, loss: 2.302238\n",
      "Epoch 79, loss: 2.302687\n",
      "Epoch 80, loss: 2.303138\n",
      "Epoch 81, loss: 2.302525\n",
      "Epoch 82, loss: 2.302047\n",
      "Epoch 83, loss: 2.301716\n",
      "Epoch 84, loss: 2.302992\n",
      "Epoch 85, loss: 2.301913\n",
      "Epoch 86, loss: 2.302716\n",
      "Epoch 87, loss: 2.302029\n",
      "Epoch 88, loss: 2.302268\n",
      "Epoch 89, loss: 2.303230\n",
      "Epoch 90, loss: 2.302290\n",
      "Epoch 91, loss: 2.302325\n",
      "Epoch 92, loss: 2.303096\n",
      "Epoch 93, loss: 2.303203\n",
      "Epoch 94, loss: 2.302176\n",
      "Epoch 95, loss: 2.302230\n",
      "Epoch 96, loss: 2.301678\n",
      "Epoch 97, loss: 2.301944\n",
      "Epoch 98, loss: 2.302545\n",
      "Epoch 99, loss: 2.302535\n",
      "Epoch 100, loss: 2.302680\n",
      "Epoch 101, loss: 2.302735\n",
      "Epoch 102, loss: 2.303134\n",
      "Epoch 103, loss: 2.303858\n",
      "Epoch 104, loss: 2.302582\n",
      "Epoch 105, loss: 2.301901\n",
      "Epoch 106, loss: 2.302597\n",
      "Epoch 107, loss: 2.301856\n",
      "Epoch 108, loss: 2.302113\n",
      "Epoch 109, loss: 2.302327\n",
      "Epoch 110, loss: 2.302522\n",
      "Epoch 111, loss: 2.302117\n",
      "Epoch 112, loss: 2.302761\n",
      "Epoch 113, loss: 2.302819\n",
      "Epoch 114, loss: 2.301532\n",
      "Epoch 115, loss: 2.302691\n",
      "Epoch 116, loss: 2.303091\n",
      "Epoch 117, loss: 2.301910\n",
      "Epoch 118, loss: 2.302951\n",
      "Epoch 119, loss: 2.302915\n",
      "Epoch 120, loss: 2.302845\n",
      "Epoch 121, loss: 2.301702\n",
      "Epoch 122, loss: 2.301714\n",
      "Epoch 123, loss: 2.302555\n",
      "Epoch 124, loss: 2.302139\n",
      "Epoch 125, loss: 2.303394\n",
      "Epoch 126, loss: 2.302097\n",
      "Epoch 127, loss: 2.302583\n",
      "Epoch 128, loss: 2.302261\n",
      "Epoch 129, loss: 2.301991\n",
      "Epoch 130, loss: 2.302207\n",
      "Epoch 131, loss: 2.303385\n",
      "Epoch 132, loss: 2.302802\n",
      "Epoch 133, loss: 2.303472\n",
      "Epoch 134, loss: 2.302393\n",
      "Epoch 135, loss: 2.302695\n",
      "Epoch 136, loss: 2.301793\n",
      "Epoch 137, loss: 2.303017\n",
      "Epoch 138, loss: 2.301675\n",
      "Epoch 139, loss: 2.302558\n",
      "Epoch 140, loss: 2.301495\n",
      "Epoch 141, loss: 2.301854\n",
      "Epoch 142, loss: 2.302411\n",
      "Epoch 143, loss: 2.301893\n",
      "Epoch 144, loss: 2.302296\n",
      "Epoch 145, loss: 2.302132\n",
      "Epoch 146, loss: 2.302108\n",
      "Epoch 147, loss: 2.303391\n",
      "Epoch 148, loss: 2.302525\n",
      "Epoch 149, loss: 2.302872\n",
      "Epoch 150, loss: 2.302817\n",
      "Epoch 151, loss: 2.301524\n",
      "Epoch 152, loss: 2.302851\n",
      "Epoch 153, loss: 2.302959\n",
      "Epoch 154, loss: 2.302379\n",
      "Epoch 155, loss: 2.302559\n",
      "Epoch 156, loss: 2.302610\n",
      "Epoch 157, loss: 2.302475\n",
      "Epoch 158, loss: 2.302244\n",
      "Epoch 159, loss: 2.302157\n",
      "Epoch 160, loss: 2.303116\n",
      "Epoch 161, loss: 2.303185\n",
      "Epoch 162, loss: 2.303002\n",
      "Epoch 163, loss: 2.302811\n",
      "Epoch 164, loss: 2.301964\n",
      "Epoch 165, loss: 2.302249\n",
      "Epoch 166, loss: 2.302988\n",
      "Epoch 167, loss: 2.302484\n",
      "Epoch 168, loss: 2.302648\n",
      "Epoch 169, loss: 2.302141\n",
      "Epoch 170, loss: 2.302172\n",
      "Epoch 171, loss: 2.302553\n",
      "Epoch 172, loss: 2.302830\n",
      "Epoch 173, loss: 2.302054\n",
      "Epoch 174, loss: 2.303024\n",
      "Epoch 175, loss: 2.303195\n",
      "Epoch 176, loss: 2.302079\n",
      "Epoch 177, loss: 2.302822\n",
      "Epoch 178, loss: 2.302545\n",
      "Epoch 179, loss: 2.303147\n",
      "Epoch 180, loss: 2.302724\n",
      "Epoch 181, loss: 2.302953\n",
      "Epoch 182, loss: 2.303235\n",
      "Epoch 183, loss: 2.302466\n",
      "Epoch 184, loss: 2.302960\n",
      "Epoch 185, loss: 2.302451\n",
      "Epoch 186, loss: 2.301040\n",
      "Epoch 187, loss: 2.302666\n",
      "Epoch 188, loss: 2.302232\n",
      "Epoch 189, loss: 2.302673\n",
      "Epoch 190, loss: 2.302586\n",
      "Epoch 191, loss: 2.302412\n",
      "Epoch 192, loss: 2.301681\n",
      "Epoch 193, loss: 2.302792\n",
      "Epoch 194, loss: 2.302868\n",
      "Epoch 195, loss: 2.301255\n",
      "Epoch 196, loss: 2.302970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197, loss: 2.302301\n",
      "Epoch 198, loss: 2.302306\n",
      "Epoch 199, loss: 2.302659\n",
      "Epoch 200, loss: 2.302899\n",
      "Epoch 201, loss: 2.301982\n",
      "Epoch 202, loss: 2.302386\n",
      "Epoch 203, loss: 2.301970\n",
      "Epoch 204, loss: 2.302599\n",
      "Epoch 205, loss: 2.302227\n",
      "Epoch 206, loss: 2.302404\n",
      "Epoch 207, loss: 2.301900\n",
      "Epoch 208, loss: 2.302586\n",
      "Epoch 209, loss: 2.303153\n",
      "Epoch 210, loss: 2.302381\n",
      "Epoch 211, loss: 2.302908\n",
      "Epoch 212, loss: 2.302338\n",
      "Epoch 213, loss: 2.302342\n",
      "Epoch 214, loss: 2.302367\n",
      "Epoch 215, loss: 2.302264\n",
      "Epoch 216, loss: 2.302360\n",
      "Epoch 217, loss: 2.302050\n",
      "Epoch 218, loss: 2.302148\n",
      "Epoch 219, loss: 2.303065\n",
      "Epoch 220, loss: 2.302706\n",
      "Epoch 221, loss: 2.302038\n",
      "Epoch 222, loss: 2.301021\n",
      "Epoch 223, loss: 2.302327\n",
      "Epoch 224, loss: 2.302580\n",
      "Epoch 225, loss: 2.302393\n",
      "Epoch 226, loss: 2.301824\n",
      "Epoch 227, loss: 2.301727\n",
      "Epoch 228, loss: 2.301417\n",
      "Epoch 229, loss: 2.301815\n",
      "Epoch 230, loss: 2.302829\n",
      "Epoch 231, loss: 2.301916\n",
      "Epoch 232, loss: 2.302658\n",
      "Epoch 233, loss: 2.302570\n",
      "Epoch 234, loss: 2.302679\n",
      "Epoch 235, loss: 2.302447\n",
      "Epoch 236, loss: 2.302615\n",
      "Epoch 237, loss: 2.301862\n",
      "Epoch 238, loss: 2.302623\n",
      "Epoch 239, loss: 2.301631\n",
      "Epoch 240, loss: 2.302475\n",
      "Epoch 241, loss: 2.302939\n",
      "Epoch 242, loss: 2.302369\n",
      "Epoch 243, loss: 2.301486\n",
      "Epoch 244, loss: 2.302503\n",
      "Epoch 245, loss: 2.302543\n",
      "Epoch 246, loss: 2.302344\n",
      "Epoch 247, loss: 2.301202\n",
      "Epoch 248, loss: 2.302084\n",
      "Epoch 249, loss: 2.303145\n",
      "Epoch 250, loss: 2.301819\n",
      "Epoch 251, loss: 2.302806\n",
      "Epoch 252, loss: 2.303754\n",
      "Epoch 253, loss: 2.303272\n",
      "Epoch 254, loss: 2.301752\n",
      "Epoch 255, loss: 2.303496\n",
      "Epoch 256, loss: 2.302159\n",
      "Epoch 257, loss: 2.302563\n",
      "Epoch 258, loss: 2.302914\n",
      "Epoch 259, loss: 2.302445\n",
      "Epoch 260, loss: 2.302989\n",
      "Epoch 261, loss: 2.303249\n",
      "Epoch 262, loss: 2.302905\n",
      "Epoch 263, loss: 2.301817\n",
      "Epoch 264, loss: 2.302463\n",
      "Epoch 265, loss: 2.302186\n",
      "Epoch 266, loss: 2.302004\n",
      "Epoch 267, loss: 2.303388\n",
      "Epoch 268, loss: 2.302777\n",
      "Epoch 269, loss: 2.303149\n",
      "Epoch 270, loss: 2.301599\n",
      "Epoch 271, loss: 2.301947\n",
      "Epoch 272, loss: 2.303109\n",
      "Epoch 273, loss: 2.302487\n",
      "Epoch 274, loss: 2.301700\n",
      "Epoch 275, loss: 2.302727\n",
      "Epoch 276, loss: 2.303693\n",
      "Epoch 277, loss: 2.302369\n",
      "Epoch 278, loss: 2.302648\n",
      "Epoch 279, loss: 2.301932\n",
      "Epoch 280, loss: 2.302931\n",
      "Epoch 281, loss: 2.302718\n",
      "Epoch 282, loss: 2.302957\n",
      "Epoch 283, loss: 2.301316\n",
      "Epoch 284, loss: 2.302592\n",
      "Epoch 285, loss: 2.301605\n",
      "Epoch 286, loss: 2.301736\n",
      "Epoch 287, loss: 2.301368\n",
      "Epoch 288, loss: 2.301932\n",
      "Epoch 289, loss: 2.302847\n",
      "Epoch 290, loss: 2.302341\n",
      "Epoch 291, loss: 2.302192\n",
      "Epoch 292, loss: 2.302323\n",
      "Epoch 293, loss: 2.302216\n",
      "Epoch 294, loss: 2.301835\n",
      "Epoch 295, loss: 2.302079\n",
      "Epoch 296, loss: 2.302095\n",
      "Epoch 297, loss: 2.302872\n",
      "Epoch 298, loss: 2.302269\n",
      "Epoch 299, loss: 2.302362\n",
      "Epoch 0, loss: 2.304180\n",
      "Epoch 1, loss: 2.302573\n",
      "Epoch 2, loss: 2.302374\n",
      "Epoch 3, loss: 2.302477\n",
      "Epoch 4, loss: 2.302826\n",
      "Epoch 5, loss: 2.301973\n",
      "Epoch 6, loss: 2.303998\n",
      "Epoch 7, loss: 2.302927\n",
      "Epoch 8, loss: 2.303109\n",
      "Epoch 9, loss: 2.302654\n",
      "Epoch 10, loss: 2.302887\n",
      "Epoch 11, loss: 2.301884\n",
      "Epoch 12, loss: 2.302254\n",
      "Epoch 13, loss: 2.303066\n",
      "Epoch 14, loss: 2.303323\n",
      "Epoch 15, loss: 2.302259\n",
      "Epoch 16, loss: 2.302790\n",
      "Epoch 17, loss: 2.303036\n",
      "Epoch 18, loss: 2.302807\n",
      "Epoch 19, loss: 2.302312\n",
      "Epoch 20, loss: 2.302495\n",
      "Epoch 21, loss: 2.302463\n",
      "Epoch 22, loss: 2.302117\n",
      "Epoch 23, loss: 2.301688\n",
      "Epoch 24, loss: 2.301996\n",
      "Epoch 25, loss: 2.303028\n",
      "Epoch 26, loss: 2.302945\n",
      "Epoch 27, loss: 2.303026\n",
      "Epoch 28, loss: 2.302750\n",
      "Epoch 29, loss: 2.301377\n",
      "Epoch 30, loss: 2.302467\n",
      "Epoch 31, loss: 2.302407\n",
      "Epoch 32, loss: 2.302393\n",
      "Epoch 33, loss: 2.302248\n",
      "Epoch 34, loss: 2.302191\n",
      "Epoch 35, loss: 2.303060\n",
      "Epoch 36, loss: 2.302356\n",
      "Epoch 37, loss: 2.302956\n",
      "Epoch 38, loss: 2.302712\n",
      "Epoch 39, loss: 2.303430\n",
      "Epoch 40, loss: 2.302571\n",
      "Epoch 41, loss: 2.301830\n",
      "Epoch 42, loss: 2.303241\n",
      "Epoch 43, loss: 2.302934\n",
      "Epoch 44, loss: 2.302391\n",
      "Epoch 45, loss: 2.301570\n",
      "Epoch 46, loss: 2.302803\n",
      "Epoch 47, loss: 2.303013\n",
      "Epoch 48, loss: 2.302936\n",
      "Epoch 49, loss: 2.302854\n",
      "Epoch 50, loss: 2.302453\n",
      "Epoch 51, loss: 2.303132\n",
      "Epoch 52, loss: 2.302816\n",
      "Epoch 53, loss: 2.301432\n",
      "Epoch 54, loss: 2.301523\n",
      "Epoch 55, loss: 2.301873\n",
      "Epoch 56, loss: 2.302563\n",
      "Epoch 57, loss: 2.302520\n",
      "Epoch 58, loss: 2.302167\n",
      "Epoch 59, loss: 2.303357\n",
      "Epoch 60, loss: 2.304144\n",
      "Epoch 61, loss: 2.302758\n",
      "Epoch 62, loss: 2.302658\n",
      "Epoch 63, loss: 2.302487\n",
      "Epoch 64, loss: 2.301770\n",
      "Epoch 65, loss: 2.302164\n",
      "Epoch 66, loss: 2.301707\n",
      "Epoch 67, loss: 2.302339\n",
      "Epoch 68, loss: 2.302863\n",
      "Epoch 69, loss: 2.302850\n",
      "Epoch 70, loss: 2.301824\n",
      "Epoch 71, loss: 2.302335\n",
      "Epoch 72, loss: 2.302974\n",
      "Epoch 73, loss: 2.302859\n",
      "Epoch 74, loss: 2.302876\n",
      "Epoch 75, loss: 2.302135\n",
      "Epoch 76, loss: 2.302161\n",
      "Epoch 77, loss: 2.303206\n",
      "Epoch 78, loss: 2.302235\n",
      "Epoch 79, loss: 2.302685\n",
      "Epoch 80, loss: 2.303135\n",
      "Epoch 81, loss: 2.302523\n",
      "Epoch 82, loss: 2.302045\n",
      "Epoch 83, loss: 2.301713\n",
      "Epoch 84, loss: 2.302989\n",
      "Epoch 85, loss: 2.301910\n",
      "Epoch 86, loss: 2.302713\n",
      "Epoch 87, loss: 2.302026\n",
      "Epoch 88, loss: 2.302265\n",
      "Epoch 89, loss: 2.303227\n",
      "Epoch 90, loss: 2.302287\n",
      "Epoch 91, loss: 2.302322\n",
      "Epoch 92, loss: 2.303094\n",
      "Epoch 93, loss: 2.303201\n",
      "Epoch 94, loss: 2.302173\n",
      "Epoch 95, loss: 2.302227\n",
      "Epoch 96, loss: 2.301675\n",
      "Epoch 97, loss: 2.301941\n",
      "Epoch 98, loss: 2.302542\n",
      "Epoch 99, loss: 2.302533\n",
      "Epoch 100, loss: 2.302677\n",
      "Epoch 101, loss: 2.302733\n",
      "Epoch 102, loss: 2.303131\n",
      "Epoch 103, loss: 2.303855\n",
      "Epoch 104, loss: 2.302579\n",
      "Epoch 105, loss: 2.301898\n",
      "Epoch 106, loss: 2.302594\n",
      "Epoch 107, loss: 2.301854\n",
      "Epoch 108, loss: 2.302110\n",
      "Epoch 109, loss: 2.302324\n",
      "Epoch 110, loss: 2.302519\n",
      "Epoch 111, loss: 2.302114\n",
      "Epoch 112, loss: 2.302758\n",
      "Epoch 113, loss: 2.302816\n",
      "Epoch 114, loss: 2.301529\n",
      "Epoch 115, loss: 2.302688\n",
      "Epoch 116, loss: 2.303088\n",
      "Epoch 117, loss: 2.301908\n",
      "Epoch 118, loss: 2.302948\n",
      "Epoch 119, loss: 2.302912\n",
      "Epoch 120, loss: 2.302842\n",
      "Epoch 121, loss: 2.301699\n",
      "Epoch 122, loss: 2.301712\n",
      "Epoch 123, loss: 2.302553\n",
      "Epoch 124, loss: 2.302136\n",
      "Epoch 125, loss: 2.303392\n",
      "Epoch 126, loss: 2.302095\n",
      "Epoch 127, loss: 2.302580\n",
      "Epoch 128, loss: 2.302258\n",
      "Epoch 129, loss: 2.301989\n",
      "Epoch 130, loss: 2.302204\n",
      "Epoch 131, loss: 2.303382\n",
      "Epoch 132, loss: 2.302800\n",
      "Epoch 133, loss: 2.303469\n",
      "Epoch 134, loss: 2.302390\n",
      "Epoch 135, loss: 2.302693\n",
      "Epoch 136, loss: 2.301790\n",
      "Epoch 137, loss: 2.303014\n",
      "Epoch 138, loss: 2.301673\n",
      "Epoch 139, loss: 2.302555\n",
      "Epoch 140, loss: 2.301492\n",
      "Epoch 141, loss: 2.301851\n",
      "Epoch 142, loss: 2.302408\n",
      "Epoch 143, loss: 2.301890\n",
      "Epoch 144, loss: 2.302294\n",
      "Epoch 145, loss: 2.302129\n",
      "Epoch 146, loss: 2.302106\n",
      "Epoch 147, loss: 2.303388\n",
      "Epoch 148, loss: 2.302522\n",
      "Epoch 149, loss: 2.302869\n",
      "Epoch 150, loss: 2.302815\n",
      "Epoch 151, loss: 2.301521\n",
      "Epoch 152, loss: 2.302848\n",
      "Epoch 153, loss: 2.302956\n",
      "Epoch 154, loss: 2.302376\n",
      "Epoch 155, loss: 2.302556\n",
      "Epoch 156, loss: 2.302607\n",
      "Epoch 157, loss: 2.302472\n",
      "Epoch 158, loss: 2.302241\n",
      "Epoch 159, loss: 2.302155\n",
      "Epoch 160, loss: 2.303113\n",
      "Epoch 161, loss: 2.303182\n",
      "Epoch 162, loss: 2.302999\n",
      "Epoch 163, loss: 2.302809\n",
      "Epoch 164, loss: 2.301961\n",
      "Epoch 165, loss: 2.302246\n",
      "Epoch 166, loss: 2.302985\n",
      "Epoch 167, loss: 2.302481\n",
      "Epoch 168, loss: 2.302645\n",
      "Epoch 169, loss: 2.302138\n",
      "Epoch 170, loss: 2.302169\n",
      "Epoch 171, loss: 2.302550\n",
      "Epoch 172, loss: 2.302827\n",
      "Epoch 173, loss: 2.302051\n",
      "Epoch 174, loss: 2.303021\n",
      "Epoch 175, loss: 2.303192\n",
      "Epoch 176, loss: 2.302076\n",
      "Epoch 177, loss: 2.302820\n",
      "Epoch 178, loss: 2.302542\n",
      "Epoch 179, loss: 2.303144\n",
      "Epoch 180, loss: 2.302721\n",
      "Epoch 181, loss: 2.302950\n",
      "Epoch 182, loss: 2.303232\n",
      "Epoch 183, loss: 2.302463\n",
      "Epoch 184, loss: 2.302957\n",
      "Epoch 185, loss: 2.302448\n",
      "Epoch 186, loss: 2.301037\n",
      "Epoch 187, loss: 2.302664\n",
      "Epoch 188, loss: 2.302229\n",
      "Epoch 189, loss: 2.302670\n",
      "Epoch 190, loss: 2.302583\n",
      "Epoch 191, loss: 2.302409\n",
      "Epoch 192, loss: 2.301678\n",
      "Epoch 193, loss: 2.302790\n",
      "Epoch 194, loss: 2.302865\n",
      "Epoch 195, loss: 2.301252\n",
      "Epoch 196, loss: 2.302967\n",
      "Epoch 197, loss: 2.302298\n",
      "Epoch 198, loss: 2.302303\n",
      "Epoch 199, loss: 2.302657\n",
      "Epoch 200, loss: 2.302896\n",
      "Epoch 201, loss: 2.301980\n",
      "Epoch 202, loss: 2.302383\n",
      "Epoch 203, loss: 2.301967\n",
      "Epoch 204, loss: 2.302596\n",
      "Epoch 205, loss: 2.302224\n",
      "Epoch 206, loss: 2.302401\n",
      "Epoch 207, loss: 2.301897\n",
      "Epoch 208, loss: 2.302583\n",
      "Epoch 209, loss: 2.303150\n",
      "Epoch 210, loss: 2.302379\n",
      "Epoch 211, loss: 2.302905\n",
      "Epoch 212, loss: 2.302335\n",
      "Epoch 213, loss: 2.302340\n",
      "Epoch 214, loss: 2.302364\n",
      "Epoch 215, loss: 2.302262\n",
      "Epoch 216, loss: 2.302358\n",
      "Epoch 217, loss: 2.302047\n",
      "Epoch 218, loss: 2.302146\n",
      "Epoch 219, loss: 2.303062\n",
      "Epoch 220, loss: 2.302703\n",
      "Epoch 221, loss: 2.302036\n",
      "Epoch 222, loss: 2.301018\n",
      "Epoch 223, loss: 2.302324\n",
      "Epoch 224, loss: 2.302577\n",
      "Epoch 225, loss: 2.302390\n",
      "Epoch 226, loss: 2.301821\n",
      "Epoch 227, loss: 2.301724\n",
      "Epoch 228, loss: 2.301414\n",
      "Epoch 229, loss: 2.301812\n",
      "Epoch 230, loss: 2.302826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231, loss: 2.301913\n",
      "Epoch 232, loss: 2.302656\n",
      "Epoch 233, loss: 2.302568\n",
      "Epoch 234, loss: 2.302676\n",
      "Epoch 235, loss: 2.302444\n",
      "Epoch 236, loss: 2.302613\n",
      "Epoch 237, loss: 2.301859\n",
      "Epoch 238, loss: 2.302620\n",
      "Epoch 239, loss: 2.301629\n",
      "Epoch 240, loss: 2.302472\n",
      "Epoch 241, loss: 2.302936\n",
      "Epoch 242, loss: 2.302366\n",
      "Epoch 243, loss: 2.301483\n",
      "Epoch 244, loss: 2.302500\n",
      "Epoch 245, loss: 2.302540\n",
      "Epoch 246, loss: 2.302341\n",
      "Epoch 247, loss: 2.301199\n",
      "Epoch 248, loss: 2.302081\n",
      "Epoch 249, loss: 2.303142\n",
      "Epoch 250, loss: 2.301816\n",
      "Epoch 251, loss: 2.302803\n",
      "Epoch 252, loss: 2.303751\n",
      "Epoch 253, loss: 2.303270\n",
      "Epoch 254, loss: 2.301749\n",
      "Epoch 255, loss: 2.303493\n",
      "Epoch 256, loss: 2.302157\n",
      "Epoch 257, loss: 2.302560\n",
      "Epoch 258, loss: 2.302911\n",
      "Epoch 259, loss: 2.302442\n",
      "Epoch 260, loss: 2.302986\n",
      "Epoch 261, loss: 2.303246\n",
      "Epoch 262, loss: 2.302902\n",
      "Epoch 263, loss: 2.301815\n",
      "Epoch 264, loss: 2.302460\n",
      "Epoch 265, loss: 2.302184\n",
      "Epoch 266, loss: 2.302001\n",
      "Epoch 267, loss: 2.303385\n",
      "Epoch 268, loss: 2.302774\n",
      "Epoch 269, loss: 2.303146\n",
      "Epoch 270, loss: 2.301597\n",
      "Epoch 271, loss: 2.301944\n",
      "Epoch 272, loss: 2.303106\n",
      "Epoch 273, loss: 2.302484\n",
      "Epoch 274, loss: 2.301697\n",
      "Epoch 275, loss: 2.302725\n",
      "Epoch 276, loss: 2.303690\n",
      "Epoch 277, loss: 2.302367\n",
      "Epoch 278, loss: 2.302645\n",
      "Epoch 279, loss: 2.301929\n",
      "Epoch 280, loss: 2.302928\n",
      "Epoch 281, loss: 2.302715\n",
      "Epoch 282, loss: 2.302954\n",
      "Epoch 283, loss: 2.301313\n",
      "Epoch 284, loss: 2.302589\n",
      "Epoch 285, loss: 2.301602\n",
      "Epoch 286, loss: 2.301734\n",
      "Epoch 287, loss: 2.301366\n",
      "Epoch 288, loss: 2.301929\n",
      "Epoch 289, loss: 2.302844\n",
      "Epoch 290, loss: 2.302338\n",
      "Epoch 291, loss: 2.302189\n",
      "Epoch 292, loss: 2.302321\n",
      "Epoch 293, loss: 2.302213\n",
      "Epoch 294, loss: 2.301832\n",
      "Epoch 295, loss: 2.302076\n",
      "Epoch 296, loss: 2.302092\n",
      "Epoch 297, loss: 2.302870\n",
      "Epoch 298, loss: 2.302266\n",
      "Epoch 299, loss: 2.302359\n",
      "Epoch 0, loss: 2.304180\n",
      "Epoch 1, loss: 2.302573\n",
      "Epoch 2, loss: 2.302374\n",
      "Epoch 3, loss: 2.302477\n",
      "Epoch 4, loss: 2.302826\n",
      "Epoch 5, loss: 2.301973\n",
      "Epoch 6, loss: 2.303997\n",
      "Epoch 7, loss: 2.302927\n",
      "Epoch 8, loss: 2.303109\n",
      "Epoch 9, loss: 2.302653\n",
      "Epoch 10, loss: 2.302887\n",
      "Epoch 11, loss: 2.301884\n",
      "Epoch 12, loss: 2.302254\n",
      "Epoch 13, loss: 2.303066\n",
      "Epoch 14, loss: 2.303322\n",
      "Epoch 15, loss: 2.302259\n",
      "Epoch 16, loss: 2.302790\n",
      "Epoch 17, loss: 2.303035\n",
      "Epoch 18, loss: 2.302806\n",
      "Epoch 19, loss: 2.302311\n",
      "Epoch 20, loss: 2.302495\n",
      "Epoch 21, loss: 2.302463\n",
      "Epoch 22, loss: 2.302117\n",
      "Epoch 23, loss: 2.301688\n",
      "Epoch 24, loss: 2.301996\n",
      "Epoch 25, loss: 2.303028\n",
      "Epoch 26, loss: 2.302945\n",
      "Epoch 27, loss: 2.303026\n",
      "Epoch 28, loss: 2.302749\n",
      "Epoch 29, loss: 2.301377\n",
      "Epoch 30, loss: 2.302467\n",
      "Epoch 31, loss: 2.302407\n",
      "Epoch 32, loss: 2.302393\n",
      "Epoch 33, loss: 2.302248\n",
      "Epoch 34, loss: 2.302190\n",
      "Epoch 35, loss: 2.303060\n",
      "Epoch 36, loss: 2.302356\n",
      "Epoch 37, loss: 2.302956\n",
      "Epoch 38, loss: 2.302712\n",
      "Epoch 39, loss: 2.303430\n",
      "Epoch 40, loss: 2.302571\n",
      "Epoch 41, loss: 2.301830\n",
      "Epoch 42, loss: 2.303241\n",
      "Epoch 43, loss: 2.302933\n",
      "Epoch 44, loss: 2.302390\n",
      "Epoch 45, loss: 2.301570\n",
      "Epoch 46, loss: 2.302803\n",
      "Epoch 47, loss: 2.303012\n",
      "Epoch 48, loss: 2.302936\n",
      "Epoch 49, loss: 2.302853\n",
      "Epoch 50, loss: 2.302453\n",
      "Epoch 51, loss: 2.303132\n",
      "Epoch 52, loss: 2.302815\n",
      "Epoch 53, loss: 2.301432\n",
      "Epoch 54, loss: 2.301523\n",
      "Epoch 55, loss: 2.301873\n",
      "Epoch 56, loss: 2.302563\n",
      "Epoch 57, loss: 2.302519\n",
      "Epoch 58, loss: 2.302167\n",
      "Epoch 59, loss: 2.303357\n",
      "Epoch 60, loss: 2.304143\n",
      "Epoch 61, loss: 2.302758\n",
      "Epoch 62, loss: 2.302658\n",
      "Epoch 63, loss: 2.302486\n",
      "Epoch 64, loss: 2.301770\n",
      "Epoch 65, loss: 2.302164\n",
      "Epoch 66, loss: 2.301707\n",
      "Epoch 67, loss: 2.302339\n",
      "Epoch 68, loss: 2.302863\n",
      "Epoch 69, loss: 2.302849\n",
      "Epoch 70, loss: 2.301824\n",
      "Epoch 71, loss: 2.302334\n",
      "Epoch 72, loss: 2.302974\n",
      "Epoch 73, loss: 2.302859\n",
      "Epoch 74, loss: 2.302875\n",
      "Epoch 75, loss: 2.302135\n",
      "Epoch 76, loss: 2.302161\n",
      "Epoch 77, loss: 2.303206\n",
      "Epoch 78, loss: 2.302235\n",
      "Epoch 79, loss: 2.302684\n",
      "Epoch 80, loss: 2.303135\n",
      "Epoch 81, loss: 2.302522\n",
      "Epoch 82, loss: 2.302044\n",
      "Epoch 83, loss: 2.301712\n",
      "Epoch 84, loss: 2.302989\n",
      "Epoch 85, loss: 2.301910\n",
      "Epoch 86, loss: 2.302713\n",
      "Epoch 87, loss: 2.302026\n",
      "Epoch 88, loss: 2.302265\n",
      "Epoch 89, loss: 2.303227\n",
      "Epoch 90, loss: 2.302287\n",
      "Epoch 91, loss: 2.302322\n",
      "Epoch 92, loss: 2.303093\n",
      "Epoch 93, loss: 2.303200\n",
      "Epoch 94, loss: 2.302173\n",
      "Epoch 95, loss: 2.302227\n",
      "Epoch 96, loss: 2.301675\n",
      "Epoch 97, loss: 2.301941\n",
      "Epoch 98, loss: 2.302542\n",
      "Epoch 99, loss: 2.302532\n",
      "Epoch 100, loss: 2.302677\n",
      "Epoch 101, loss: 2.302732\n",
      "Epoch 102, loss: 2.303131\n",
      "Epoch 103, loss: 2.303855\n",
      "Epoch 104, loss: 2.302579\n",
      "Epoch 105, loss: 2.301898\n",
      "Epoch 106, loss: 2.302594\n",
      "Epoch 107, loss: 2.301853\n",
      "Epoch 108, loss: 2.302110\n",
      "Epoch 109, loss: 2.302324\n",
      "Epoch 110, loss: 2.302519\n",
      "Epoch 111, loss: 2.302114\n",
      "Epoch 112, loss: 2.302758\n",
      "Epoch 113, loss: 2.302816\n",
      "Epoch 114, loss: 2.301529\n",
      "Epoch 115, loss: 2.302688\n",
      "Epoch 116, loss: 2.303088\n",
      "Epoch 117, loss: 2.301907\n",
      "Epoch 118, loss: 2.302948\n",
      "Epoch 119, loss: 2.302912\n",
      "Epoch 120, loss: 2.302842\n",
      "Epoch 121, loss: 2.301699\n",
      "Epoch 122, loss: 2.301711\n",
      "Epoch 123, loss: 2.302552\n",
      "Epoch 124, loss: 2.302136\n",
      "Epoch 125, loss: 2.303391\n",
      "Epoch 126, loss: 2.302094\n",
      "Epoch 127, loss: 2.302579\n",
      "Epoch 128, loss: 2.302258\n",
      "Epoch 129, loss: 2.301988\n",
      "Epoch 130, loss: 2.302204\n",
      "Epoch 131, loss: 2.303382\n",
      "Epoch 132, loss: 2.302799\n",
      "Epoch 133, loss: 2.303469\n",
      "Epoch 134, loss: 2.302390\n",
      "Epoch 135, loss: 2.302692\n",
      "Epoch 136, loss: 2.301790\n",
      "Epoch 137, loss: 2.303013\n",
      "Epoch 138, loss: 2.301672\n",
      "Epoch 139, loss: 2.302555\n",
      "Epoch 140, loss: 2.301492\n",
      "Epoch 141, loss: 2.301851\n",
      "Epoch 142, loss: 2.302408\n",
      "Epoch 143, loss: 2.301890\n",
      "Epoch 144, loss: 2.302293\n",
      "Epoch 145, loss: 2.302129\n",
      "Epoch 146, loss: 2.302105\n",
      "Epoch 147, loss: 2.303388\n",
      "Epoch 148, loss: 2.302522\n",
      "Epoch 149, loss: 2.302869\n",
      "Epoch 150, loss: 2.302814\n",
      "Epoch 151, loss: 2.301521\n",
      "Epoch 152, loss: 2.302848\n",
      "Epoch 153, loss: 2.302956\n",
      "Epoch 154, loss: 2.302376\n",
      "Epoch 155, loss: 2.302556\n",
      "Epoch 156, loss: 2.302607\n",
      "Epoch 157, loss: 2.302472\n",
      "Epoch 158, loss: 2.302241\n",
      "Epoch 159, loss: 2.302154\n",
      "Epoch 160, loss: 2.303113\n",
      "Epoch 161, loss: 2.303182\n",
      "Epoch 162, loss: 2.302999\n",
      "Epoch 163, loss: 2.302808\n",
      "Epoch 164, loss: 2.301961\n",
      "Epoch 165, loss: 2.302246\n",
      "Epoch 166, loss: 2.302985\n",
      "Epoch 167, loss: 2.302481\n",
      "Epoch 168, loss: 2.302645\n",
      "Epoch 169, loss: 2.302138\n",
      "Epoch 170, loss: 2.302169\n",
      "Epoch 171, loss: 2.302549\n",
      "Epoch 172, loss: 2.302827\n",
      "Epoch 173, loss: 2.302051\n",
      "Epoch 174, loss: 2.303021\n",
      "Epoch 175, loss: 2.303192\n",
      "Epoch 176, loss: 2.302076\n",
      "Epoch 177, loss: 2.302819\n",
      "Epoch 178, loss: 2.302542\n",
      "Epoch 179, loss: 2.303144\n",
      "Epoch 180, loss: 2.302721\n",
      "Epoch 181, loss: 2.302950\n",
      "Epoch 182, loss: 2.303232\n",
      "Epoch 183, loss: 2.302463\n",
      "Epoch 184, loss: 2.302957\n",
      "Epoch 185, loss: 2.302447\n",
      "Epoch 186, loss: 2.301037\n",
      "Epoch 187, loss: 2.302663\n",
      "Epoch 188, loss: 2.302229\n",
      "Epoch 189, loss: 2.302670\n",
      "Epoch 190, loss: 2.302583\n",
      "Epoch 191, loss: 2.302409\n",
      "Epoch 192, loss: 2.301678\n",
      "Epoch 193, loss: 2.302789\n",
      "Epoch 194, loss: 2.302865\n",
      "Epoch 195, loss: 2.301252\n",
      "Epoch 196, loss: 2.302967\n",
      "Epoch 197, loss: 2.302298\n",
      "Epoch 198, loss: 2.302303\n",
      "Epoch 199, loss: 2.302656\n",
      "Epoch 200, loss: 2.302895\n",
      "Epoch 201, loss: 2.301979\n",
      "Epoch 202, loss: 2.302383\n",
      "Epoch 203, loss: 2.301967\n",
      "Epoch 204, loss: 2.302596\n",
      "Epoch 205, loss: 2.302224\n",
      "Epoch 206, loss: 2.302401\n",
      "Epoch 207, loss: 2.301896\n",
      "Epoch 208, loss: 2.302583\n",
      "Epoch 209, loss: 2.303150\n",
      "Epoch 210, loss: 2.302378\n",
      "Epoch 211, loss: 2.302905\n",
      "Epoch 212, loss: 2.302335\n",
      "Epoch 213, loss: 2.302339\n",
      "Epoch 214, loss: 2.302364\n",
      "Epoch 215, loss: 2.302261\n",
      "Epoch 216, loss: 2.302357\n",
      "Epoch 217, loss: 2.302047\n",
      "Epoch 218, loss: 2.302145\n",
      "Epoch 219, loss: 2.303062\n",
      "Epoch 220, loss: 2.302703\n",
      "Epoch 221, loss: 2.302035\n",
      "Epoch 222, loss: 2.301018\n",
      "Epoch 223, loss: 2.302324\n",
      "Epoch 224, loss: 2.302577\n",
      "Epoch 225, loss: 2.302390\n",
      "Epoch 226, loss: 2.301821\n",
      "Epoch 227, loss: 2.301724\n",
      "Epoch 228, loss: 2.301414\n",
      "Epoch 229, loss: 2.301812\n",
      "Epoch 230, loss: 2.302826\n",
      "Epoch 231, loss: 2.301913\n",
      "Epoch 232, loss: 2.302655\n",
      "Epoch 233, loss: 2.302567\n",
      "Epoch 234, loss: 2.302675\n",
      "Epoch 235, loss: 2.302444\n",
      "Epoch 236, loss: 2.302612\n",
      "Epoch 237, loss: 2.301859\n",
      "Epoch 238, loss: 2.302620\n",
      "Epoch 239, loss: 2.301628\n",
      "Epoch 240, loss: 2.302472\n",
      "Epoch 241, loss: 2.302936\n",
      "Epoch 242, loss: 2.302366\n",
      "Epoch 243, loss: 2.301483\n",
      "Epoch 244, loss: 2.302500\n",
      "Epoch 245, loss: 2.302540\n",
      "Epoch 246, loss: 2.302341\n",
      "Epoch 247, loss: 2.301199\n",
      "Epoch 248, loss: 2.302081\n",
      "Epoch 249, loss: 2.303142\n",
      "Epoch 250, loss: 2.301816\n",
      "Epoch 251, loss: 2.302803\n",
      "Epoch 252, loss: 2.303751\n",
      "Epoch 253, loss: 2.303269\n",
      "Epoch 254, loss: 2.301748\n",
      "Epoch 255, loss: 2.303493\n",
      "Epoch 256, loss: 2.302156\n",
      "Epoch 257, loss: 2.302560\n",
      "Epoch 258, loss: 2.302910\n",
      "Epoch 259, loss: 2.302442\n",
      "Epoch 260, loss: 2.302986\n",
      "Epoch 261, loss: 2.303246\n",
      "Epoch 262, loss: 2.302902\n",
      "Epoch 263, loss: 2.301814\n",
      "Epoch 264, loss: 2.302460\n",
      "Epoch 265, loss: 2.302183\n",
      "Epoch 266, loss: 2.302001\n",
      "Epoch 267, loss: 2.303385\n",
      "Epoch 268, loss: 2.302774\n",
      "Epoch 269, loss: 2.303146\n",
      "Epoch 270, loss: 2.301596\n",
      "Epoch 271, loss: 2.301944\n",
      "Epoch 272, loss: 2.303106\n",
      "Epoch 273, loss: 2.302484\n",
      "Epoch 274, loss: 2.301697\n",
      "Epoch 275, loss: 2.302724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 276, loss: 2.303690\n",
      "Epoch 277, loss: 2.302366\n",
      "Epoch 278, loss: 2.302645\n",
      "Epoch 279, loss: 2.301929\n",
      "Epoch 280, loss: 2.302928\n",
      "Epoch 281, loss: 2.302715\n",
      "Epoch 282, loss: 2.302954\n",
      "Epoch 283, loss: 2.301313\n",
      "Epoch 284, loss: 2.302589\n",
      "Epoch 285, loss: 2.301602\n",
      "Epoch 286, loss: 2.301733\n",
      "Epoch 287, loss: 2.301365\n",
      "Epoch 288, loss: 2.301929\n",
      "Epoch 289, loss: 2.302843\n",
      "Epoch 290, loss: 2.302338\n",
      "Epoch 291, loss: 2.302189\n",
      "Epoch 292, loss: 2.302320\n",
      "Epoch 293, loss: 2.302213\n",
      "Epoch 294, loss: 2.301832\n",
      "Epoch 295, loss: 2.302076\n",
      "Epoch 296, loss: 2.302092\n",
      "Epoch 297, loss: 2.302869\n",
      "Epoch 298, loss: 2.302266\n",
      "Epoch 299, loss: 2.302359\n",
      "Best accuracy is 0.19407407407407407 for Linear Classifier with Learning Rate 0.001 and Reg Strength 0.0001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 350\n",
    "batch_size = 300\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "# dict for saving accuracy results\n",
    "accuracy_dict = dict()\n",
    "\n",
    "# split array on train and test arrays\n",
    "np.random.seed(17)\n",
    "num_train = train_X.shape[0]\n",
    "shuffled_indices = np.arange(num_train)\n",
    "np.random.shuffle(shuffled_indices)\n",
    "split = int(num_train*0.7)\n",
    "train_indices = shuffled_indices[:split]\n",
    "test_indices = shuffled_indices[split:]\n",
    "\n",
    "for i in list(itertools.product(learning_rates, reg_strengths)):\n",
    "    acc_list = list()\n",
    "    # initialize Linear Classifier\n",
    "    classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "    classifier.fit(train_X[train_indices], train_y[train_indices], epochs=num_epochs, learning_rate=i[0], \n",
    "                   batch_size=batch_size, reg=i[1])\n",
    "    # make a prediction\n",
    "    pred = classifier.predict(train_X[test_indices])\n",
    "    accuracy = multiclass_accuracy(pred, train_y[test_indices])\n",
    "    accuracy_dict[i] = (accuracy, classifier)\n",
    "\n",
    "sort_accuracy_dict = {k: v for k, v in sorted(accuracy_dict.items(), key=lambda item: item[1][0], reverse=True)}\n",
    "best_accuracy = list(sort_accuracy_dict.values())[0][0]\n",
    "best_classifier = list(sort_accuracy_dict.values())[0][1]\n",
    "\n",
    "print(f'Best accuracy is {best_accuracy} for Linear Classifier with Learning Rate {best_learning_rate} and ' \\\n",
    "      f'Reg Strength {best_reg_strength}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.188\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print(f'Linear softmax classifier test set accuracy: {test_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
