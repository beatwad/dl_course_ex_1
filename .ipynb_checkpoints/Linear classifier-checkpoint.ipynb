{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer\n",
    "import itertools  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([[-10, 0, 10]]))\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([[1000, 0, 0]]))\n",
    "assert np.isclose(probs[0, 0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5046407076661206"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([[-5, 0, 5], [-4, 0, 6]]))\n",
    "linear_classifer.cross_entropy_loss(probs, [1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([[1, 0, 0]]), [[1]])\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, [[1]]), np.array([[1, 0, 0]], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(17)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# # Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.608786\n",
      "Epoch 1, loss: 2.578784\n",
      "Epoch 2, loss: 2.550954\n",
      "Epoch 3, loss: 2.526245\n",
      "Epoch 4, loss: 2.505394\n",
      "Epoch 5, loss: 2.483122\n",
      "Epoch 6, loss: 2.467298\n",
      "Epoch 7, loss: 2.451356\n",
      "Epoch 8, loss: 2.435842\n",
      "Epoch 9, loss: 2.424889\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f548ca4d780>]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXyU5bn/8c+VEJawL2ELAUQSlX0JCIKKgIobgiuoVK1KaRVBbWvr8fS0tfb0Z1tF3ABFRcWNxX1fQEAWDcgelU2QPSCQQAjZrt8fGU9jDDAhgSfJfN+vV17O3M/9zFzPtMx37me7zd0REZHIExV0ASIiEgwFgIhIhFIAiIhEKAWAiEiEUgCIiESoKkEXUBKNGjXy1q1bB12GiEiFsnjx4l3uHle0vUIFQOvWrUlJSQm6DBGRCsXMNhbXrl1AIiIRSgEgIhKhFAAiIhFKASAiEqEUACIiEUoBICISoRQAIiIRKiIC4KtNe5j42bqgyxARKVcq1IVgx+q1r7bw3IKN7DuYw+/OPwUzC7okEZHARUQA/PmS9uTmO4/PXkdmdh5/urgdUVEKARGJbBERAFFRxv1DOlAjJprJ8zaQmZ3L/17WiWiFgIhEsIgIAAAz496LTqNmtSqM/2QNB3PyefCqzsRER8RhEBGRn4mYAICCELjz3CRiq0bzj/e+5mB2Ho9e05XqMdFBlyYicsJF5M/fUWefzH2Xtufj1B3c8lwKmdm5QZckInLCRWQAAIzo3Zp/XdmZz9fu4vqnvyA9KyfokkRETqijBoCZJZjZLDNbbWarzGzMYfr1M7OloT6fFWofZGbfmNlaM/tDofaTzGxRqP0VM6taNpsUviu6t+CR4d34atNerntqEXsOZJ/oEkREAhPOCCAXuMvd2wG9gFvNrF3hDmZWD3gcGOzu7YErQ+3RwGPABUA7YHihdf8f8JC7twX2ADeVwfaU2EWdmjFxRHe+3p7BsEkL2ZmRFUQZIiIn3FEDwN23ufuS0OMMIBWIL9LtGmCmu28K9dsZau8JrHX39e6eDbwMXGoFV2L1B6aH+k0BhpR2Y47VgNOa8MwNPfh+TyZXT1zI1r0HgypFROSEKdExADNrDXQFFhVZlATUN7PZZrbYzH4Rao8Hvi/Ub3OorSGw191zi7QX954jzSzFzFLS0tJKUm6J9GnbiOdv6smujENcOWEBG3cfOG7vJSJSHoQdAGZWC5gBjHX39CKLqwDdgYuA84H/NrOksijQ3Se5e7K7J8fF/WxO4zLVvVUDXhrZi8zsXK6csIA1OzKO6/uJiAQprAAwsxgKvvynuvvMYrpsBj5w9wPuvguYA3QGtgAJhfq1CLXtBuqZWZUi7YHrEF+XV37VGweunrSQlVv2BV2SiMhxEc5ZQAZMBlLd/cHDdHsD6GtmVcwsFjidgmMFXwKJoTN+qgLDgDfd3YFZwBWh9a8PvUa5kNSkNtN+1ZsaMdEMf3IhizfuCbokEZEyF84IoA8wAugfOs1zqZldaGajzGwUgLunAu8Dy4EvgKfcfWVoH/9twAcUBMKr7r4q9Lp3A3ea2VoKjglMLtMtK6XWjWry6qjeNKxZlRGTFzF/3a6gSxIRKVNW8GO8YkhOTvaUlJQT+p4707O4bvIiNu7OZMJ13Tnn1MYn9P1FRErLzBa7e3LR9oi9EjhcjetU5+WRvUlsUouRz6fw3optQZckIlImFABhaFCzKi/e0otOLepx64tLmLF4c9AliYiUmgIgTHWqx/D8TT3pfXJD7pq2jBcWbgy6JBGRUlEAlEBs1SpMvr4HA05tzL2vr+TJOeuDLklE5JgpAEqoekw0E0Z056JOzbj/3VTGffwtFelAuojIjyJqQpiyEhMdxfhhXakRE824j9eQmZ3HHy84VZPNi0iFogA4RtFRxgOXdyK2ajST5qwnMzuXvw7uoMnmRaTCUACUQlSU8ZfB7YmtWoUJn60jMzuPBy7vRBXNMywiFYACoJTMjLsHnULNqtH8+6NvycrJY9zVXalaRSEgIuWbAqAMmBmjByRSo2o0f3snlYPZKTxxXXdNNi8i5Zp+ppahm89sw9+HdmT2t2nc+MyXHDikyeZFpPxSAJSxa05vyUNXdeGL735gxORF7DuoyeZFpHxSABwHQ7rG89g13VixZR/DJy1k9/5DQZckIvIzCoDjZFCHpjz5i2TWpe1n2KSF7EjXZPMiUr4oAI6jfqc0Zsove7J170GumriAzXsygy5JROT/KACOs15tGvLCzaez50A2V05YwPq0/UGXJCICKABOiK4t6/PyyN5k5+Zz5YQFLP1+b9AliYiENSdwgpnNMrPVZrbKzMYU06efme0rNGXkn0LtpxRqW2pm6WY2NrTsz2a2pfA0k2W/eeVHu+Z1mDaqN7HVohk2aQGfpO4IuiQRiXDhjABygbvcvR3QC7jVzNoV02+uu3cJ/f0VwN2/+bEN6A5kAq8VWuehQuu8W8ptKffaxNVi5q/7kNSkNrc8l8LURZpTQESCc9QAcPdt7r4k9DiDgsnd44/hvQYA69w9or/14mpX4+WRvTg7KY7/em0l//rgG91OWkQCUaJjAGbWGugKLCpmcW8zW2Zm75lZ+2KWDwNeKtJ2m5ktN7Onzax+SWqpyGKrVuHJXyQzvGcCj85ay13TlpGdmx90WSISYcIOADOrBcwAxrp7epHFS4BW7t4ZeAR4vci6VYHBwLRCzU8AJwNdgG3Avw/zviPNLMXMUtLS0sItt9yrEh3F34d25M5zk5i5ZAs3TfmSjCxdNSwiJ05YAWBmMRR8+U9195lFl7t7urvvDz1+F4gxs0aFulwALHH3HYXW2eHuee6eDzwJ9Czuvd19krsnu3tyXFxc2BtWEZgZtw9I5IErOjF/3W6unqgLxkTkxAnnLCADJgOp7v7gYfo0DfXDzHqGXnd3oS7DKbL7x8yaFXo6FFhZstIrj6uSE3j6hh5s3H2Ayx6fz9qdGUGXJCIRIJwRQB9gBNC/8CmbZjbKzEaF+lwBrDSzZcB4YJiHjmyaWU3gXKDoyOEBM1thZsuBc4A7ymKDKqqzk+J45Ve9yc7L57LH5/PFhh+CLklEKjmrSGegJCcne0pKStBlHFff/5DJ9c98weY9B3noqi5c1KnZ0VcSETkCM1vs7slF23UlcDmT0CCWmb8+g07xdbntpSVMnrch6JJEpJJSAJRD9WKr8sLNp3N+u6bc9/Zq7nt7Nfn5FWekJiIVgwKgnKoeE81j13bjhjNaM3neBka/9BVZOXlBlyUilYjmBC7HoqOM/7mkHfH1anD/u6mk7T/EkyOSqRsbE3RpIlIJaARQzpkZt5zVhvHDu7J0014unzBf8wqISJlQAFQQgzs3Z8ove7IjPYvLHp/Pqq37gi5JRCo4BUAF0vvkhsz49RlERxlXT1zI3DWV59YYInLiKQAqmKQmtXntN31oUb8GNz7zJTMWbw66JBGpoBQAFVDTutV5dVRvTm/TgLumLeOxWWt1S2kRKTEFQAVVp3oMz9zQkyFdmvPPD77h3tdXkpunW0qLSPh0GmgFVrVKFA9d3YVm9WrwxOx17EjPYvzwrsRW1f+sInJ0GgFUcGbG3YNO5b5L2/Pp1zsZ/uQidu0/FHRZIlIBKAAqiRG9WzPhuu58sz2dy5+Yz3e7DgRdkoiUcwqASuS89k158ZZepB/M4bIn5vPVpj1BlyQi5ZgCoJLp1rI+M359BrWqVWH4kwv5ePWOo68kIhFJAVAJtYmrxYxfn0FSk9qMfD6FqYs2Bl2SiJRDCoBKKq52NV4e2Yuzk+L4r9dW8q8PvtG1AiLyEwqASiy2ahWe/EUyw3sm8Oistdw1bRnZubpWQEQKhDMpfIKZzTKz1Wa2yszGFNOnn5ntKzRn8J8KLfsuNPfvUjNLKdTewMw+MrM1of/WL7vNkh9ViY7i70M7cue5ScxcsoWbpnxJRlZO0GWJSDkQzgggF7jL3dsBvYBbzaxdMf3munuX0N9fiyw7J9ReeE7KPwCfuHsi8EnouRwHZsbtAxL55xWdWLBuN1dNXMj2fVlBlyUiATtqALj7NndfEnqcAaQC8WXw3pcCU0KPpwBDyuA15QiuTE5g8g092LT7AJc8Oo8vv/sh6JJEJEAlOgZgZq2BrsCiYhb3NrNlZvaembUv1O7Ah2a22MxGFmpv4u7bQo+3A00O854jzSzFzFLS0nT749I6OymO127tU3Ca6KSFPPP5Bh0cFolQYQeAmdUCZgBj3T29yOIlQCt37ww8ArxeaFlfd+8GXEDB7qOzir62F3wDFfst5O6T3D3Z3ZPj4uLCLVeOIKlJbd64rQ/nnNqYv7y1mrGvLCUzOzfoskTkBAsrAMwshoIv/6nuPrPocndPd/f9ocfvAjFm1ij0fEvovzuB14CeodV2mFmz0Os3A3aWclukBOpUj2Hidd353fmn8OayrVz2uG4fIRJpwjkLyIDJQKq7P3iYPk1D/TCznqHX3W1mNc2sdqi9JnAesDK02pvA9aHH1wNvlGZDpOSiooxbz2nLszf2ZHt6Fpc8Oo9PUnXlsEikCGcE0AcYAfQvdJrnhWY2ysxGhfpcAaw0s2XAeGBYaLdOE2BeqP0L4B13fz+0zj+Ac81sDTAw9FwCcHZSHG/d1peWDWK5aUoKD370Lfn5Oi4gUtlZRToAmJyc7CkpKUfvKMckKyePe19fyfTFm+l3Shzjru5CvdiqQZclIqVkZouLnIYP6EpgKaR6TDT/vKITfxvSgc/X7uKSR+exauu+oMsSkeNEASA/YWZc16sVr/yqNzm5zmWPz2fmEk08L1IZKQCkWN1a1uet0X3pklCPO19dxp/eWKn7CIlUMgoAOay42tWYevPp3Nz3JJ5bsJHhTy5kR7puISFSWSgA5IiqREdx78XteGR4V1K3pXPR+HksWr876LJEpAwoACQsl3Ruzuu39qF29Spc89QiJs/TLSREKjoFgITtx1tI9D+1Mfe9vZoxL+sWEiIVmQJASqTwLSTeWr6VoY/NZ4NuISFSISkApMR+vIXElBt7siMji8GPztPk8yIVkAJAjtlZoVtItGoYy83PpfDvD78hT7eQEKkwFABSKgkNYpk+6gyu7N6CRz5dyy+f/ZK9mdlBlyUiYVAASKlVj4nmgSs6cf/QDsxfV3ALiZVbdAsJkfJOASBlwsy49vRWvBq6hcTlT8xnxmLdQkKkPFMASJnq2rI+b9/el64t63HXtGX89+u6hYRIeaUAkDLXqFY1XrjpdEae1YbnF25k2KQFbN+nW0iIlDcKADkuqkRHcc+Fp/HYNd34ensGFz+iW0iIlDcKADmuLurUjDdu7UMd3UJCpNxRAMhxlxi6hcTA0wpuIXG7biEhUi6EMyl8gpnNMrPVZrbKzMYU06efme0rNGfwn462rpn92cy2FJ5nuGw3TcqT2tVjmHBdd34/6BTe0S0kRMqFcEYAucBd7t4O6AXcambtiuk31927hP7+Gua6DxVa593SbIiUf2bGb/q15blfns7OjCwGPzKPN5Zu0S4hkYAcNQDcfZu7Lwk9zgBSgfhwXrw060rl1TexEW+N7ktik1qMeXkpI59fzE5NNCNywpXoGICZtQa6AouKWdzbzJaZ2Xtm1j7MdW8zs+Vm9rSZ1T/Me440sxQzS0lLSytJuVKOtagfy7RRZ3DvRacx59s0zn1oDjOXbNZoQOQEsnD/wZlZLeAz4H53n1lkWR0g3933h/blP+zuiUda18yaALsAB+4Dmrn7L49UQ3JysqekpIS9cVIxrE/bz++nLydl4x4GnNqY+4d2pGnd6kGXJVJpmNlid08u2h7WCMDMYoAZwNSiX/4A7p7u7vtDj98FYsys0ZHWdfcd7p7n7vnAk0DPY9guqQTaxNXilV/15r8vbsfn63Zx7kOfMS3le40GRI6zcM4CMmAykOruDx6mT9NQP8ysZ+h1dx9pXTNrVujpUGDlsW2CVAbRUcZNfU/i/TFncVrTOvxu+nJufPZLtu07GHRpIpXWUXcBmVlfYC6wAvjxpi73AC0B3H2Cmd0G/JqCs34OAne6+/zDrevu75rZ80AXCnYBfQf8yt23HakW7QKKDPn5zvMLN/KP976mSpRx78WncVVyAqHfGCJSQofbBRT2MYDyQAEQWTbtzuT3M5axcP0PnJnYiH9c3on4ejWCLkukwinVMQCRILRsGMuLN/fivkvbs3jjHs5/aA4vLtqkYwMiZUQBIOVaVJQxondrPhh7Fp1a1OWe11YwYvIXfP9DZtCliVR4CgCpEBIaxDL15tP5+9COfLVpD4PGzeH5hRvJ1xzEIsdMASAVhplxzekt+eCOs+jWqj7//fpKrn1qEZt2azQgciwUAFLhtKgfy3O/7Mk/LuvIii37OH/cHKbM/06jAZESUgBIhWRmDOvZkg/vOIueJzXgf95cxbAnF7Jxt+4wKhIuBYBUaM3r1eDZG3vwwBWdSN2Wzvnj5vD0vA0aDYiEQQEgFZ6ZcVVyAh/dcTZnnNyIv769mqsmLmB92v6gSxMp1xQAUmk0rVudydcn8+8rO/PtjgwueHguT81dT55GAyLFUgBIpWJmXN69BR/deTZnJjbib++kcuWE+azTaEDkZxQAUik1qVOdJ3+RzLiru7Au7QAXPDyXiZ+t02hApBAFgFRaZsaQrvF8dOdZ9EuK43/f+5rLnpjPmh0ZQZcmUi4oAKTSa1y7OhNHdGf88K5s2n2Ai8bP4/HZa8nNyz/6yiKVmAJAIoKZMbhzcz6842wGnNaYB97/hsuemM832zUakMilAJCIEle7Gk9c153HrunG5j0HufiRuTz66RqNBiQiKQAkIl3UqRkf3XEW57Vvyr8+/JarJi7QPYUk4igAJGI1rFWNx67pxvjhXVmzcz8XPDxHcxFLRFEASMQb3Lk57489iw7xdfnd9OXc+uIS9mZmB12WyHEXzqTwCWY2y8xWm9kqMxtTTJ9+ZrbPzJaG/v5UaNkgM/vGzNaa2R8KtZ9kZotC7a+YWdWy2yyRkomvV4MXb+nF7wedwoerdjBo3Fzmr90VdFkix1U4I4Bc4C53bwf0Am41s3bF9Jvr7l1Cf38FMLNo4DHgAqAdMLzQuv8PeMjd2wJ7gJtKuS0ipRIdZfymX1te+00fYqtFc+3kRfz93VQO5eYFXZrIcXHUAHD3be6+JPQ4A0gF4sN8/Z7AWndf7+7ZwMvApWZmQH9geqjfFGBISYsXOR46tqjL26P7ck3Plkyas56hj81n7U6dLiqVT4mOAZhZa6ArsKiYxb3NbJmZvWdm7UNt8cD3hfpsDrU1BPa6e26R9uLec6SZpZhZSlpaWknKFTlmsVWrcP/Qjjz1i2S2p2dx0fh5PLfgOx0glkol7AAws1rADGCsu6cXWbwEaOXunYFHgNfLqkB3n+Tuye6eHBcXV1YvKxKWge2a8P7YM+nVpiF/emMVv3z2S9IyDgVdlkiZCCsAzCyGgi//qe4+s+hyd0939/2hx+8CMWbWCNgCJBTq2iLUthuoZ2ZVirSLlDuNa1fn2Rt78JfB7Zm/bjeDxs3hk9QdQZclUmrhnAVkwGQg1d0fPEyfpqF+mFnP0OvuBr4EEkNn/FQFhgFvesE4ehZwReglrgfeKO3GiBwvZsb1Z7TmrdF9iatdjZumpHDv6ys4mK0DxFJxhTMC6AOMAPoXOs3zQjMbZWajQn2uAFaa2TJgPDDMC+QCtwEfUHDw+FV3XxVa527gTjNbS8ExgclluF0ix0VSk9q8cVsfbjnzJF5YuImLH5nLyi37gi5L5JhYRTqolZyc7CkpKUGXIQLAvDW7uGvaUn44kM1d553CLWe2ITrKgi5L5GfMbLG7Jxdt15XAIseob2Ij3h9zFgNPa8I/3vuaa59ayNa9B4MuSyRsCgCRUqhfsyqPX9uNB67oxIrN+xg0bg5vLdsadFkiYVEAiJSSmXFVcgLvjjmTNnG1GP3SV9z5ylIysnKCLk3kiBQAImWkVcOaTBvVm9sHJPL60i1cOH4uKd/9EHRZIoelABApQzHRUdx5bhLTRvUG4KqJC3jww2/I0YQzUg4pAESOg+6tGvDu7WcytGsLxn+6lisnLOC7XQeCLkvkJxQAIsdJ7eox/Puqzjx6TVfWp+3nwvFzeeXLTbqfkJQbCgCR4+ziTgUTznRqUZe7Z6zg1y8sYc8BTTgjwVMAiJwAzevV4MWbe/HHC07lk693MOjhOcxbowlnJFgKAJETJCrK+NXZJ/Pab/pQu3oM101exN/eXq0JZyQwCgCRE6xDfF3euq0vI3q14ql5G7j00c/5docmnJETTwEgEoAaVaO5b0gHnr4hmV37D3HxI/N4au56snN1uqicOAoAkQD1P7UJ7405i75tG/G3d1I551+zeemLTbpuQE4IBYBIwOJqV2Py9ck8c2MPGtWqyh9nruCcf83mlS8VBHJ86XbQIuWIuzP7mzQe+vhblm/eR0KDGow+J5Gh3eKJidbvNTk2h7sdtAJApBxydz79eifjPl7Dii37aNkgltH92zK0azxVFARSQgoAkQrI3fkkdScPffwtq7am06phLKP7JzKkS3MFgYRNASBSgbk7H63ewbiP17B6WzonNarJ6P5tGdxZQSBHd8wzgplZgpnNMrPVZrbKzMYcoW8PM8s1sytCz88pNI/wUjPLMrMhoWXPmtmGQsu6lGYDRSozM+O89k155/a+TBzRneox0dz56jLOe2gOr321mbz8ivNDTsqPo44AzKwZ0Mzdl5hZbWAxMMTdVxfpFw18BGQBT7v79CLLGwBrgRbunmlmzwJvF+13JBoBiBTIz3c+XL2dcR+v4evtGbSJq8mYAYlc3Km55iWWnznmEYC7b3P3JaHHGUAqEF9M19HADGDnYV7qCuA9d88Mu2oRKVZUlDGoQzPevf1Mnri2GzFRUYx5eSnnPfQZbyzdohGBhKVEOw/NrDXQFVhUpD0eGAo8cYTVhwEvFWm738yWm9lDZlbtMO850sxSzCwlLS2tJOWKVHpRUcYFHZvx3pgzeeyabkRHGWNeXsr5obmJ8xUEcgRhB4CZ1aLgF/5Yd08vsngccLe7F3vVSmg3Ukfgg0LNfwROBXoADYC7i1vX3Se5e7K7J8fFxYVbrkhEiYoyLurUjPfHnMWj13TFgNEvfcWgh+fwzvJtCgIpVlhnAZlZDPA28IG7P1jM8g3AjzseGwGZwEh3fz20fAzQ3t1HHub1+wG/dfeLj1SHjgGIhCcv33lnxTYe/vhb1qUd4JQmtRkzMJFB7ZsSpWMEEac0ZwEZMBlILe7LH8DdT3L31u7eGpgO/ObHL/+Q4RTZ/RMaFfz4+kOAlWFui4gcRXSUMbhzcz6842weHtaFnPx8fjN1CReOn8v7KzUikAJVwujTBxgBrDCzpaG2e4CWAO4+4Ugrh44bJACfFVk01cziKBg5LAVGhV21iIQlOsq4tEs8F3dqzlvLtjL+kzWMemEJpzWrw5gBiZzfvgkFv8EkEulCMJEIkpuXz1vLtzL+k7Vs2HWAds3qMHZgIue2UxBUZroSWET+T25ePm8s3cojn67hu92ZtG9eh7EDkxh4WmMFQSWkABCRn8nNy+f1UBBs3J1Jh/g63DEwif6nKggqEwWAiBxWTl4+r321hUc/XcumHzLp3KIuY89Nol9SnIKgElAAiMhR5eTl89qSLYz/dA2b9xyka8t63DEwiTMTGykIKjAFgIiELTs3n+mLN/Pop2vYui+L5Fb1ufPcJHqf3FBBUAEpAESkxA7l5vHql9/z2Kx1bE/P4vSTGnDHuUn0atMw6NKkBBQAInLMsnLyePmLTTw2ex1pGYc44+SG3HluEsmtGwRdmoRBASAipZaVk8cLCzcy4bN17NqfzZmJjbjj3CS6tawfdGlyBAoAESkzmdm5oSBYzw8Hsul3Shx3DEyic0K9oEuTYigARKTMHTiUy3MLNjJxzjr2ZuYw8LTGjB2YRIf4ukGXJoUoAETkuMnIymHK/O+YNGc96Vm5nNeuCWMHJtGueZ2gSxMUACJyAqRn5fDMvO94at56MrJyubBjU8YMSOKUprWDLi2iKQBE5ITZdzCHyfM28PS8DRzIzuWijs0YOzCRto0VBEFQAIjICbc3M5sn567nmc+/42BOHpd2bs7tAxJpE1cr6NIiigJARALzw4FsJs1Zz5T533EoN48hXeO5vX8irRvVDLq0iKAAEJHA7dp/iImfreP5hRvJyXMu6xrP6P6JtGwYG3RplZoCQETKjZ0ZWUyYvZ4XFm0kP9+5MrkFt57Tlhb1FQTHgwJARMqdHelZPD5rLS998T2Oc1VyAree05bm9WoEXVqlUppJ4RPMbJaZrTazVWY25gh9e5hZrpldUagtz8yWhv7eLNR+kpktMrO1ZvaKmVU9lg0TkYqrSZ3q/OXSDsz+XT+u7pHAqynf0++fs/mfN1ayfV9W0OVVekcdAZhZM6CZuy8xs9rAYmCIu68u0i8a+AjIAp529+mh9v3u/rND/mb2KjDT3V82swnAMnd/4ki1aAQgUrlt3pPJY7PWMi1lM1FRxrAeCYw6+2SNCErpmEcA7r7N3ZeEHmcAqUB8MV1HAzOAnWEUY0B/YHqoaQow5GjriUjl1qJ+LP97WSdm/bYfl3eL56UvNnH2P2dxz2sr2LwnM+jyKp2jBkBhZtYa6AosKtIeDwwFivsFX93MUsxsoZn9+CXfENjr7rmh55spPlQws5Gh9VPS0tJKUq6IVFAJDf4TBFf3SGB6ymb6/XM2f5ixnE27FQRlJewAMLNaFPzCH+vu6UUWjwPudvf8YlZtFRp6XAOMM7OTS1Kgu09y92R3T46LiyvJqiJSwbWoH8vfhnTks9/349rTWzLzqy2c8+/Z/HbaMjbsOhB0eRVeWGcBmVkM8Dbwgbs/WMzyDcCP88Q1AjKBke7+epF+z4ZeZwaQBjR191wz6w382d3PP1IdOgYgEtl2pGcx8bP1TF20kZy8fIZ0iefW/m05WVcWH9ExnwYa2l8/BfjB3ceG8UbPAm+7+3Qzqw9kuvshM2sELAAudffVZjYNmFHoIPByd3/8SK+tABARKLiO4Mk563lh4SaycvO4pFNzRvdvS2qzWWUAAAh6SURBVGIT3WuoOKUJgL7AXGAF8OMunnuAlgDuPqFI/2f5TwCcAUwMrRcFjHP3yaF+bYCXgQbAV8B17n7oSLUoAESksF37D/HU3A08t6DgXkMXdmzG6P5tObWpbkNdmC4EE5FK64cD2Uyet54p8zey/1Aug9o3ZfSAtrRvrolpQAEgIhFgb2Y2T3/+Hc98voGMrFwGntaEMQMS6dgisoNAASAiEWPfwYIZyibP28C+gzn0P7Uxtw9IpEuEzlmsABCRiJORlcNzCzby5Nz17M3M4aykOMYMSKR7q/pBl3ZCKQBEJGLtP5TL86Eg+OFANn3bNuL2AYn0PKlB0KWdEAoAEYl4mdm5TF24iYlz1rFrfza92jTg9gGJ9G7TkIIz3isnBYCISMjB7Dxe/GITEz5bR1rGIXq2LgiCPm0rZxAoAEREisjKyeOVL7/nidnr2J6eRbeW9bh9QCJnJ8VVqiBQAIiIHMah3DympWzmidnr2LL3IJ0T6jFmQFvOOaVxpQgCBYCIyFFk5+YzY8lmHpu1ls17DtIhvg6/6NWa8zs0pW6NmKDLO2YKABGRMOXk5fPaV1uYMHsd63cdoGp0FP1OiWNwl+YMOLUJNapGB11iiSgARERKyN1Ztnkfby7dytvLt7Iz4xA1q0ZzbrsmDO7SnDMT44iJLtG0KoFQAIiIlEJevrNow27eXLqV91ZuZ9/BHOrFxnBBh2YM7tyc009qQFRU+TxeoAAQESkj2bn5zPk2jTeXbeWj1Ts4mJNH0zrVubhTMwZ3aU7H+Lrl6uCxAkBE5DjIzM7l49SdvLl0K599u5OcPKd1w1gGd27O4C7Nads4+DkKFAAiIsfZvswc3lu5jTeXbWXB+t24w2nN6jC4c3Mu6dyMFvVjA6lLASAicgLtTM/i7eUFYbD0+70AJLeqz+AuzbmwYzMa1ap2wmpRAIiIBGTT7kzeWr6VN5du5ZsdGURHGWec3JDBnZtzfoem1Kl+fK8xUACIiJQDX29P582lW3lz2VY27zlI1SpRnHNKHIM7xzPgtMZUjyn7awxKMydwAvAc0ARwYJK7P3yYvj0omPh9WGhO4C7AE0AdIA+4391fCfV9Fjgb2Bda/QZ3X3qkWhQAIlJZuDtffb83dI3BNnbtP0StalU4r10TLunSnL5tG5XZNQalCYBmQDN3X2JmtYHFwBB3X12kXzTwEZAFPB0KgKSC7fQ1ZtY8tO5p7r638OTx4W6EAkBEKqO8fGfh+t28sXQL763cTkZWLvVjY7iwY8E1Bj1al+4ag8MFQJWjreju24BtoccZZpYKxAOri3QdDcwAehRa99tCj7ea2U4gDth7LBshIlIZRUcZfdo2ok/bRtw3pAOffVNwjcGMJZuZumgTzepW599XduaMto3K9H2PGgCFmVlroCuwqEh7PDAUOIdCAVCkT0+gKrCuUPP9ZvYn4BPgD+5+qJj1RgIjAVq2bFmSckVEKpxqVaI5r31TzmvflAOHcvk4dQdvLt1KQoOyP4U07B1MZlaLgl/4Y909vcjiccDd7p5/mHWbAc8DNxbq80fgVAoCowFwd3Hruvskd0929+S4uLhwyxURqfBqVqvCpV3imXxDj+MSAGGNAMwshoIv/6nuPrOYLsnAy6FLnxsBF5pZrru/bmZ1gHeA/3L3hT+uENq1BHDIzJ4BfluK7RARkRI6agBYwbf6ZCDV3R8sro+7n1So/7MUHNx93cyqAq8BzxU92Gtmzdx9W+j1hwArj30zRESkpMIZAfQBRgArzOzH0zTvAVoCuPuEI6x7FXAW0NDMbgi1/Xi651QziwMMWAqMKnn5IiJyrMI5C2geBV/SYXH3Gwo9fgF44TD9+of7miIiUvbK/0wGIiJyXCgAREQilAJARCRCKQBERCJUhbobqJmlARuPcfVGwK4yLKei0+fxH/osfkqfx09Vhs+jlbv/7EraChUApWFmKcXdDClS6fP4D30WP6XP46cq8+ehXUAiIhFKASAiEqEiKQAmBV1AOaPP4z/0WfyUPo+fqrSfR8QcAxARkZ+KpBGAiIgUogAQEYlQEREAZjbIzL4xs7Vm9oeg6wmKmSWY2SwzW21mq8xsTNA1lQdmFm1mX5nZ20HXEjQzq2dm083sazNLNbPeQdcUFDO7I/TvZKWZvWRm1YOuqaxV+gAITVb/GHAB0A4Ybmbtgq0qMLnAXe7eDugF3BrBn0VhY4DUoIsoJx4G3nf3U4HOROjnEprm9nYg2d07ANHAsGCrKnuVPgCAnsBad1/v7tnAy8ClAdcUCHff5u5LQo8zKPjHHR9sVcEysxbARcBTQdcSNDOrS8H8HZMB3D3b3fcGW1WgqgA1zKwKEAtsDbieMhcJARAPfF/o+WYi/EsPwMxaA12BRcFWErhxwO+BYuezjjAnAWnAM6FdYk+ZWc2giwqCu28B/gVsArYB+9z9w2CrKnuREABShJnVomCO57Hunh50PUExs4uBne6+OOhayokqQDfgCXfvChwAIvKYmZnVp2BPwUlAc6CmmV0XbFVlLxICYAuQUOh5i1BbRDKzGAq+/Ke6+8yg6wlYH2CwmX1Hwa7B/mZW7Ax2EWIzsNndfxwVTqcgECLRQGCDu6e5ew4wEzgj4JrKXCQEwJdAopmdFJqkfhjwZsA1BcLMjIL9u6nu/mDQ9QTN3f/o7i3cvTUF/7/41N0r3a+8cLn7duB7Mzsl1DQAWB1gSUHaBPQys9jQv5sBVMID4uFMCl+huXuumd0GfEDBkfyn3X1VwGUFpQ8wAlhhZktDbfe4+7sB1iTly2hgaujH0nrgxoDrCYS7LzKz6cASCs6e+4pKeEsI3QpCRCRCRcIuIBERKYYCQEQkQikAREQilAJARCRCKQBERCKUAkBEJEIpAEREItT/B1WuRdZPOkvEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.115\n",
      "Epoch 0, loss: 2.411435\n",
      "Epoch 1, loss: 2.402383\n",
      "Epoch 2, loss: 2.392385\n",
      "Epoch 3, loss: 2.384350\n",
      "Epoch 4, loss: 2.376266\n",
      "Epoch 5, loss: 2.370571\n",
      "Epoch 6, loss: 2.363494\n",
      "Epoch 7, loss: 2.358460\n",
      "Epoch 8, loss: 2.353469\n",
      "Epoch 9, loss: 2.348225\n",
      "Epoch 10, loss: 2.343689\n",
      "Epoch 11, loss: 2.340393\n",
      "Epoch 12, loss: 2.336975\n",
      "Epoch 13, loss: 2.334680\n",
      "Epoch 14, loss: 2.330778\n",
      "Epoch 15, loss: 2.328536\n",
      "Epoch 16, loss: 2.325855\n",
      "Epoch 17, loss: 2.323803\n",
      "Epoch 18, loss: 2.321549\n",
      "Epoch 19, loss: 2.320268\n",
      "Epoch 20, loss: 2.318411\n",
      "Epoch 21, loss: 2.316635\n",
      "Epoch 22, loss: 2.314989\n",
      "Epoch 23, loss: 2.314042\n",
      "Epoch 24, loss: 2.314574\n",
      "Epoch 25, loss: 2.312237\n",
      "Epoch 26, loss: 2.310462\n",
      "Epoch 27, loss: 2.311160\n",
      "Epoch 28, loss: 2.310537\n",
      "Epoch 29, loss: 2.309730\n",
      "Epoch 30, loss: 2.309156\n",
      "Epoch 31, loss: 2.308272\n",
      "Epoch 32, loss: 2.307523\n",
      "Epoch 33, loss: 2.307544\n",
      "Epoch 34, loss: 2.307470\n",
      "Epoch 35, loss: 2.305915\n",
      "Epoch 36, loss: 2.305841\n",
      "Epoch 37, loss: 2.306043\n",
      "Epoch 38, loss: 2.305413\n",
      "Epoch 39, loss: 2.304049\n",
      "Epoch 40, loss: 2.304210\n",
      "Epoch 41, loss: 2.305435\n",
      "Epoch 42, loss: 2.304918\n",
      "Epoch 43, loss: 2.303742\n",
      "Epoch 44, loss: 2.304399\n",
      "Epoch 45, loss: 2.304424\n",
      "Epoch 46, loss: 2.304119\n",
      "Epoch 47, loss: 2.303987\n",
      "Epoch 48, loss: 2.303519\n",
      "Epoch 49, loss: 2.303328\n",
      "Epoch 50, loss: 2.303856\n",
      "Epoch 51, loss: 2.302821\n",
      "Epoch 52, loss: 2.303799\n",
      "Epoch 53, loss: 2.303374\n",
      "Epoch 54, loss: 2.302641\n",
      "Epoch 55, loss: 2.303469\n",
      "Epoch 56, loss: 2.302970\n",
      "Epoch 57, loss: 2.303289\n",
      "Epoch 58, loss: 2.302713\n",
      "Epoch 59, loss: 2.303074\n",
      "Epoch 60, loss: 2.303329\n",
      "Epoch 61, loss: 2.303090\n",
      "Epoch 62, loss: 2.302834\n",
      "Epoch 63, loss: 2.302505\n",
      "Epoch 64, loss: 2.302710\n",
      "Epoch 65, loss: 2.302430\n",
      "Epoch 66, loss: 2.303355\n",
      "Epoch 67, loss: 2.303159\n",
      "Epoch 68, loss: 2.302894\n",
      "Epoch 69, loss: 2.302487\n",
      "Epoch 70, loss: 2.302503\n",
      "Epoch 71, loss: 2.302982\n",
      "Epoch 72, loss: 2.302529\n",
      "Epoch 73, loss: 2.302272\n",
      "Epoch 74, loss: 2.302781\n",
      "Epoch 75, loss: 2.302474\n",
      "Epoch 76, loss: 2.302235\n",
      "Epoch 77, loss: 2.302601\n",
      "Epoch 78, loss: 2.302057\n",
      "Epoch 79, loss: 2.302418\n",
      "Epoch 80, loss: 2.302225\n",
      "Epoch 81, loss: 2.303092\n",
      "Epoch 82, loss: 2.302851\n",
      "Epoch 83, loss: 2.302235\n",
      "Epoch 84, loss: 2.302633\n",
      "Epoch 85, loss: 2.302329\n",
      "Epoch 86, loss: 2.302941\n",
      "Epoch 87, loss: 2.302676\n",
      "Epoch 88, loss: 2.302393\n",
      "Epoch 89, loss: 2.302554\n",
      "Epoch 90, loss: 2.302069\n",
      "Epoch 91, loss: 2.302260\n",
      "Epoch 92, loss: 2.303130\n",
      "Epoch 93, loss: 2.302801\n",
      "Epoch 94, loss: 2.302465\n",
      "Epoch 95, loss: 2.302971\n",
      "Epoch 96, loss: 2.302868\n",
      "Epoch 97, loss: 2.302421\n",
      "Epoch 98, loss: 2.302081\n",
      "Epoch 99, loss: 2.302757\n",
      "Accuracy after training for 100 epochs:  0.149\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.303107\n",
      "Epoch 1, loss: 2.298352\n",
      "Epoch 2, loss: 2.291409\n",
      "Epoch 3, loss: 2.321569\n",
      "Epoch 4, loss: 2.298574\n",
      "Epoch 5, loss: 2.298739\n",
      "Epoch 6, loss: 2.284277\n",
      "Epoch 7, loss: 2.308537\n",
      "Epoch 8, loss: 2.290149\n",
      "Epoch 9, loss: 2.295892\n",
      "Epoch 10, loss: 2.301409\n",
      "Epoch 11, loss: 2.293976\n",
      "Epoch 12, loss: 2.304407\n",
      "Epoch 13, loss: 2.303759\n",
      "Epoch 14, loss: 2.288359\n",
      "Epoch 15, loss: 2.288554\n",
      "Epoch 16, loss: 2.294052\n",
      "Epoch 17, loss: 2.284230\n",
      "Epoch 18, loss: 2.290522\n",
      "Epoch 19, loss: 2.288011\n",
      "Epoch 20, loss: 2.286332\n",
      "Epoch 21, loss: 2.279167\n",
      "Epoch 22, loss: 2.275416\n",
      "Epoch 23, loss: 2.303829\n",
      "Epoch 24, loss: 2.282042\n",
      "Epoch 25, loss: 2.273066\n",
      "Epoch 26, loss: 2.283080\n",
      "Epoch 27, loss: 2.282230\n",
      "Epoch 28, loss: 2.264282\n",
      "Epoch 29, loss: 2.278269\n",
      "Epoch 30, loss: 2.286192\n",
      "Epoch 31, loss: 2.277216\n",
      "Epoch 32, loss: 2.288335\n",
      "Epoch 33, loss: 2.277958\n",
      "Epoch 34, loss: 2.266385\n",
      "Epoch 35, loss: 2.278299\n",
      "Epoch 36, loss: 2.270331\n",
      "Epoch 37, loss: 2.266881\n",
      "Epoch 38, loss: 2.271995\n",
      "Epoch 39, loss: 2.273577\n",
      "Epoch 40, loss: 2.267328\n",
      "Epoch 41, loss: 2.287928\n",
      "Epoch 42, loss: 2.257745\n",
      "Epoch 43, loss: 2.274490\n",
      "Epoch 44, loss: 2.258807\n",
      "Epoch 45, loss: 2.268534\n",
      "Epoch 46, loss: 2.265381\n",
      "Epoch 47, loss: 2.262722\n",
      "Epoch 48, loss: 2.261960\n",
      "Epoch 49, loss: 2.255511\n",
      "Epoch 50, loss: 2.268983\n",
      "Epoch 51, loss: 2.270267\n",
      "Epoch 52, loss: 2.259615\n",
      "Epoch 53, loss: 2.254547\n",
      "Epoch 54, loss: 2.247037\n",
      "Epoch 55, loss: 2.257762\n",
      "Epoch 56, loss: 2.262293\n",
      "Epoch 57, loss: 2.258845\n",
      "Epoch 58, loss: 2.273662\n",
      "Epoch 59, loss: 2.259853\n",
      "Epoch 60, loss: 2.258283\n",
      "Epoch 61, loss: 2.256708\n",
      "Epoch 62, loss: 2.251635\n",
      "Epoch 63, loss: 2.260557\n",
      "Epoch 64, loss: 2.253293\n",
      "Epoch 65, loss: 2.251415\n",
      "Epoch 66, loss: 2.252173\n",
      "Epoch 67, loss: 2.255716\n",
      "Epoch 68, loss: 2.249346\n",
      "Epoch 69, loss: 2.261876\n",
      "Epoch 70, loss: 2.260815\n",
      "Epoch 71, loss: 2.263650\n",
      "Epoch 72, loss: 2.243289\n",
      "Epoch 73, loss: 2.238647\n",
      "Epoch 74, loss: 2.236572\n",
      "Epoch 75, loss: 2.249928\n",
      "Epoch 76, loss: 2.247375\n",
      "Epoch 77, loss: 2.252033\n",
      "Epoch 78, loss: 2.246844\n",
      "Epoch 79, loss: 2.256229\n",
      "Epoch 80, loss: 2.246443\n",
      "Epoch 81, loss: 2.257437\n",
      "Epoch 82, loss: 2.247173\n",
      "Epoch 83, loss: 2.252588\n",
      "Epoch 84, loss: 2.241730\n",
      "Epoch 85, loss: 2.267598\n",
      "Epoch 86, loss: 2.240600\n",
      "Epoch 87, loss: 2.250043\n",
      "Epoch 88, loss: 2.255146\n",
      "Epoch 89, loss: 2.253007\n",
      "Epoch 90, loss: 2.249105\n",
      "Epoch 91, loss: 2.233879\n",
      "Epoch 92, loss: 2.250796\n",
      "Epoch 93, loss: 2.245856\n",
      "Epoch 94, loss: 2.254267\n",
      "Epoch 95, loss: 2.256212\n",
      "Epoch 96, loss: 2.249433\n",
      "Epoch 97, loss: 2.250689\n",
      "Epoch 98, loss: 2.248075\n",
      "Epoch 99, loss: 2.252979\n",
      "Epoch 100, loss: 2.243911\n",
      "Epoch 101, loss: 2.232065\n",
      "Epoch 102, loss: 2.247063\n",
      "Epoch 103, loss: 2.245022\n",
      "Epoch 104, loss: 2.239642\n",
      "Epoch 105, loss: 2.242991\n",
      "Epoch 106, loss: 2.238263\n",
      "Epoch 107, loss: 2.233985\n",
      "Epoch 108, loss: 2.243913\n",
      "Epoch 109, loss: 2.246035\n",
      "Epoch 110, loss: 2.244642\n",
      "Epoch 111, loss: 2.239741\n",
      "Epoch 112, loss: 2.231473\n",
      "Epoch 113, loss: 2.243406\n",
      "Epoch 114, loss: 2.240707\n",
      "Epoch 115, loss: 2.238169\n",
      "Epoch 116, loss: 2.243376\n",
      "Epoch 117, loss: 2.245812\n",
      "Epoch 118, loss: 2.231234\n",
      "Epoch 119, loss: 2.228973\n",
      "Epoch 120, loss: 2.238360\n",
      "Epoch 121, loss: 2.239446\n",
      "Epoch 122, loss: 2.236427\n",
      "Epoch 123, loss: 2.234783\n",
      "Epoch 124, loss: 2.237681\n",
      "Epoch 125, loss: 2.237724\n",
      "Epoch 126, loss: 2.231941\n",
      "Epoch 127, loss: 2.243370\n",
      "Epoch 128, loss: 2.244744\n",
      "Epoch 129, loss: 2.245992\n",
      "Epoch 130, loss: 2.228655\n",
      "Epoch 131, loss: 2.230547\n",
      "Epoch 132, loss: 2.244142\n",
      "Epoch 133, loss: 2.231453\n",
      "Epoch 134, loss: 2.235306\n",
      "Epoch 135, loss: 2.240544\n",
      "Epoch 136, loss: 2.234076\n",
      "Epoch 137, loss: 2.228146\n",
      "Epoch 138, loss: 2.235050\n",
      "Epoch 139, loss: 2.241938\n",
      "Epoch 140, loss: 2.241774\n",
      "Epoch 141, loss: 2.241929\n",
      "Epoch 142, loss: 2.247982\n",
      "Epoch 143, loss: 2.232386\n",
      "Epoch 144, loss: 2.220344\n",
      "Epoch 145, loss: 2.221891\n",
      "Epoch 146, loss: 2.246145\n",
      "Epoch 147, loss: 2.232102\n",
      "Epoch 148, loss: 2.234690\n",
      "Epoch 149, loss: 2.236935\n",
      "Epoch 150, loss: 2.239905\n",
      "Epoch 151, loss: 2.227910\n",
      "Epoch 152, loss: 2.238339\n",
      "Epoch 153, loss: 2.222563\n",
      "Epoch 154, loss: 2.252438\n",
      "Epoch 155, loss: 2.234125\n",
      "Epoch 156, loss: 2.233293\n",
      "Epoch 157, loss: 2.228257\n",
      "Epoch 158, loss: 2.228914\n",
      "Epoch 159, loss: 2.233286\n",
      "Epoch 160, loss: 2.222173\n",
      "Epoch 161, loss: 2.231784\n",
      "Epoch 162, loss: 2.233168\n",
      "Epoch 163, loss: 2.242192\n",
      "Epoch 164, loss: 2.232673\n",
      "Epoch 165, loss: 2.248771\n",
      "Epoch 166, loss: 2.230159\n",
      "Epoch 167, loss: 2.233184\n",
      "Epoch 168, loss: 2.235307\n",
      "Epoch 169, loss: 2.228999\n",
      "Epoch 170, loss: 2.228408\n",
      "Epoch 171, loss: 2.233761\n",
      "Epoch 172, loss: 2.237224\n",
      "Epoch 173, loss: 2.234365\n",
      "Epoch 174, loss: 2.227838\n",
      "Epoch 175, loss: 2.232701\n",
      "Epoch 176, loss: 2.221332\n",
      "Epoch 177, loss: 2.238331\n",
      "Epoch 178, loss: 2.231400\n",
      "Epoch 179, loss: 2.223506\n",
      "Epoch 180, loss: 2.241166\n",
      "Epoch 181, loss: 2.235678\n",
      "Epoch 182, loss: 2.216966\n",
      "Epoch 183, loss: 2.232645\n",
      "Epoch 184, loss: 2.258590\n",
      "Epoch 185, loss: 2.222855\n",
      "Epoch 186, loss: 2.227675\n",
      "Epoch 187, loss: 2.225568\n",
      "Epoch 188, loss: 2.235169\n",
      "Epoch 189, loss: 2.240773\n",
      "Epoch 190, loss: 2.229003\n",
      "Epoch 191, loss: 2.228322\n",
      "Epoch 192, loss: 2.226034\n",
      "Epoch 193, loss: 2.224791\n",
      "Epoch 194, loss: 2.233353\n",
      "Epoch 195, loss: 2.247823\n",
      "Epoch 196, loss: 2.233467\n",
      "Epoch 197, loss: 2.226744\n",
      "Epoch 198, loss: 2.230742\n",
      "Epoch 199, loss: 2.230497\n",
      "Epoch 0, loss: 2.302212\n",
      "Epoch 1, loss: 2.297096\n",
      "Epoch 2, loss: 2.315878\n",
      "Epoch 3, loss: 2.299490\n",
      "Epoch 4, loss: 2.304140\n",
      "Epoch 5, loss: 2.296848\n",
      "Epoch 6, loss: 2.299781\n",
      "Epoch 7, loss: 2.298962\n",
      "Epoch 8, loss: 2.299605\n",
      "Epoch 9, loss: 2.289225\n",
      "Epoch 10, loss: 2.296644\n",
      "Epoch 11, loss: 2.295644\n",
      "Epoch 12, loss: 2.286427\n",
      "Epoch 13, loss: 2.291896\n",
      "Epoch 14, loss: 2.287287\n",
      "Epoch 15, loss: 2.297521\n",
      "Epoch 16, loss: 2.288415\n",
      "Epoch 17, loss: 2.292083\n",
      "Epoch 18, loss: 2.281081\n",
      "Epoch 19, loss: 2.288908\n",
      "Epoch 20, loss: 2.289490\n",
      "Epoch 21, loss: 2.283615\n",
      "Epoch 22, loss: 2.292671\n",
      "Epoch 23, loss: 2.276714\n",
      "Epoch 24, loss: 2.287948\n",
      "Epoch 25, loss: 2.274129\n",
      "Epoch 26, loss: 2.286065\n",
      "Epoch 27, loss: 2.270725\n",
      "Epoch 28, loss: 2.284259\n",
      "Epoch 29, loss: 2.280951\n",
      "Epoch 30, loss: 2.287413\n",
      "Epoch 31, loss: 2.279786\n",
      "Epoch 32, loss: 2.266729\n",
      "Epoch 33, loss: 2.275170\n",
      "Epoch 34, loss: 2.279496\n",
      "Epoch 35, loss: 2.266576\n",
      "Epoch 36, loss: 2.264725\n",
      "Epoch 37, loss: 2.270422\n",
      "Epoch 38, loss: 2.271129\n",
      "Epoch 39, loss: 2.271696\n",
      "Epoch 40, loss: 2.264532\n",
      "Epoch 41, loss: 2.274401\n",
      "Epoch 42, loss: 2.275541\n",
      "Epoch 43, loss: 2.267154\n",
      "Epoch 44, loss: 2.260149\n",
      "Epoch 45, loss: 2.269328\n",
      "Epoch 46, loss: 2.265734\n",
      "Epoch 47, loss: 2.271554\n",
      "Epoch 48, loss: 2.259102\n",
      "Epoch 49, loss: 2.261629\n",
      "Epoch 50, loss: 2.258313\n",
      "Epoch 51, loss: 2.262396\n",
      "Epoch 52, loss: 2.258263\n",
      "Epoch 53, loss: 2.272955\n",
      "Epoch 54, loss: 2.255352\n",
      "Epoch 55, loss: 2.256291\n",
      "Epoch 56, loss: 2.244096\n",
      "Epoch 57, loss: 2.255569\n",
      "Epoch 58, loss: 2.259989\n",
      "Epoch 59, loss: 2.264826\n",
      "Epoch 60, loss: 2.257567\n",
      "Epoch 61, loss: 2.258131\n",
      "Epoch 62, loss: 2.252515\n",
      "Epoch 63, loss: 2.272684\n",
      "Epoch 64, loss: 2.273447\n",
      "Epoch 65, loss: 2.241522\n",
      "Epoch 66, loss: 2.256952\n",
      "Epoch 67, loss: 2.252897\n",
      "Epoch 68, loss: 2.251777\n",
      "Epoch 69, loss: 2.243471\n",
      "Epoch 70, loss: 2.263628\n",
      "Epoch 71, loss: 2.245998\n",
      "Epoch 72, loss: 2.265221\n",
      "Epoch 73, loss: 2.253423\n",
      "Epoch 74, loss: 2.249239\n",
      "Epoch 75, loss: 2.242706\n",
      "Epoch 76, loss: 2.261364\n",
      "Epoch 77, loss: 2.247895\n",
      "Epoch 78, loss: 2.253739\n",
      "Epoch 79, loss: 2.241697\n",
      "Epoch 80, loss: 2.251094\n",
      "Epoch 81, loss: 2.241731\n",
      "Epoch 82, loss: 2.239726\n",
      "Epoch 83, loss: 2.261869\n",
      "Epoch 84, loss: 2.242128\n",
      "Epoch 85, loss: 2.245940\n",
      "Epoch 86, loss: 2.264311\n",
      "Epoch 87, loss: 2.231048\n",
      "Epoch 88, loss: 2.273267\n",
      "Epoch 89, loss: 2.248513\n",
      "Epoch 90, loss: 2.235334\n",
      "Epoch 91, loss: 2.247982\n",
      "Epoch 92, loss: 2.254304\n",
      "Epoch 93, loss: 2.249092\n",
      "Epoch 94, loss: 2.247549\n",
      "Epoch 95, loss: 2.246911\n",
      "Epoch 96, loss: 2.235122\n",
      "Epoch 97, loss: 2.238935\n",
      "Epoch 98, loss: 2.252525\n",
      "Epoch 99, loss: 2.246966\n",
      "Epoch 100, loss: 2.238430\n",
      "Epoch 101, loss: 2.242767\n",
      "Epoch 102, loss: 2.239468\n",
      "Epoch 103, loss: 2.249197\n",
      "Epoch 104, loss: 2.225078\n",
      "Epoch 105, loss: 2.253844\n",
      "Epoch 106, loss: 2.231215\n",
      "Epoch 107, loss: 2.234344\n",
      "Epoch 108, loss: 2.254398\n",
      "Epoch 109, loss: 2.238247\n",
      "Epoch 110, loss: 2.247223\n",
      "Epoch 111, loss: 2.238905\n",
      "Epoch 112, loss: 2.222027\n",
      "Epoch 113, loss: 2.261029\n",
      "Epoch 114, loss: 2.237016\n",
      "Epoch 115, loss: 2.231255\n",
      "Epoch 116, loss: 2.256102\n",
      "Epoch 117, loss: 2.234081\n",
      "Epoch 118, loss: 2.232226\n",
      "Epoch 119, loss: 2.230642\n",
      "Epoch 120, loss: 2.238372\n",
      "Epoch 121, loss: 2.242352\n",
      "Epoch 122, loss: 2.240440\n",
      "Epoch 123, loss: 2.231748\n",
      "Epoch 124, loss: 2.233887\n",
      "Epoch 125, loss: 2.244774\n",
      "Epoch 126, loss: 2.256046\n",
      "Epoch 127, loss: 2.250244\n",
      "Epoch 128, loss: 2.220215\n",
      "Epoch 129, loss: 2.241307\n",
      "Epoch 130, loss: 2.234608\n",
      "Epoch 131, loss: 2.239141\n",
      "Epoch 132, loss: 2.239846\n",
      "Epoch 133, loss: 2.243222\n",
      "Epoch 134, loss: 2.245566\n",
      "Epoch 135, loss: 2.238096\n",
      "Epoch 136, loss: 2.243080\n",
      "Epoch 137, loss: 2.226799\n",
      "Epoch 138, loss: 2.231302\n",
      "Epoch 139, loss: 2.228929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140, loss: 2.237859\n",
      "Epoch 141, loss: 2.234055\n",
      "Epoch 142, loss: 2.239790\n",
      "Epoch 143, loss: 2.229586\n",
      "Epoch 144, loss: 2.232246\n",
      "Epoch 145, loss: 2.251027\n",
      "Epoch 146, loss: 2.226108\n",
      "Epoch 147, loss: 2.236912\n",
      "Epoch 148, loss: 2.240498\n",
      "Epoch 149, loss: 2.230324\n",
      "Epoch 150, loss: 2.233000\n",
      "Epoch 151, loss: 2.233503\n",
      "Epoch 152, loss: 2.230587\n",
      "Epoch 153, loss: 2.233577\n",
      "Epoch 154, loss: 2.241310\n",
      "Epoch 155, loss: 2.226446\n",
      "Epoch 156, loss: 2.224038\n",
      "Epoch 157, loss: 2.230352\n",
      "Epoch 158, loss: 2.234371\n",
      "Epoch 159, loss: 2.233152\n",
      "Epoch 160, loss: 2.232825\n",
      "Epoch 161, loss: 2.228742\n",
      "Epoch 162, loss: 2.229542\n",
      "Epoch 163, loss: 2.225186\n",
      "Epoch 164, loss: 2.232754\n",
      "Epoch 165, loss: 2.226924\n",
      "Epoch 166, loss: 2.216933\n",
      "Epoch 167, loss: 2.246819\n",
      "Epoch 168, loss: 2.217168\n",
      "Epoch 169, loss: 2.240339\n",
      "Epoch 170, loss: 2.239872\n",
      "Epoch 171, loss: 2.221388\n",
      "Epoch 172, loss: 2.223244\n",
      "Epoch 173, loss: 2.235949\n",
      "Epoch 174, loss: 2.224187\n",
      "Epoch 175, loss: 2.237053\n",
      "Epoch 176, loss: 2.236634\n",
      "Epoch 177, loss: 2.226368\n",
      "Epoch 178, loss: 2.226284\n",
      "Epoch 179, loss: 2.222649\n",
      "Epoch 180, loss: 2.236474\n",
      "Epoch 181, loss: 2.236391\n",
      "Epoch 182, loss: 2.228264\n",
      "Epoch 183, loss: 2.226920\n",
      "Epoch 184, loss: 2.246024\n",
      "Epoch 185, loss: 2.228223\n",
      "Epoch 186, loss: 2.227691\n",
      "Epoch 187, loss: 2.219726\n",
      "Epoch 188, loss: 2.214234\n",
      "Epoch 189, loss: 2.245401\n",
      "Epoch 190, loss: 2.227828\n",
      "Epoch 191, loss: 2.232183\n",
      "Epoch 192, loss: 2.229264\n",
      "Epoch 193, loss: 2.234337\n",
      "Epoch 194, loss: 2.252653\n",
      "Epoch 195, loss: 2.225384\n",
      "Epoch 196, loss: 2.219590\n",
      "Epoch 197, loss: 2.228658\n",
      "Epoch 198, loss: 2.227387\n",
      "Epoch 199, loss: 2.233822\n",
      "Epoch 0, loss: 2.301658\n",
      "Epoch 1, loss: 2.301413\n",
      "Epoch 2, loss: 2.294846\n",
      "Epoch 3, loss: 2.311437\n",
      "Epoch 4, loss: 2.295165\n",
      "Epoch 5, loss: 2.307086\n",
      "Epoch 6, loss: 2.306233\n",
      "Epoch 7, loss: 2.303255\n",
      "Epoch 8, loss: 2.308445\n",
      "Epoch 9, loss: 2.313044\n",
      "Epoch 10, loss: 2.289682\n",
      "Epoch 11, loss: 2.298653\n",
      "Epoch 12, loss: 2.285167\n",
      "Epoch 13, loss: 2.290193\n",
      "Epoch 14, loss: 2.289328\n",
      "Epoch 15, loss: 2.293251\n",
      "Epoch 16, loss: 2.276125\n",
      "Epoch 17, loss: 2.302942\n",
      "Epoch 18, loss: 2.279703\n",
      "Epoch 19, loss: 2.280969\n",
      "Epoch 20, loss: 2.288738\n",
      "Epoch 21, loss: 2.287419\n",
      "Epoch 22, loss: 2.277798\n",
      "Epoch 23, loss: 2.272750\n",
      "Epoch 24, loss: 2.280731\n",
      "Epoch 25, loss: 2.291107\n",
      "Epoch 26, loss: 2.280612\n",
      "Epoch 27, loss: 2.274096\n",
      "Epoch 28, loss: 2.279279\n",
      "Epoch 29, loss: 2.281760\n",
      "Epoch 30, loss: 2.269042\n",
      "Epoch 31, loss: 2.284543\n",
      "Epoch 32, loss: 2.272131\n",
      "Epoch 33, loss: 2.273450\n",
      "Epoch 34, loss: 2.264624\n",
      "Epoch 35, loss: 2.268659\n",
      "Epoch 36, loss: 2.295650\n",
      "Epoch 37, loss: 2.266661\n",
      "Epoch 38, loss: 2.268719\n",
      "Epoch 39, loss: 2.276548\n",
      "Epoch 40, loss: 2.276467\n",
      "Epoch 41, loss: 2.277078\n",
      "Epoch 42, loss: 2.268027\n",
      "Epoch 43, loss: 2.264194\n",
      "Epoch 44, loss: 2.277005\n",
      "Epoch 45, loss: 2.270010\n",
      "Epoch 46, loss: 2.264993\n",
      "Epoch 47, loss: 2.262210\n",
      "Epoch 48, loss: 2.260365\n",
      "Epoch 49, loss: 2.263351\n",
      "Epoch 50, loss: 2.263206\n",
      "Epoch 51, loss: 2.259955\n",
      "Epoch 52, loss: 2.261155\n",
      "Epoch 53, loss: 2.268415\n",
      "Epoch 54, loss: 2.262079\n",
      "Epoch 55, loss: 2.262836\n",
      "Epoch 56, loss: 2.267934\n",
      "Epoch 57, loss: 2.265360\n",
      "Epoch 58, loss: 2.260977\n",
      "Epoch 59, loss: 2.255661\n",
      "Epoch 60, loss: 2.265178\n",
      "Epoch 61, loss: 2.256746\n",
      "Epoch 62, loss: 2.249909\n",
      "Epoch 63, loss: 2.247316\n",
      "Epoch 64, loss: 2.254038\n",
      "Epoch 65, loss: 2.259789\n",
      "Epoch 66, loss: 2.248036\n",
      "Epoch 67, loss: 2.243277\n",
      "Epoch 68, loss: 2.249906\n",
      "Epoch 69, loss: 2.261325\n",
      "Epoch 70, loss: 2.246856\n",
      "Epoch 71, loss: 2.237042\n",
      "Epoch 72, loss: 2.258646\n",
      "Epoch 73, loss: 2.254345\n",
      "Epoch 74, loss: 2.252970\n",
      "Epoch 75, loss: 2.245102\n",
      "Epoch 76, loss: 2.251812\n",
      "Epoch 77, loss: 2.245606\n",
      "Epoch 78, loss: 2.255250\n",
      "Epoch 79, loss: 2.259966\n",
      "Epoch 80, loss: 2.240196\n",
      "Epoch 81, loss: 2.254731\n",
      "Epoch 82, loss: 2.248073\n",
      "Epoch 83, loss: 2.253991\n",
      "Epoch 84, loss: 2.247882\n",
      "Epoch 85, loss: 2.237914\n",
      "Epoch 86, loss: 2.260947\n",
      "Epoch 87, loss: 2.247997\n",
      "Epoch 88, loss: 2.247103\n",
      "Epoch 89, loss: 2.254104\n",
      "Epoch 90, loss: 2.245739\n",
      "Epoch 91, loss: 2.247237\n",
      "Epoch 92, loss: 2.233334\n",
      "Epoch 93, loss: 2.257063\n",
      "Epoch 94, loss: 2.242283\n",
      "Epoch 95, loss: 2.229621\n",
      "Epoch 96, loss: 2.243905\n",
      "Epoch 97, loss: 2.247361\n",
      "Epoch 98, loss: 2.245110\n",
      "Epoch 99, loss: 2.244864\n",
      "Epoch 100, loss: 2.244473\n",
      "Epoch 101, loss: 2.248546\n",
      "Epoch 102, loss: 2.241539\n",
      "Epoch 103, loss: 2.255825\n",
      "Epoch 104, loss: 2.256728\n",
      "Epoch 105, loss: 2.248652\n",
      "Epoch 106, loss: 2.241453\n",
      "Epoch 107, loss: 2.238081\n",
      "Epoch 108, loss: 2.244350\n",
      "Epoch 109, loss: 2.232073\n",
      "Epoch 110, loss: 2.237423\n",
      "Epoch 111, loss: 2.233437\n",
      "Epoch 112, loss: 2.250974\n",
      "Epoch 113, loss: 2.237484\n",
      "Epoch 114, loss: 2.242743\n",
      "Epoch 115, loss: 2.231806\n",
      "Epoch 116, loss: 2.237052\n",
      "Epoch 117, loss: 2.256641\n",
      "Epoch 118, loss: 2.236234\n",
      "Epoch 119, loss: 2.239121\n",
      "Epoch 120, loss: 2.240092\n",
      "Epoch 121, loss: 2.247788\n",
      "Epoch 122, loss: 2.244238\n",
      "Epoch 123, loss: 2.249527\n",
      "Epoch 124, loss: 2.231856\n",
      "Epoch 125, loss: 2.249541\n",
      "Epoch 126, loss: 2.234976\n",
      "Epoch 127, loss: 2.244888\n",
      "Epoch 128, loss: 2.231028\n",
      "Epoch 129, loss: 2.236510\n",
      "Epoch 130, loss: 2.230883\n",
      "Epoch 131, loss: 2.241545\n",
      "Epoch 132, loss: 2.231269\n",
      "Epoch 133, loss: 2.228036\n",
      "Epoch 134, loss: 2.233660\n",
      "Epoch 135, loss: 2.231712\n",
      "Epoch 136, loss: 2.234533\n",
      "Epoch 137, loss: 2.240974\n",
      "Epoch 138, loss: 2.234139\n",
      "Epoch 139, loss: 2.244079\n",
      "Epoch 140, loss: 2.226123\n",
      "Epoch 141, loss: 2.244885\n",
      "Epoch 142, loss: 2.229426\n",
      "Epoch 143, loss: 2.233697\n",
      "Epoch 144, loss: 2.238706\n",
      "Epoch 145, loss: 2.242010\n",
      "Epoch 146, loss: 2.225916\n",
      "Epoch 147, loss: 2.233842\n",
      "Epoch 148, loss: 2.235329\n",
      "Epoch 149, loss: 2.232627\n",
      "Epoch 150, loss: 2.231504\n",
      "Epoch 151, loss: 2.235367\n",
      "Epoch 152, loss: 2.237350\n",
      "Epoch 153, loss: 2.239569\n",
      "Epoch 154, loss: 2.231561\n",
      "Epoch 155, loss: 2.233100\n",
      "Epoch 156, loss: 2.235771\n",
      "Epoch 157, loss: 2.236729\n",
      "Epoch 158, loss: 2.220381\n",
      "Epoch 159, loss: 2.228717\n",
      "Epoch 160, loss: 2.243077\n",
      "Epoch 161, loss: 2.243475\n",
      "Epoch 162, loss: 2.229423\n",
      "Epoch 163, loss: 2.230261\n",
      "Epoch 164, loss: 2.234163\n",
      "Epoch 165, loss: 2.215528\n",
      "Epoch 166, loss: 2.227926\n",
      "Epoch 167, loss: 2.250060\n",
      "Epoch 168, loss: 2.242281\n",
      "Epoch 169, loss: 2.227807\n",
      "Epoch 170, loss: 2.225011\n",
      "Epoch 171, loss: 2.227689\n",
      "Epoch 172, loss: 2.231160\n",
      "Epoch 173, loss: 2.217396\n",
      "Epoch 174, loss: 2.239054\n",
      "Epoch 175, loss: 2.243887\n",
      "Epoch 176, loss: 2.221198\n",
      "Epoch 177, loss: 2.237767\n",
      "Epoch 178, loss: 2.228808\n",
      "Epoch 179, loss: 2.227241\n",
      "Epoch 180, loss: 2.218110\n",
      "Epoch 181, loss: 2.231063\n",
      "Epoch 182, loss: 2.258189\n",
      "Epoch 183, loss: 2.231615\n",
      "Epoch 184, loss: 2.206454\n",
      "Epoch 185, loss: 2.238951\n",
      "Epoch 186, loss: 2.229410\n",
      "Epoch 187, loss: 2.234705\n",
      "Epoch 188, loss: 2.213223\n",
      "Epoch 189, loss: 2.232603\n",
      "Epoch 190, loss: 2.233084\n",
      "Epoch 191, loss: 2.235264\n",
      "Epoch 192, loss: 2.231387\n",
      "Epoch 193, loss: 2.219651\n",
      "Epoch 194, loss: 2.232388\n",
      "Epoch 195, loss: 2.243158\n",
      "Epoch 196, loss: 2.219264\n",
      "Epoch 197, loss: 2.228770\n",
      "Epoch 198, loss: 2.220501\n",
      "Epoch 199, loss: 2.243311\n",
      "Epoch 0, loss: 2.302276\n",
      "Epoch 1, loss: 2.302872\n",
      "Epoch 2, loss: 2.299938\n",
      "Epoch 3, loss: 2.300491\n",
      "Epoch 4, loss: 2.304151\n",
      "Epoch 5, loss: 2.299043\n",
      "Epoch 6, loss: 2.302409\n",
      "Epoch 7, loss: 2.297815\n",
      "Epoch 8, loss: 2.299849\n",
      "Epoch 9, loss: 2.304284\n",
      "Epoch 10, loss: 2.299771\n",
      "Epoch 11, loss: 2.298344\n",
      "Epoch 12, loss: 2.300429\n",
      "Epoch 13, loss: 2.296625\n",
      "Epoch 14, loss: 2.294575\n",
      "Epoch 15, loss: 2.294502\n",
      "Epoch 16, loss: 2.298778\n",
      "Epoch 17, loss: 2.298439\n",
      "Epoch 18, loss: 2.299941\n",
      "Epoch 19, loss: 2.291389\n",
      "Epoch 20, loss: 2.290077\n",
      "Epoch 21, loss: 2.296607\n",
      "Epoch 22, loss: 2.291448\n",
      "Epoch 23, loss: 2.298406\n",
      "Epoch 24, loss: 2.295414\n",
      "Epoch 25, loss: 2.299327\n",
      "Epoch 26, loss: 2.294109\n",
      "Epoch 27, loss: 2.292069\n",
      "Epoch 28, loss: 2.289494\n",
      "Epoch 29, loss: 2.289842\n",
      "Epoch 30, loss: 2.297363\n",
      "Epoch 31, loss: 2.290912\n",
      "Epoch 32, loss: 2.286708\n",
      "Epoch 33, loss: 2.289121\n",
      "Epoch 34, loss: 2.291047\n",
      "Epoch 35, loss: 2.296644\n",
      "Epoch 36, loss: 2.298042\n",
      "Epoch 37, loss: 2.291234\n",
      "Epoch 38, loss: 2.288278\n",
      "Epoch 39, loss: 2.287225\n",
      "Epoch 40, loss: 2.286401\n",
      "Epoch 41, loss: 2.289356\n",
      "Epoch 42, loss: 2.283359\n",
      "Epoch 43, loss: 2.291569\n",
      "Epoch 44, loss: 2.287590\n",
      "Epoch 45, loss: 2.283643\n",
      "Epoch 46, loss: 2.285404\n",
      "Epoch 47, loss: 2.283742\n",
      "Epoch 48, loss: 2.282434\n",
      "Epoch 49, loss: 2.289714\n",
      "Epoch 50, loss: 2.288733\n",
      "Epoch 51, loss: 2.282468\n",
      "Epoch 52, loss: 2.278247\n",
      "Epoch 53, loss: 2.278667\n",
      "Epoch 54, loss: 2.288210\n",
      "Epoch 55, loss: 2.288836\n",
      "Epoch 56, loss: 2.284783\n",
      "Epoch 57, loss: 2.287709\n",
      "Epoch 58, loss: 2.278107\n",
      "Epoch 59, loss: 2.282257\n",
      "Epoch 60, loss: 2.280062\n",
      "Epoch 61, loss: 2.282299\n",
      "Epoch 62, loss: 2.280562\n",
      "Epoch 63, loss: 2.279492\n",
      "Epoch 64, loss: 2.277568\n",
      "Epoch 65, loss: 2.279871\n",
      "Epoch 66, loss: 2.277374\n",
      "Epoch 67, loss: 2.272784\n",
      "Epoch 68, loss: 2.285661\n",
      "Epoch 69, loss: 2.279150\n",
      "Epoch 70, loss: 2.279487\n",
      "Epoch 71, loss: 2.274490\n",
      "Epoch 72, loss: 2.276891\n",
      "Epoch 73, loss: 2.276927\n",
      "Epoch 74, loss: 2.279883\n",
      "Epoch 75, loss: 2.281450\n",
      "Epoch 76, loss: 2.274239\n",
      "Epoch 77, loss: 2.279015\n",
      "Epoch 78, loss: 2.280031\n",
      "Epoch 79, loss: 2.281168\n",
      "Epoch 80, loss: 2.272868\n",
      "Epoch 81, loss: 2.267589\n",
      "Epoch 82, loss: 2.278639\n",
      "Epoch 83, loss: 2.272661\n",
      "Epoch 84, loss: 2.271959\n",
      "Epoch 85, loss: 2.272440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86, loss: 2.280259\n",
      "Epoch 87, loss: 2.274035\n",
      "Epoch 88, loss: 2.273790\n",
      "Epoch 89, loss: 2.270735\n",
      "Epoch 90, loss: 2.273006\n",
      "Epoch 91, loss: 2.278108\n",
      "Epoch 92, loss: 2.275779\n",
      "Epoch 93, loss: 2.273482\n",
      "Epoch 94, loss: 2.271664\n",
      "Epoch 95, loss: 2.270561\n",
      "Epoch 96, loss: 2.268475\n",
      "Epoch 97, loss: 2.266793\n",
      "Epoch 98, loss: 2.270103\n",
      "Epoch 99, loss: 2.277588\n",
      "Epoch 100, loss: 2.267241\n",
      "Epoch 101, loss: 2.271655\n",
      "Epoch 102, loss: 2.271625\n",
      "Epoch 103, loss: 2.264223\n",
      "Epoch 104, loss: 2.269894\n",
      "Epoch 105, loss: 2.267861\n",
      "Epoch 106, loss: 2.262173\n",
      "Epoch 107, loss: 2.273339\n",
      "Epoch 108, loss: 2.267378\n",
      "Epoch 109, loss: 2.264850\n",
      "Epoch 110, loss: 2.262110\n",
      "Epoch 111, loss: 2.273860\n",
      "Epoch 112, loss: 2.273246\n",
      "Epoch 113, loss: 2.267817\n",
      "Epoch 114, loss: 2.273498\n",
      "Epoch 115, loss: 2.265580\n",
      "Epoch 116, loss: 2.263189\n",
      "Epoch 117, loss: 2.268452\n",
      "Epoch 118, loss: 2.263874\n",
      "Epoch 119, loss: 2.265668\n",
      "Epoch 120, loss: 2.264145\n",
      "Epoch 121, loss: 2.266295\n",
      "Epoch 122, loss: 2.265552\n",
      "Epoch 123, loss: 2.267768\n",
      "Epoch 124, loss: 2.270805\n",
      "Epoch 125, loss: 2.260995\n",
      "Epoch 126, loss: 2.269562\n",
      "Epoch 127, loss: 2.256823\n",
      "Epoch 128, loss: 2.272685\n",
      "Epoch 129, loss: 2.266614\n",
      "Epoch 130, loss: 2.264521\n",
      "Epoch 131, loss: 2.262368\n",
      "Epoch 132, loss: 2.269989\n",
      "Epoch 133, loss: 2.261226\n",
      "Epoch 134, loss: 2.259584\n",
      "Epoch 135, loss: 2.256375\n",
      "Epoch 136, loss: 2.265739\n",
      "Epoch 137, loss: 2.256073\n",
      "Epoch 138, loss: 2.258168\n",
      "Epoch 139, loss: 2.267988\n",
      "Epoch 140, loss: 2.259119\n",
      "Epoch 141, loss: 2.257006\n",
      "Epoch 142, loss: 2.263049\n",
      "Epoch 143, loss: 2.262094\n",
      "Epoch 144, loss: 2.260646\n",
      "Epoch 145, loss: 2.253679\n",
      "Epoch 146, loss: 2.267432\n",
      "Epoch 147, loss: 2.264259\n",
      "Epoch 148, loss: 2.262011\n",
      "Epoch 149, loss: 2.259288\n",
      "Epoch 150, loss: 2.259132\n",
      "Epoch 151, loss: 2.256218\n",
      "Epoch 152, loss: 2.260671\n",
      "Epoch 153, loss: 2.255521\n",
      "Epoch 154, loss: 2.256493\n",
      "Epoch 155, loss: 2.262985\n",
      "Epoch 156, loss: 2.258406\n",
      "Epoch 157, loss: 2.261565\n",
      "Epoch 158, loss: 2.261942\n",
      "Epoch 159, loss: 2.257457\n",
      "Epoch 160, loss: 2.255293\n",
      "Epoch 161, loss: 2.256008\n",
      "Epoch 162, loss: 2.257869\n",
      "Epoch 163, loss: 2.256091\n",
      "Epoch 164, loss: 2.255392\n",
      "Epoch 165, loss: 2.254111\n",
      "Epoch 166, loss: 2.257796\n",
      "Epoch 167, loss: 2.258372\n",
      "Epoch 168, loss: 2.256555\n",
      "Epoch 169, loss: 2.255722\n",
      "Epoch 170, loss: 2.254763\n",
      "Epoch 171, loss: 2.252755\n",
      "Epoch 172, loss: 2.253746\n",
      "Epoch 173, loss: 2.244728\n",
      "Epoch 174, loss: 2.261604\n",
      "Epoch 175, loss: 2.257451\n",
      "Epoch 176, loss: 2.255003\n",
      "Epoch 177, loss: 2.255134\n",
      "Epoch 178, loss: 2.258629\n",
      "Epoch 179, loss: 2.247829\n",
      "Epoch 180, loss: 2.256695\n",
      "Epoch 181, loss: 2.259166\n",
      "Epoch 182, loss: 2.252982\n",
      "Epoch 183, loss: 2.250188\n",
      "Epoch 184, loss: 2.248035\n",
      "Epoch 185, loss: 2.254605\n",
      "Epoch 186, loss: 2.250593\n",
      "Epoch 187, loss: 2.251856\n",
      "Epoch 188, loss: 2.252909\n",
      "Epoch 189, loss: 2.246521\n",
      "Epoch 190, loss: 2.247897\n",
      "Epoch 191, loss: 2.248140\n",
      "Epoch 192, loss: 2.263645\n",
      "Epoch 193, loss: 2.257498\n",
      "Epoch 194, loss: 2.252217\n",
      "Epoch 195, loss: 2.257239\n",
      "Epoch 196, loss: 2.250538\n",
      "Epoch 197, loss: 2.256082\n",
      "Epoch 198, loss: 2.246157\n",
      "Epoch 199, loss: 2.252043\n",
      "Epoch 0, loss: 2.303441\n",
      "Epoch 1, loss: 2.302128\n",
      "Epoch 2, loss: 2.304807\n",
      "Epoch 3, loss: 2.301936\n",
      "Epoch 4, loss: 2.301415\n",
      "Epoch 5, loss: 2.303220\n",
      "Epoch 6, loss: 2.303777\n",
      "Epoch 7, loss: 2.298464\n",
      "Epoch 8, loss: 2.297652\n",
      "Epoch 9, loss: 2.301089\n",
      "Epoch 10, loss: 2.302068\n",
      "Epoch 11, loss: 2.297487\n",
      "Epoch 12, loss: 2.296211\n",
      "Epoch 13, loss: 2.296014\n",
      "Epoch 14, loss: 2.300038\n",
      "Epoch 15, loss: 2.291313\n",
      "Epoch 16, loss: 2.302877\n",
      "Epoch 17, loss: 2.292518\n",
      "Epoch 18, loss: 2.321341\n",
      "Epoch 19, loss: 2.296367\n",
      "Epoch 20, loss: 2.296811\n",
      "Epoch 21, loss: 2.294204\n",
      "Epoch 22, loss: 2.293326\n",
      "Epoch 23, loss: 2.296014\n",
      "Epoch 24, loss: 2.297910\n",
      "Epoch 25, loss: 2.295525\n",
      "Epoch 26, loss: 2.291407\n",
      "Epoch 27, loss: 2.295039\n",
      "Epoch 28, loss: 2.289779\n",
      "Epoch 29, loss: 2.290736\n",
      "Epoch 30, loss: 2.289952\n",
      "Epoch 31, loss: 2.291110\n",
      "Epoch 32, loss: 2.291256\n",
      "Epoch 33, loss: 2.294536\n",
      "Epoch 34, loss: 2.291680\n",
      "Epoch 35, loss: 2.287443\n",
      "Epoch 36, loss: 2.289742\n",
      "Epoch 37, loss: 2.295161\n",
      "Epoch 38, loss: 2.291059\n",
      "Epoch 39, loss: 2.288879\n",
      "Epoch 40, loss: 2.285169\n",
      "Epoch 41, loss: 2.283889\n",
      "Epoch 42, loss: 2.286130\n",
      "Epoch 43, loss: 2.283675\n",
      "Epoch 44, loss: 2.286882\n",
      "Epoch 45, loss: 2.290365\n",
      "Epoch 46, loss: 2.292273\n",
      "Epoch 47, loss: 2.289699\n",
      "Epoch 48, loss: 2.291272\n",
      "Epoch 49, loss: 2.290223\n",
      "Epoch 50, loss: 2.281896\n",
      "Epoch 51, loss: 2.284658\n",
      "Epoch 52, loss: 2.283560\n",
      "Epoch 53, loss: 2.280996\n",
      "Epoch 54, loss: 2.278145\n",
      "Epoch 55, loss: 2.283763\n",
      "Epoch 56, loss: 2.285139\n",
      "Epoch 57, loss: 2.287752\n",
      "Epoch 58, loss: 2.289030\n",
      "Epoch 59, loss: 2.281875\n",
      "Epoch 60, loss: 2.283339\n",
      "Epoch 61, loss: 2.274550\n",
      "Epoch 62, loss: 2.283388\n",
      "Epoch 63, loss: 2.278195\n",
      "Epoch 64, loss: 2.277585\n",
      "Epoch 65, loss: 2.285224\n",
      "Epoch 66, loss: 2.281828\n",
      "Epoch 67, loss: 2.276362\n",
      "Epoch 68, loss: 2.286484\n",
      "Epoch 69, loss: 2.280711\n",
      "Epoch 70, loss: 2.282747\n",
      "Epoch 71, loss: 2.276922\n",
      "Epoch 72, loss: 2.277525\n",
      "Epoch 73, loss: 2.285027\n",
      "Epoch 74, loss: 2.273266\n",
      "Epoch 75, loss: 2.275883\n",
      "Epoch 76, loss: 2.275045\n",
      "Epoch 77, loss: 2.276373\n",
      "Epoch 78, loss: 2.277227\n",
      "Epoch 79, loss: 2.275456\n",
      "Epoch 80, loss: 2.284867\n",
      "Epoch 81, loss: 2.274649\n",
      "Epoch 82, loss: 2.276173\n",
      "Epoch 83, loss: 2.274075\n",
      "Epoch 84, loss: 2.275210\n",
      "Epoch 85, loss: 2.269114\n",
      "Epoch 86, loss: 2.272098\n",
      "Epoch 87, loss: 2.280231\n",
      "Epoch 88, loss: 2.277632\n",
      "Epoch 89, loss: 2.268740\n",
      "Epoch 90, loss: 2.272593\n",
      "Epoch 91, loss: 2.274443\n",
      "Epoch 92, loss: 2.272558\n",
      "Epoch 93, loss: 2.272905\n",
      "Epoch 94, loss: 2.271920\n",
      "Epoch 95, loss: 2.272523\n",
      "Epoch 96, loss: 2.266337\n",
      "Epoch 97, loss: 2.266908\n",
      "Epoch 98, loss: 2.278215\n",
      "Epoch 99, loss: 2.269591\n",
      "Epoch 100, loss: 2.268008\n",
      "Epoch 101, loss: 2.274104\n",
      "Epoch 102, loss: 2.264189\n",
      "Epoch 103, loss: 2.267579\n",
      "Epoch 104, loss: 2.273179\n",
      "Epoch 105, loss: 2.270015\n",
      "Epoch 106, loss: 2.266592\n",
      "Epoch 107, loss: 2.268428\n",
      "Epoch 108, loss: 2.268381\n",
      "Epoch 109, loss: 2.267038\n",
      "Epoch 110, loss: 2.273090\n",
      "Epoch 111, loss: 2.269475\n",
      "Epoch 112, loss: 2.266109\n",
      "Epoch 113, loss: 2.267093\n",
      "Epoch 114, loss: 2.262823\n",
      "Epoch 115, loss: 2.272577\n",
      "Epoch 116, loss: 2.265138\n",
      "Epoch 117, loss: 2.267047\n",
      "Epoch 118, loss: 2.269545\n",
      "Epoch 119, loss: 2.267517\n",
      "Epoch 120, loss: 2.270115\n",
      "Epoch 121, loss: 2.265809\n",
      "Epoch 122, loss: 2.267982\n",
      "Epoch 123, loss: 2.265898\n",
      "Epoch 124, loss: 2.268819\n",
      "Epoch 125, loss: 2.267114\n",
      "Epoch 126, loss: 2.263642\n",
      "Epoch 127, loss: 2.267573\n",
      "Epoch 128, loss: 2.260877\n",
      "Epoch 129, loss: 2.264488\n",
      "Epoch 130, loss: 2.259611\n",
      "Epoch 131, loss: 2.263869\n",
      "Epoch 132, loss: 2.262150\n",
      "Epoch 133, loss: 2.257720\n",
      "Epoch 134, loss: 2.261899\n",
      "Epoch 135, loss: 2.262938\n",
      "Epoch 136, loss: 2.263648\n",
      "Epoch 137, loss: 2.259980\n",
      "Epoch 138, loss: 2.260208\n",
      "Epoch 139, loss: 2.265034\n",
      "Epoch 140, loss: 2.258371\n",
      "Epoch 141, loss: 2.256365\n",
      "Epoch 142, loss: 2.251892\n",
      "Epoch 143, loss: 2.269042\n",
      "Epoch 144, loss: 2.261530\n",
      "Epoch 145, loss: 2.259360\n",
      "Epoch 146, loss: 2.268111\n",
      "Epoch 147, loss: 2.259074\n",
      "Epoch 148, loss: 2.255715\n",
      "Epoch 149, loss: 2.256094\n",
      "Epoch 150, loss: 2.264230\n",
      "Epoch 151, loss: 2.256211\n",
      "Epoch 152, loss: 2.261065\n",
      "Epoch 153, loss: 2.257127\n",
      "Epoch 154, loss: 2.260970\n",
      "Epoch 155, loss: 2.258801\n",
      "Epoch 156, loss: 2.254974\n",
      "Epoch 157, loss: 2.255329\n",
      "Epoch 158, loss: 2.252243\n",
      "Epoch 159, loss: 2.264285\n",
      "Epoch 160, loss: 2.257505\n",
      "Epoch 161, loss: 2.255911\n",
      "Epoch 162, loss: 2.260449\n",
      "Epoch 163, loss: 2.253675\n",
      "Epoch 164, loss: 2.251353\n",
      "Epoch 165, loss: 2.255603\n",
      "Epoch 166, loss: 2.258485\n",
      "Epoch 167, loss: 2.260340\n",
      "Epoch 168, loss: 2.259609\n",
      "Epoch 169, loss: 2.248857\n",
      "Epoch 170, loss: 2.256615\n",
      "Epoch 171, loss: 2.257680\n",
      "Epoch 172, loss: 2.257111\n",
      "Epoch 173, loss: 2.259651\n",
      "Epoch 174, loss: 2.253940\n",
      "Epoch 175, loss: 2.257851\n",
      "Epoch 176, loss: 2.253156\n",
      "Epoch 177, loss: 2.250285\n",
      "Epoch 178, loss: 2.254270\n",
      "Epoch 179, loss: 2.251020\n",
      "Epoch 180, loss: 2.254479\n",
      "Epoch 181, loss: 2.259042\n",
      "Epoch 182, loss: 2.255209\n",
      "Epoch 183, loss: 2.253924\n",
      "Epoch 184, loss: 2.255495\n",
      "Epoch 185, loss: 2.254470\n",
      "Epoch 186, loss: 2.252236\n",
      "Epoch 187, loss: 2.246776\n",
      "Epoch 188, loss: 2.253726\n",
      "Epoch 189, loss: 2.253662\n",
      "Epoch 190, loss: 2.254699\n",
      "Epoch 191, loss: 2.254780\n",
      "Epoch 192, loss: 2.248526\n",
      "Epoch 193, loss: 2.252367\n",
      "Epoch 194, loss: 2.252440\n",
      "Epoch 195, loss: 2.248742\n",
      "Epoch 196, loss: 2.253065\n",
      "Epoch 197, loss: 2.249208\n",
      "Epoch 198, loss: 2.246195\n",
      "Epoch 199, loss: 2.249929\n",
      "Epoch 0, loss: 2.302225\n",
      "Epoch 1, loss: 2.300249\n",
      "Epoch 2, loss: 2.303225\n",
      "Epoch 3, loss: 2.308512\n",
      "Epoch 4, loss: 2.298298\n",
      "Epoch 5, loss: 2.305320\n",
      "Epoch 6, loss: 2.301996\n",
      "Epoch 7, loss: 2.302563\n",
      "Epoch 8, loss: 2.298567\n",
      "Epoch 9, loss: 2.297453\n",
      "Epoch 10, loss: 2.297093\n",
      "Epoch 11, loss: 2.298389\n",
      "Epoch 12, loss: 2.297061\n",
      "Epoch 13, loss: 2.299358\n",
      "Epoch 14, loss: 2.292341\n",
      "Epoch 15, loss: 2.308326\n",
      "Epoch 16, loss: 2.294754\n",
      "Epoch 17, loss: 2.293565\n",
      "Epoch 18, loss: 2.303021\n",
      "Epoch 19, loss: 2.297234\n",
      "Epoch 20, loss: 2.297706\n",
      "Epoch 21, loss: 2.297169\n",
      "Epoch 22, loss: 2.295783\n",
      "Epoch 23, loss: 2.293865\n",
      "Epoch 24, loss: 2.296829\n",
      "Epoch 25, loss: 2.292520\n",
      "Epoch 26, loss: 2.287894\n",
      "Epoch 27, loss: 2.291384\n",
      "Epoch 28, loss: 2.302873\n",
      "Epoch 29, loss: 2.288660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, loss: 2.290979\n",
      "Epoch 31, loss: 2.296036\n",
      "Epoch 32, loss: 2.292004\n",
      "Epoch 33, loss: 2.290609\n",
      "Epoch 34, loss: 2.289886\n",
      "Epoch 35, loss: 2.291992\n",
      "Epoch 36, loss: 2.292794\n",
      "Epoch 37, loss: 2.289128\n",
      "Epoch 38, loss: 2.289996\n",
      "Epoch 39, loss: 2.288326\n",
      "Epoch 40, loss: 2.290520\n",
      "Epoch 41, loss: 2.285484\n",
      "Epoch 42, loss: 2.290810\n",
      "Epoch 43, loss: 2.291144\n",
      "Epoch 44, loss: 2.285250\n",
      "Epoch 45, loss: 2.281819\n",
      "Epoch 46, loss: 2.289775\n",
      "Epoch 47, loss: 2.286879\n",
      "Epoch 48, loss: 2.284883\n",
      "Epoch 49, loss: 2.275156\n",
      "Epoch 50, loss: 2.290822\n",
      "Epoch 51, loss: 2.292181\n",
      "Epoch 52, loss: 2.289799\n",
      "Epoch 53, loss: 2.290515\n",
      "Epoch 54, loss: 2.284501\n",
      "Epoch 55, loss: 2.281219\n",
      "Epoch 56, loss: 2.286063\n",
      "Epoch 57, loss: 2.278214\n",
      "Epoch 58, loss: 2.287259\n",
      "Epoch 59, loss: 2.280966\n",
      "Epoch 60, loss: 2.281473\n",
      "Epoch 61, loss: 2.285966\n",
      "Epoch 62, loss: 2.285493\n",
      "Epoch 63, loss: 2.281274\n",
      "Epoch 64, loss: 2.279150\n",
      "Epoch 65, loss: 2.278372\n",
      "Epoch 66, loss: 2.284921\n",
      "Epoch 67, loss: 2.275462\n",
      "Epoch 68, loss: 2.277043\n",
      "Epoch 69, loss: 2.283936\n",
      "Epoch 70, loss: 2.274928\n",
      "Epoch 71, loss: 2.284539\n",
      "Epoch 72, loss: 2.277348\n",
      "Epoch 73, loss: 2.274093\n",
      "Epoch 74, loss: 2.285618\n",
      "Epoch 75, loss: 2.273470\n",
      "Epoch 76, loss: 2.269420\n",
      "Epoch 77, loss: 2.273181\n",
      "Epoch 78, loss: 2.286564\n",
      "Epoch 79, loss: 2.274160\n",
      "Epoch 80, loss: 2.273745\n",
      "Epoch 81, loss: 2.270312\n",
      "Epoch 82, loss: 2.274920\n",
      "Epoch 83, loss: 2.275545\n",
      "Epoch 84, loss: 2.270747\n",
      "Epoch 85, loss: 2.265563\n",
      "Epoch 86, loss: 2.281188\n",
      "Epoch 87, loss: 2.281047\n",
      "Epoch 88, loss: 2.276416\n",
      "Epoch 89, loss: 2.272879\n",
      "Epoch 90, loss: 2.272964\n",
      "Epoch 91, loss: 2.272414\n",
      "Epoch 92, loss: 2.278155\n",
      "Epoch 93, loss: 2.277578\n",
      "Epoch 94, loss: 2.270942\n",
      "Epoch 95, loss: 2.269633\n",
      "Epoch 96, loss: 2.270068\n",
      "Epoch 97, loss: 2.269299\n",
      "Epoch 98, loss: 2.263195\n",
      "Epoch 99, loss: 2.263681\n",
      "Epoch 100, loss: 2.272759\n",
      "Epoch 101, loss: 2.275315\n",
      "Epoch 102, loss: 2.273470\n",
      "Epoch 103, loss: 2.271761\n",
      "Epoch 104, loss: 2.268865\n",
      "Epoch 105, loss: 2.270607\n",
      "Epoch 106, loss: 2.269607\n",
      "Epoch 107, loss: 2.268672\n",
      "Epoch 108, loss: 2.265588\n",
      "Epoch 109, loss: 2.271649\n",
      "Epoch 110, loss: 2.265863\n",
      "Epoch 111, loss: 2.267714\n",
      "Epoch 112, loss: 2.263684\n",
      "Epoch 113, loss: 2.270212\n",
      "Epoch 114, loss: 2.262978\n",
      "Epoch 115, loss: 2.267748\n",
      "Epoch 116, loss: 2.267556\n",
      "Epoch 117, loss: 2.266799\n",
      "Epoch 118, loss: 2.268358\n",
      "Epoch 119, loss: 2.261347\n",
      "Epoch 120, loss: 2.274792\n",
      "Epoch 121, loss: 2.266136\n",
      "Epoch 122, loss: 2.267378\n",
      "Epoch 123, loss: 2.261324\n",
      "Epoch 124, loss: 2.260271\n",
      "Epoch 125, loss: 2.263391\n",
      "Epoch 126, loss: 2.262685\n",
      "Epoch 127, loss: 2.267843\n",
      "Epoch 128, loss: 2.262506\n",
      "Epoch 129, loss: 2.262290\n",
      "Epoch 130, loss: 2.263950\n",
      "Epoch 131, loss: 2.258736\n",
      "Epoch 132, loss: 2.261673\n",
      "Epoch 133, loss: 2.266411\n",
      "Epoch 134, loss: 2.258690\n",
      "Epoch 135, loss: 2.262731\n",
      "Epoch 136, loss: 2.262652\n",
      "Epoch 137, loss: 2.260696\n",
      "Epoch 138, loss: 2.259170\n",
      "Epoch 139, loss: 2.258548\n",
      "Epoch 140, loss: 2.266255\n",
      "Epoch 141, loss: 2.260533\n",
      "Epoch 142, loss: 2.256808\n",
      "Epoch 143, loss: 2.259998\n",
      "Epoch 144, loss: 2.260328\n",
      "Epoch 145, loss: 2.259303\n",
      "Epoch 146, loss: 2.258207\n",
      "Epoch 147, loss: 2.254439\n",
      "Epoch 148, loss: 2.264727\n",
      "Epoch 149, loss: 2.259107\n",
      "Epoch 150, loss: 2.262117\n",
      "Epoch 151, loss: 2.260175\n",
      "Epoch 152, loss: 2.267352\n",
      "Epoch 153, loss: 2.255086\n",
      "Epoch 154, loss: 2.252891\n",
      "Epoch 155, loss: 2.254167\n",
      "Epoch 156, loss: 2.248967\n",
      "Epoch 157, loss: 2.248718\n",
      "Epoch 158, loss: 2.258125\n",
      "Epoch 159, loss: 2.261879\n",
      "Epoch 160, loss: 2.251051\n",
      "Epoch 161, loss: 2.256986\n",
      "Epoch 162, loss: 2.259185\n",
      "Epoch 163, loss: 2.253417\n",
      "Epoch 164, loss: 2.256727\n",
      "Epoch 165, loss: 2.254879\n",
      "Epoch 166, loss: 2.260156\n",
      "Epoch 167, loss: 2.255657\n",
      "Epoch 168, loss: 2.255577\n",
      "Epoch 169, loss: 2.257003\n",
      "Epoch 170, loss: 2.255261\n",
      "Epoch 171, loss: 2.250842\n",
      "Epoch 172, loss: 2.264511\n",
      "Epoch 173, loss: 2.251209\n",
      "Epoch 174, loss: 2.256303\n",
      "Epoch 175, loss: 2.254968\n",
      "Epoch 176, loss: 2.256522\n",
      "Epoch 177, loss: 2.254068\n",
      "Epoch 178, loss: 2.252712\n",
      "Epoch 179, loss: 2.255871\n",
      "Epoch 180, loss: 2.244842\n",
      "Epoch 181, loss: 2.259470\n",
      "Epoch 182, loss: 2.251250\n",
      "Epoch 183, loss: 2.255825\n",
      "Epoch 184, loss: 2.250701\n",
      "Epoch 185, loss: 2.253923\n",
      "Epoch 186, loss: 2.251511\n",
      "Epoch 187, loss: 2.253740\n",
      "Epoch 188, loss: 2.254224\n",
      "Epoch 189, loss: 2.252902\n",
      "Epoch 190, loss: 2.251119\n",
      "Epoch 191, loss: 2.256947\n",
      "Epoch 192, loss: 2.253026\n",
      "Epoch 193, loss: 2.253606\n",
      "Epoch 194, loss: 2.252249\n",
      "Epoch 195, loss: 2.251750\n",
      "Epoch 196, loss: 2.247574\n",
      "Epoch 197, loss: 2.250418\n",
      "Epoch 198, loss: 2.245065\n",
      "Epoch 199, loss: 2.245005\n",
      "Epoch 0, loss: 2.302879\n",
      "Epoch 1, loss: 2.302218\n",
      "Epoch 2, loss: 2.301959\n",
      "Epoch 3, loss: 2.302710\n",
      "Epoch 4, loss: 2.304334\n",
      "Epoch 5, loss: 2.301782\n",
      "Epoch 6, loss: 2.301882\n",
      "Epoch 7, loss: 2.303164\n",
      "Epoch 8, loss: 2.302011\n",
      "Epoch 9, loss: 2.301128\n",
      "Epoch 10, loss: 2.301470\n",
      "Epoch 11, loss: 2.301062\n",
      "Epoch 12, loss: 2.301105\n",
      "Epoch 13, loss: 2.301382\n",
      "Epoch 14, loss: 2.300198\n",
      "Epoch 15, loss: 2.303986\n",
      "Epoch 16, loss: 2.301314\n",
      "Epoch 17, loss: 2.302410\n",
      "Epoch 18, loss: 2.301816\n",
      "Epoch 19, loss: 2.303036\n",
      "Epoch 20, loss: 2.300475\n",
      "Epoch 21, loss: 2.302250\n",
      "Epoch 22, loss: 2.300523\n",
      "Epoch 23, loss: 2.301405\n",
      "Epoch 24, loss: 2.302712\n",
      "Epoch 25, loss: 2.300703\n",
      "Epoch 26, loss: 2.303874\n",
      "Epoch 27, loss: 2.301954\n",
      "Epoch 28, loss: 2.301967\n",
      "Epoch 29, loss: 2.300387\n",
      "Epoch 30, loss: 2.301189\n",
      "Epoch 31, loss: 2.300230\n",
      "Epoch 32, loss: 2.299953\n",
      "Epoch 33, loss: 2.299460\n",
      "Epoch 34, loss: 2.301467\n",
      "Epoch 35, loss: 2.301158\n",
      "Epoch 36, loss: 2.301952\n",
      "Epoch 37, loss: 2.300560\n",
      "Epoch 38, loss: 2.300070\n",
      "Epoch 39, loss: 2.301522\n",
      "Epoch 40, loss: 2.301478\n",
      "Epoch 41, loss: 2.301363\n",
      "Epoch 42, loss: 2.301205\n",
      "Epoch 43, loss: 2.300888\n",
      "Epoch 44, loss: 2.299535\n",
      "Epoch 45, loss: 2.299663\n",
      "Epoch 46, loss: 2.302284\n",
      "Epoch 47, loss: 2.299582\n",
      "Epoch 48, loss: 2.300676\n",
      "Epoch 49, loss: 2.299377\n",
      "Epoch 50, loss: 2.300436\n",
      "Epoch 51, loss: 2.299828\n",
      "Epoch 52, loss: 2.300419\n",
      "Epoch 53, loss: 2.299967\n",
      "Epoch 54, loss: 2.301082\n",
      "Epoch 55, loss: 2.300269\n",
      "Epoch 56, loss: 2.299720\n",
      "Epoch 57, loss: 2.299159\n",
      "Epoch 58, loss: 2.299935\n",
      "Epoch 59, loss: 2.300605\n",
      "Epoch 60, loss: 2.300498\n",
      "Epoch 61, loss: 2.298473\n",
      "Epoch 62, loss: 2.300040\n",
      "Epoch 63, loss: 2.300213\n",
      "Epoch 64, loss: 2.299644\n",
      "Epoch 65, loss: 2.300159\n",
      "Epoch 66, loss: 2.300782\n",
      "Epoch 67, loss: 2.299015\n",
      "Epoch 68, loss: 2.300124\n",
      "Epoch 69, loss: 2.300148\n",
      "Epoch 70, loss: 2.299195\n",
      "Epoch 71, loss: 2.302285\n",
      "Epoch 72, loss: 2.299799\n",
      "Epoch 73, loss: 2.298550\n",
      "Epoch 74, loss: 2.299090\n",
      "Epoch 75, loss: 2.298745\n",
      "Epoch 76, loss: 2.300702\n",
      "Epoch 77, loss: 2.300059\n",
      "Epoch 78, loss: 2.299929\n",
      "Epoch 79, loss: 2.298464\n",
      "Epoch 80, loss: 2.299931\n",
      "Epoch 81, loss: 2.299061\n",
      "Epoch 82, loss: 2.299985\n",
      "Epoch 83, loss: 2.299168\n",
      "Epoch 84, loss: 2.299391\n",
      "Epoch 85, loss: 2.299513\n",
      "Epoch 86, loss: 2.299305\n",
      "Epoch 87, loss: 2.297500\n",
      "Epoch 88, loss: 2.298072\n",
      "Epoch 89, loss: 2.298752\n",
      "Epoch 90, loss: 2.298914\n",
      "Epoch 91, loss: 2.299034\n",
      "Epoch 92, loss: 2.299199\n",
      "Epoch 93, loss: 2.299643\n",
      "Epoch 94, loss: 2.299654\n",
      "Epoch 95, loss: 2.298419\n",
      "Epoch 96, loss: 2.298510\n",
      "Epoch 97, loss: 2.297258\n",
      "Epoch 98, loss: 2.298977\n",
      "Epoch 99, loss: 2.297091\n",
      "Epoch 100, loss: 2.297779\n",
      "Epoch 101, loss: 2.297570\n",
      "Epoch 102, loss: 2.298279\n",
      "Epoch 103, loss: 2.296491\n",
      "Epoch 104, loss: 2.298325\n",
      "Epoch 105, loss: 2.298051\n",
      "Epoch 106, loss: 2.296357\n",
      "Epoch 107, loss: 2.298156\n",
      "Epoch 108, loss: 2.299213\n",
      "Epoch 109, loss: 2.299398\n",
      "Epoch 110, loss: 2.297317\n",
      "Epoch 111, loss: 2.298397\n",
      "Epoch 112, loss: 2.297601\n",
      "Epoch 113, loss: 2.299603\n",
      "Epoch 114, loss: 2.297197\n",
      "Epoch 115, loss: 2.298075\n",
      "Epoch 116, loss: 2.298276\n",
      "Epoch 117, loss: 2.298156\n",
      "Epoch 118, loss: 2.297467\n",
      "Epoch 119, loss: 2.297340\n",
      "Epoch 120, loss: 2.297630\n",
      "Epoch 121, loss: 2.300037\n",
      "Epoch 122, loss: 2.297077\n",
      "Epoch 123, loss: 2.296295\n",
      "Epoch 124, loss: 2.298726\n",
      "Epoch 125, loss: 2.297700\n",
      "Epoch 126, loss: 2.297358\n",
      "Epoch 127, loss: 2.296906\n",
      "Epoch 128, loss: 2.296918\n",
      "Epoch 129, loss: 2.295919\n",
      "Epoch 130, loss: 2.297098\n",
      "Epoch 131, loss: 2.297731\n",
      "Epoch 132, loss: 2.297762\n",
      "Epoch 133, loss: 2.295502\n",
      "Epoch 134, loss: 2.298543\n",
      "Epoch 135, loss: 2.297604\n",
      "Epoch 136, loss: 2.295886\n",
      "Epoch 137, loss: 2.296326\n",
      "Epoch 138, loss: 2.297158\n",
      "Epoch 139, loss: 2.297847\n",
      "Epoch 140, loss: 2.298186\n",
      "Epoch 141, loss: 2.296743\n",
      "Epoch 142, loss: 2.296699\n",
      "Epoch 143, loss: 2.297853\n",
      "Epoch 144, loss: 2.296573\n",
      "Epoch 145, loss: 2.296222\n",
      "Epoch 146, loss: 2.297388\n",
      "Epoch 147, loss: 2.296796\n",
      "Epoch 148, loss: 2.297070\n",
      "Epoch 149, loss: 2.294040\n",
      "Epoch 150, loss: 2.295803\n",
      "Epoch 151, loss: 2.295643\n",
      "Epoch 152, loss: 2.296382\n",
      "Epoch 153, loss: 2.296964\n",
      "Epoch 154, loss: 2.296246\n",
      "Epoch 155, loss: 2.297058\n",
      "Epoch 156, loss: 2.296847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157, loss: 2.295251\n",
      "Epoch 158, loss: 2.294734\n",
      "Epoch 159, loss: 2.296680\n",
      "Epoch 160, loss: 2.296789\n",
      "Epoch 161, loss: 2.296603\n",
      "Epoch 162, loss: 2.296861\n",
      "Epoch 163, loss: 2.297000\n",
      "Epoch 164, loss: 2.295929\n",
      "Epoch 165, loss: 2.296785\n",
      "Epoch 166, loss: 2.295983\n",
      "Epoch 167, loss: 2.296111\n",
      "Epoch 168, loss: 2.295819\n",
      "Epoch 169, loss: 2.296560\n",
      "Epoch 170, loss: 2.296014\n",
      "Epoch 171, loss: 2.295677\n",
      "Epoch 172, loss: 2.295282\n",
      "Epoch 173, loss: 2.294581\n",
      "Epoch 174, loss: 2.295821\n",
      "Epoch 175, loss: 2.295678\n",
      "Epoch 176, loss: 2.296139\n",
      "Epoch 177, loss: 2.294448\n",
      "Epoch 178, loss: 2.294989\n",
      "Epoch 179, loss: 2.294580\n",
      "Epoch 180, loss: 2.296639\n",
      "Epoch 181, loss: 2.294773\n",
      "Epoch 182, loss: 2.296660\n",
      "Epoch 183, loss: 2.294741\n",
      "Epoch 184, loss: 2.295167\n",
      "Epoch 185, loss: 2.295513\n",
      "Epoch 186, loss: 2.295419\n",
      "Epoch 187, loss: 2.295194\n",
      "Epoch 188, loss: 2.294630\n",
      "Epoch 189, loss: 2.295386\n",
      "Epoch 190, loss: 2.293866\n",
      "Epoch 191, loss: 2.294007\n",
      "Epoch 192, loss: 2.294582\n",
      "Epoch 193, loss: 2.294162\n",
      "Epoch 194, loss: 2.294767\n",
      "Epoch 195, loss: 2.295223\n",
      "Epoch 196, loss: 2.295355\n",
      "Epoch 197, loss: 2.293648\n",
      "Epoch 198, loss: 2.294083\n",
      "Epoch 199, loss: 2.293679\n",
      "Epoch 0, loss: 2.302295\n",
      "Epoch 1, loss: 2.302061\n",
      "Epoch 2, loss: 2.301801\n",
      "Epoch 3, loss: 2.302163\n",
      "Epoch 4, loss: 2.303038\n",
      "Epoch 5, loss: 2.302002\n",
      "Epoch 6, loss: 2.302050\n",
      "Epoch 7, loss: 2.302338\n",
      "Epoch 8, loss: 2.301614\n",
      "Epoch 9, loss: 2.302103\n",
      "Epoch 10, loss: 2.303337\n",
      "Epoch 11, loss: 2.302063\n",
      "Epoch 12, loss: 2.302989\n",
      "Epoch 13, loss: 2.301990\n",
      "Epoch 14, loss: 2.302056\n",
      "Epoch 15, loss: 2.302480\n",
      "Epoch 16, loss: 2.302410\n",
      "Epoch 17, loss: 2.301542\n",
      "Epoch 18, loss: 2.302771\n",
      "Epoch 19, loss: 2.302052\n",
      "Epoch 20, loss: 2.300762\n",
      "Epoch 21, loss: 2.302218\n",
      "Epoch 22, loss: 2.301080\n",
      "Epoch 23, loss: 2.302166\n",
      "Epoch 24, loss: 2.301894\n",
      "Epoch 25, loss: 2.301600\n",
      "Epoch 26, loss: 2.302208\n",
      "Epoch 27, loss: 2.301486\n",
      "Epoch 28, loss: 2.300886\n",
      "Epoch 29, loss: 2.301663\n",
      "Epoch 30, loss: 2.301357\n",
      "Epoch 31, loss: 2.301092\n",
      "Epoch 32, loss: 2.301445\n",
      "Epoch 33, loss: 2.300647\n",
      "Epoch 34, loss: 2.302004\n",
      "Epoch 35, loss: 2.300425\n",
      "Epoch 36, loss: 2.300743\n",
      "Epoch 37, loss: 2.301086\n",
      "Epoch 38, loss: 2.300091\n",
      "Epoch 39, loss: 2.301831\n",
      "Epoch 40, loss: 2.302500\n",
      "Epoch 41, loss: 2.299904\n",
      "Epoch 42, loss: 2.300060\n",
      "Epoch 43, loss: 2.301910\n",
      "Epoch 44, loss: 2.300378\n",
      "Epoch 45, loss: 2.301421\n",
      "Epoch 46, loss: 2.301670\n",
      "Epoch 47, loss: 2.301157\n",
      "Epoch 48, loss: 2.301442\n",
      "Epoch 49, loss: 2.300124\n",
      "Epoch 50, loss: 2.300341\n",
      "Epoch 51, loss: 2.299115\n",
      "Epoch 52, loss: 2.300523\n",
      "Epoch 53, loss: 2.299518\n",
      "Epoch 54, loss: 2.298835\n",
      "Epoch 55, loss: 2.300496\n",
      "Epoch 56, loss: 2.299477\n",
      "Epoch 57, loss: 2.300237\n",
      "Epoch 58, loss: 2.299271\n",
      "Epoch 59, loss: 2.299553\n",
      "Epoch 60, loss: 2.298975\n",
      "Epoch 61, loss: 2.299840\n",
      "Epoch 62, loss: 2.299060\n",
      "Epoch 63, loss: 2.298893\n",
      "Epoch 64, loss: 2.299765\n",
      "Epoch 65, loss: 2.300189\n",
      "Epoch 66, loss: 2.299598\n",
      "Epoch 67, loss: 2.299535\n",
      "Epoch 68, loss: 2.300431\n",
      "Epoch 69, loss: 2.299393\n",
      "Epoch 70, loss: 2.301377\n",
      "Epoch 71, loss: 2.299401\n",
      "Epoch 72, loss: 2.299881\n",
      "Epoch 73, loss: 2.300480\n",
      "Epoch 74, loss: 2.299377\n",
      "Epoch 75, loss: 2.300098\n",
      "Epoch 76, loss: 2.298965\n",
      "Epoch 77, loss: 2.300216\n",
      "Epoch 78, loss: 2.298988\n",
      "Epoch 79, loss: 2.298748\n",
      "Epoch 80, loss: 2.299435\n",
      "Epoch 81, loss: 2.299692\n",
      "Epoch 82, loss: 2.298138\n",
      "Epoch 83, loss: 2.298942\n",
      "Epoch 84, loss: 2.296945\n",
      "Epoch 85, loss: 2.300609\n",
      "Epoch 86, loss: 2.300029\n",
      "Epoch 87, loss: 2.299338\n",
      "Epoch 88, loss: 2.297776\n",
      "Epoch 89, loss: 2.296668\n",
      "Epoch 90, loss: 2.298957\n",
      "Epoch 91, loss: 2.298181\n",
      "Epoch 92, loss: 2.299979\n",
      "Epoch 93, loss: 2.298068\n",
      "Epoch 94, loss: 2.300126\n",
      "Epoch 95, loss: 2.299843\n",
      "Epoch 96, loss: 2.296979\n",
      "Epoch 97, loss: 2.298835\n",
      "Epoch 98, loss: 2.298562\n",
      "Epoch 99, loss: 2.297935\n",
      "Epoch 100, loss: 2.299935\n",
      "Epoch 101, loss: 2.299687\n",
      "Epoch 102, loss: 2.297355\n",
      "Epoch 103, loss: 2.299224\n",
      "Epoch 104, loss: 2.299359\n",
      "Epoch 105, loss: 2.298328\n",
      "Epoch 106, loss: 2.298288\n",
      "Epoch 107, loss: 2.299979\n",
      "Epoch 108, loss: 2.297232\n",
      "Epoch 109, loss: 2.298151\n",
      "Epoch 110, loss: 2.296719\n",
      "Epoch 111, loss: 2.297866\n",
      "Epoch 112, loss: 2.296932\n",
      "Epoch 113, loss: 2.298040\n",
      "Epoch 114, loss: 2.297439\n",
      "Epoch 115, loss: 2.298586\n",
      "Epoch 116, loss: 2.298580\n",
      "Epoch 117, loss: 2.298681\n",
      "Epoch 118, loss: 2.297079\n",
      "Epoch 119, loss: 2.297558\n",
      "Epoch 120, loss: 2.298227\n",
      "Epoch 121, loss: 2.298424\n",
      "Epoch 122, loss: 2.297977\n",
      "Epoch 123, loss: 2.298231\n",
      "Epoch 124, loss: 2.297254\n",
      "Epoch 125, loss: 2.296571\n",
      "Epoch 126, loss: 2.297637\n",
      "Epoch 127, loss: 2.297937\n",
      "Epoch 128, loss: 2.296803\n",
      "Epoch 129, loss: 2.297528\n",
      "Epoch 130, loss: 2.296323\n",
      "Epoch 131, loss: 2.297735\n",
      "Epoch 132, loss: 2.296298\n",
      "Epoch 133, loss: 2.297690\n",
      "Epoch 134, loss: 2.300030\n",
      "Epoch 135, loss: 2.297813\n",
      "Epoch 136, loss: 2.296582\n",
      "Epoch 137, loss: 2.296238\n",
      "Epoch 138, loss: 2.297494\n",
      "Epoch 139, loss: 2.298208\n",
      "Epoch 140, loss: 2.296497\n",
      "Epoch 141, loss: 2.296776\n",
      "Epoch 142, loss: 2.296914\n",
      "Epoch 143, loss: 2.296417\n",
      "Epoch 144, loss: 2.296985\n",
      "Epoch 145, loss: 2.296659\n",
      "Epoch 146, loss: 2.297174\n",
      "Epoch 147, loss: 2.296939\n",
      "Epoch 148, loss: 2.296692\n",
      "Epoch 149, loss: 2.295630\n",
      "Epoch 150, loss: 2.295895\n",
      "Epoch 151, loss: 2.296850\n",
      "Epoch 152, loss: 2.296999\n",
      "Epoch 153, loss: 2.296756\n",
      "Epoch 154, loss: 2.294826\n",
      "Epoch 155, loss: 2.296143\n",
      "Epoch 156, loss: 2.295716\n",
      "Epoch 157, loss: 2.295290\n",
      "Epoch 158, loss: 2.294888\n",
      "Epoch 159, loss: 2.298878\n",
      "Epoch 160, loss: 2.293521\n",
      "Epoch 161, loss: 2.297245\n",
      "Epoch 162, loss: 2.296420\n",
      "Epoch 163, loss: 2.297482\n",
      "Epoch 164, loss: 2.296074\n",
      "Epoch 165, loss: 2.297056\n",
      "Epoch 166, loss: 2.297140\n",
      "Epoch 167, loss: 2.297543\n",
      "Epoch 168, loss: 2.295760\n",
      "Epoch 169, loss: 2.298139\n",
      "Epoch 170, loss: 2.295076\n",
      "Epoch 171, loss: 2.295207\n",
      "Epoch 172, loss: 2.295628\n",
      "Epoch 173, loss: 2.295903\n",
      "Epoch 174, loss: 2.295750\n",
      "Epoch 175, loss: 2.294269\n",
      "Epoch 176, loss: 2.295342\n",
      "Epoch 177, loss: 2.293956\n",
      "Epoch 178, loss: 2.295376\n",
      "Epoch 179, loss: 2.295045\n",
      "Epoch 180, loss: 2.296026\n",
      "Epoch 181, loss: 2.295137\n",
      "Epoch 182, loss: 2.295462\n",
      "Epoch 183, loss: 2.294846\n",
      "Epoch 184, loss: 2.295038\n",
      "Epoch 185, loss: 2.295724\n",
      "Epoch 186, loss: 2.294593\n",
      "Epoch 187, loss: 2.294744\n",
      "Epoch 188, loss: 2.297046\n",
      "Epoch 189, loss: 2.295354\n",
      "Epoch 190, loss: 2.295374\n",
      "Epoch 191, loss: 2.293921\n",
      "Epoch 192, loss: 2.293457\n",
      "Epoch 193, loss: 2.294394\n",
      "Epoch 194, loss: 2.294114\n",
      "Epoch 195, loss: 2.294867\n",
      "Epoch 196, loss: 2.294416\n",
      "Epoch 197, loss: 2.294441\n",
      "Epoch 198, loss: 2.295071\n",
      "Epoch 199, loss: 2.294104\n",
      "Epoch 0, loss: 2.301978\n",
      "Epoch 1, loss: 2.300616\n",
      "Epoch 2, loss: 2.303499\n",
      "Epoch 3, loss: 2.302718\n",
      "Epoch 4, loss: 2.302893\n",
      "Epoch 5, loss: 2.301464\n",
      "Epoch 6, loss: 2.303411\n",
      "Epoch 7, loss: 2.301762\n",
      "Epoch 8, loss: 2.302449\n",
      "Epoch 9, loss: 2.302391\n",
      "Epoch 10, loss: 2.300598\n",
      "Epoch 11, loss: 2.302770\n",
      "Epoch 12, loss: 2.302046\n",
      "Epoch 13, loss: 2.301253\n",
      "Epoch 14, loss: 2.300119\n",
      "Epoch 15, loss: 2.301997\n",
      "Epoch 16, loss: 2.301663\n",
      "Epoch 17, loss: 2.300380\n",
      "Epoch 18, loss: 2.302833\n",
      "Epoch 19, loss: 2.303131\n",
      "Epoch 20, loss: 2.301340\n",
      "Epoch 21, loss: 2.300945\n",
      "Epoch 22, loss: 2.303268\n",
      "Epoch 23, loss: 2.300083\n",
      "Epoch 24, loss: 2.302354\n",
      "Epoch 25, loss: 2.302323\n",
      "Epoch 26, loss: 2.301630\n",
      "Epoch 27, loss: 2.300788\n",
      "Epoch 28, loss: 2.300950\n",
      "Epoch 29, loss: 2.301574\n",
      "Epoch 30, loss: 2.301753\n",
      "Epoch 31, loss: 2.301916\n",
      "Epoch 32, loss: 2.301301\n",
      "Epoch 33, loss: 2.300585\n",
      "Epoch 34, loss: 2.302348\n",
      "Epoch 35, loss: 2.301401\n",
      "Epoch 36, loss: 2.302965\n",
      "Epoch 37, loss: 2.301077\n",
      "Epoch 38, loss: 2.300897\n",
      "Epoch 39, loss: 2.300578\n",
      "Epoch 40, loss: 2.301912\n",
      "Epoch 41, loss: 2.301843\n",
      "Epoch 42, loss: 2.302049\n",
      "Epoch 43, loss: 2.300015\n",
      "Epoch 44, loss: 2.298778\n",
      "Epoch 45, loss: 2.301888\n",
      "Epoch 46, loss: 2.301881\n",
      "Epoch 47, loss: 2.299623\n",
      "Epoch 48, loss: 2.300996\n",
      "Epoch 49, loss: 2.301222\n",
      "Epoch 50, loss: 2.300302\n",
      "Epoch 51, loss: 2.300527\n",
      "Epoch 52, loss: 2.300579\n",
      "Epoch 53, loss: 2.302203\n",
      "Epoch 54, loss: 2.300451\n",
      "Epoch 55, loss: 2.300913\n",
      "Epoch 56, loss: 2.301083\n",
      "Epoch 57, loss: 2.300468\n",
      "Epoch 58, loss: 2.299632\n",
      "Epoch 59, loss: 2.300580\n",
      "Epoch 60, loss: 2.299009\n",
      "Epoch 61, loss: 2.300190\n",
      "Epoch 62, loss: 2.299377\n",
      "Epoch 63, loss: 2.298906\n",
      "Epoch 64, loss: 2.300247\n",
      "Epoch 65, loss: 2.301208\n",
      "Epoch 66, loss: 2.299595\n",
      "Epoch 67, loss: 2.299106\n",
      "Epoch 68, loss: 2.299258\n",
      "Epoch 69, loss: 2.298531\n",
      "Epoch 70, loss: 2.299409\n",
      "Epoch 71, loss: 2.299789\n",
      "Epoch 72, loss: 2.300382\n",
      "Epoch 73, loss: 2.298867\n",
      "Epoch 74, loss: 2.299890\n",
      "Epoch 75, loss: 2.300477\n",
      "Epoch 76, loss: 2.299163\n",
      "Epoch 77, loss: 2.299228\n",
      "Epoch 78, loss: 2.299865\n",
      "Epoch 79, loss: 2.297870\n",
      "Epoch 80, loss: 2.299258\n",
      "Epoch 81, loss: 2.298760\n",
      "Epoch 82, loss: 2.299390\n",
      "Epoch 83, loss: 2.299025\n",
      "Epoch 84, loss: 2.299800\n",
      "Epoch 85, loss: 2.299282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86, loss: 2.298720\n",
      "Epoch 87, loss: 2.299289\n",
      "Epoch 88, loss: 2.298581\n",
      "Epoch 89, loss: 2.299343\n",
      "Epoch 90, loss: 2.299312\n",
      "Epoch 91, loss: 2.300282\n",
      "Epoch 92, loss: 2.298472\n",
      "Epoch 93, loss: 2.299937\n",
      "Epoch 94, loss: 2.299384\n",
      "Epoch 95, loss: 2.297827\n",
      "Epoch 96, loss: 2.297546\n",
      "Epoch 97, loss: 2.297911\n",
      "Epoch 98, loss: 2.299537\n",
      "Epoch 99, loss: 2.298697\n",
      "Epoch 100, loss: 2.299527\n",
      "Epoch 101, loss: 2.298711\n",
      "Epoch 102, loss: 2.297722\n",
      "Epoch 103, loss: 2.297924\n",
      "Epoch 104, loss: 2.298528\n",
      "Epoch 105, loss: 2.297649\n",
      "Epoch 106, loss: 2.298229\n",
      "Epoch 107, loss: 2.298381\n",
      "Epoch 108, loss: 2.299304\n",
      "Epoch 109, loss: 2.299306\n",
      "Epoch 110, loss: 2.298288\n",
      "Epoch 111, loss: 2.297015\n",
      "Epoch 112, loss: 2.297764\n",
      "Epoch 113, loss: 2.297159\n",
      "Epoch 114, loss: 2.296448\n",
      "Epoch 115, loss: 2.297651\n",
      "Epoch 116, loss: 2.299011\n",
      "Epoch 117, loss: 2.298542\n",
      "Epoch 118, loss: 2.298095\n",
      "Epoch 119, loss: 2.298001\n",
      "Epoch 120, loss: 2.297704\n",
      "Epoch 121, loss: 2.296996\n",
      "Epoch 122, loss: 2.297450\n",
      "Epoch 123, loss: 2.298674\n",
      "Epoch 124, loss: 2.296163\n",
      "Epoch 125, loss: 2.297753\n",
      "Epoch 126, loss: 2.298179\n",
      "Epoch 127, loss: 2.296355\n",
      "Epoch 128, loss: 2.297139\n",
      "Epoch 129, loss: 2.298609\n",
      "Epoch 130, loss: 2.296903\n",
      "Epoch 131, loss: 2.298478\n",
      "Epoch 132, loss: 2.296492\n",
      "Epoch 133, loss: 2.296562\n",
      "Epoch 134, loss: 2.296853\n",
      "Epoch 135, loss: 2.297538\n",
      "Epoch 136, loss: 2.297877\n",
      "Epoch 137, loss: 2.295836\n",
      "Epoch 138, loss: 2.296706\n",
      "Epoch 139, loss: 2.294619\n",
      "Epoch 140, loss: 2.295290\n",
      "Epoch 141, loss: 2.296000\n",
      "Epoch 142, loss: 2.297759\n",
      "Epoch 143, loss: 2.297487\n",
      "Epoch 144, loss: 2.297590\n",
      "Epoch 145, loss: 2.296866\n",
      "Epoch 146, loss: 2.298226\n",
      "Epoch 147, loss: 2.295628\n",
      "Epoch 148, loss: 2.297027\n",
      "Epoch 149, loss: 2.297471\n",
      "Epoch 150, loss: 2.298134\n",
      "Epoch 151, loss: 2.294954\n",
      "Epoch 152, loss: 2.295162\n",
      "Epoch 153, loss: 2.295435\n",
      "Epoch 154, loss: 2.296214\n",
      "Epoch 155, loss: 2.297057\n",
      "Epoch 156, loss: 2.294669\n",
      "Epoch 157, loss: 2.295158\n",
      "Epoch 158, loss: 2.294230\n",
      "Epoch 159, loss: 2.299068\n",
      "Epoch 160, loss: 2.295923\n",
      "Epoch 161, loss: 2.297443\n",
      "Epoch 162, loss: 2.295933\n",
      "Epoch 163, loss: 2.294957\n",
      "Epoch 164, loss: 2.294553\n",
      "Epoch 165, loss: 2.294713\n",
      "Epoch 166, loss: 2.294574\n",
      "Epoch 167, loss: 2.296382\n",
      "Epoch 168, loss: 2.295333\n",
      "Epoch 169, loss: 2.295315\n",
      "Epoch 170, loss: 2.295665\n",
      "Epoch 171, loss: 2.295664\n",
      "Epoch 172, loss: 2.295510\n",
      "Epoch 173, loss: 2.295334\n",
      "Epoch 174, loss: 2.296094\n",
      "Epoch 175, loss: 2.296211\n",
      "Epoch 176, loss: 2.294288\n",
      "Epoch 177, loss: 2.294404\n",
      "Epoch 178, loss: 2.294545\n",
      "Epoch 179, loss: 2.296341\n",
      "Epoch 180, loss: 2.296679\n",
      "Epoch 181, loss: 2.296891\n",
      "Epoch 182, loss: 2.296265\n",
      "Epoch 183, loss: 2.294548\n",
      "Epoch 184, loss: 2.294737\n",
      "Epoch 185, loss: 2.293399\n",
      "Epoch 186, loss: 2.297197\n",
      "Epoch 187, loss: 2.294635\n",
      "Epoch 188, loss: 2.294196\n",
      "Epoch 189, loss: 2.293935\n",
      "Epoch 190, loss: 2.294341\n",
      "Epoch 191, loss: 2.295696\n",
      "Epoch 192, loss: 2.294015\n",
      "Epoch 193, loss: 2.293908\n",
      "Epoch 194, loss: 2.294782\n",
      "Epoch 195, loss: 2.294134\n",
      "Epoch 196, loss: 2.292681\n",
      "Epoch 197, loss: 2.294206\n",
      "Epoch 198, loss: 2.296812\n",
      "Epoch 199, loss: 2.296489\n",
      "Epoch 0, loss: 2.301662\n",
      "Epoch 1, loss: 2.301994\n",
      "Epoch 2, loss: 2.302266\n",
      "Epoch 3, loss: 2.302873\n",
      "Epoch 4, loss: 2.302866\n",
      "Epoch 5, loss: 2.303216\n",
      "Epoch 6, loss: 2.301892\n",
      "Epoch 7, loss: 2.302470\n",
      "Epoch 8, loss: 2.304215\n",
      "Epoch 9, loss: 2.303214\n",
      "Epoch 10, loss: 2.302558\n",
      "Epoch 11, loss: 2.302174\n",
      "Epoch 12, loss: 2.301307\n",
      "Epoch 13, loss: 2.303048\n",
      "Epoch 14, loss: 2.302893\n",
      "Epoch 15, loss: 2.303138\n",
      "Epoch 16, loss: 2.302436\n",
      "Epoch 17, loss: 2.303147\n",
      "Epoch 18, loss: 2.303147\n",
      "Epoch 19, loss: 2.303230\n",
      "Epoch 20, loss: 2.303024\n",
      "Epoch 21, loss: 2.303809\n",
      "Epoch 22, loss: 2.303445\n",
      "Epoch 23, loss: 2.302491\n",
      "Epoch 24, loss: 2.302555\n",
      "Epoch 25, loss: 2.302801\n",
      "Epoch 26, loss: 2.301659\n",
      "Epoch 27, loss: 2.301952\n",
      "Epoch 28, loss: 2.302681\n",
      "Epoch 29, loss: 2.302161\n",
      "Epoch 30, loss: 2.303337\n",
      "Epoch 31, loss: 2.303397\n",
      "Epoch 32, loss: 2.302870\n",
      "Epoch 33, loss: 2.302409\n",
      "Epoch 34, loss: 2.302985\n",
      "Epoch 35, loss: 2.302256\n",
      "Epoch 36, loss: 2.302174\n",
      "Epoch 37, loss: 2.301253\n",
      "Epoch 38, loss: 2.302753\n",
      "Epoch 39, loss: 2.302059\n",
      "Epoch 40, loss: 2.301934\n",
      "Epoch 41, loss: 2.302901\n",
      "Epoch 42, loss: 2.302306\n",
      "Epoch 43, loss: 2.301890\n",
      "Epoch 44, loss: 2.302396\n",
      "Epoch 45, loss: 2.303325\n",
      "Epoch 46, loss: 2.302531\n",
      "Epoch 47, loss: 2.302940\n",
      "Epoch 48, loss: 2.302449\n",
      "Epoch 49, loss: 2.303844\n",
      "Epoch 50, loss: 2.301852\n",
      "Epoch 51, loss: 2.301904\n",
      "Epoch 52, loss: 2.302366\n",
      "Epoch 53, loss: 2.302303\n",
      "Epoch 54, loss: 2.302456\n",
      "Epoch 55, loss: 2.302275\n",
      "Epoch 56, loss: 2.303408\n",
      "Epoch 57, loss: 2.302925\n",
      "Epoch 58, loss: 2.303200\n",
      "Epoch 59, loss: 2.301469\n",
      "Epoch 60, loss: 2.302769\n",
      "Epoch 61, loss: 2.303508\n",
      "Epoch 62, loss: 2.303180\n",
      "Epoch 63, loss: 2.301927\n",
      "Epoch 64, loss: 2.302278\n",
      "Epoch 65, loss: 2.302054\n",
      "Epoch 66, loss: 2.301456\n",
      "Epoch 67, loss: 2.301274\n",
      "Epoch 68, loss: 2.303296\n",
      "Epoch 69, loss: 2.301974\n",
      "Epoch 70, loss: 2.302595\n",
      "Epoch 71, loss: 2.302699\n",
      "Epoch 72, loss: 2.302457\n",
      "Epoch 73, loss: 2.302106\n",
      "Epoch 74, loss: 2.301917\n",
      "Epoch 75, loss: 2.302589\n",
      "Epoch 76, loss: 2.303122\n",
      "Epoch 77, loss: 2.302071\n",
      "Epoch 78, loss: 2.300941\n",
      "Epoch 79, loss: 2.302952\n",
      "Epoch 80, loss: 2.301716\n",
      "Epoch 81, loss: 2.302579\n",
      "Epoch 82, loss: 2.302876\n",
      "Epoch 83, loss: 2.302683\n",
      "Epoch 84, loss: 2.301665\n",
      "Epoch 85, loss: 2.303140\n",
      "Epoch 86, loss: 2.302895\n",
      "Epoch 87, loss: 2.301655\n",
      "Epoch 88, loss: 2.302922\n",
      "Epoch 89, loss: 2.302861\n",
      "Epoch 90, loss: 2.301111\n",
      "Epoch 91, loss: 2.303734\n",
      "Epoch 92, loss: 2.302325\n",
      "Epoch 93, loss: 2.302517\n",
      "Epoch 94, loss: 2.302999\n",
      "Epoch 95, loss: 2.302295\n",
      "Epoch 96, loss: 2.303253\n",
      "Epoch 97, loss: 2.302541\n",
      "Epoch 98, loss: 2.300682\n",
      "Epoch 99, loss: 2.302527\n",
      "Epoch 100, loss: 2.301944\n",
      "Epoch 101, loss: 2.302213\n",
      "Epoch 102, loss: 2.301773\n",
      "Epoch 103, loss: 2.302473\n",
      "Epoch 104, loss: 2.302412\n",
      "Epoch 105, loss: 2.301814\n",
      "Epoch 106, loss: 2.302708\n",
      "Epoch 107, loss: 2.302185\n",
      "Epoch 108, loss: 2.301823\n",
      "Epoch 109, loss: 2.301719\n",
      "Epoch 110, loss: 2.302105\n",
      "Epoch 111, loss: 2.302689\n",
      "Epoch 112, loss: 2.301073\n",
      "Epoch 113, loss: 2.302210\n",
      "Epoch 114, loss: 2.303314\n",
      "Epoch 115, loss: 2.302542\n",
      "Epoch 116, loss: 2.302795\n",
      "Epoch 117, loss: 2.303154\n",
      "Epoch 118, loss: 2.301178\n",
      "Epoch 119, loss: 2.302384\n",
      "Epoch 120, loss: 2.302478\n",
      "Epoch 121, loss: 2.301217\n",
      "Epoch 122, loss: 2.301643\n",
      "Epoch 123, loss: 2.302784\n",
      "Epoch 124, loss: 2.302930\n",
      "Epoch 125, loss: 2.302966\n",
      "Epoch 126, loss: 2.301913\n",
      "Epoch 127, loss: 2.302362\n",
      "Epoch 128, loss: 2.302301\n",
      "Epoch 129, loss: 2.302683\n",
      "Epoch 130, loss: 2.302931\n",
      "Epoch 131, loss: 2.301239\n",
      "Epoch 132, loss: 2.302081\n",
      "Epoch 133, loss: 2.301047\n",
      "Epoch 134, loss: 2.303344\n",
      "Epoch 135, loss: 2.302795\n",
      "Epoch 136, loss: 2.302006\n",
      "Epoch 137, loss: 2.301879\n",
      "Epoch 138, loss: 2.303085\n",
      "Epoch 139, loss: 2.301072\n",
      "Epoch 140, loss: 2.301093\n",
      "Epoch 141, loss: 2.301448\n",
      "Epoch 142, loss: 2.302232\n",
      "Epoch 143, loss: 2.301738\n",
      "Epoch 144, loss: 2.302116\n",
      "Epoch 145, loss: 2.300767\n",
      "Epoch 146, loss: 2.302482\n",
      "Epoch 147, loss: 2.302947\n",
      "Epoch 148, loss: 2.302091\n",
      "Epoch 149, loss: 2.303312\n",
      "Epoch 150, loss: 2.302882\n",
      "Epoch 151, loss: 2.302031\n",
      "Epoch 152, loss: 2.302143\n",
      "Epoch 153, loss: 2.302119\n",
      "Epoch 154, loss: 2.301749\n",
      "Epoch 155, loss: 2.302091\n",
      "Epoch 156, loss: 2.301448\n",
      "Epoch 157, loss: 2.301101\n",
      "Epoch 158, loss: 2.301598\n",
      "Epoch 159, loss: 2.302574\n",
      "Epoch 160, loss: 2.301555\n",
      "Epoch 161, loss: 2.302557\n",
      "Epoch 162, loss: 2.301535\n",
      "Epoch 163, loss: 2.302448\n",
      "Epoch 164, loss: 2.301548\n",
      "Epoch 165, loss: 2.301280\n",
      "Epoch 166, loss: 2.301738\n",
      "Epoch 167, loss: 2.302808\n",
      "Epoch 168, loss: 2.302521\n",
      "Epoch 169, loss: 2.302251\n",
      "Epoch 170, loss: 2.301648\n",
      "Epoch 171, loss: 2.301677\n",
      "Epoch 172, loss: 2.302394\n",
      "Epoch 173, loss: 2.302510\n",
      "Epoch 174, loss: 2.302903\n",
      "Epoch 175, loss: 2.302528\n",
      "Epoch 176, loss: 2.301807\n",
      "Epoch 177, loss: 2.301390\n",
      "Epoch 178, loss: 2.301159\n",
      "Epoch 179, loss: 2.302651\n",
      "Epoch 180, loss: 2.302132\n",
      "Epoch 181, loss: 2.300650\n",
      "Epoch 182, loss: 2.301073\n",
      "Epoch 183, loss: 2.302715\n",
      "Epoch 184, loss: 2.302885\n",
      "Epoch 185, loss: 2.302693\n",
      "Epoch 186, loss: 2.301787\n",
      "Epoch 187, loss: 2.301655\n",
      "Epoch 188, loss: 2.303413\n",
      "Epoch 189, loss: 2.302221\n",
      "Epoch 190, loss: 2.302141\n",
      "Epoch 191, loss: 2.302098\n",
      "Epoch 192, loss: 2.301538\n",
      "Epoch 193, loss: 2.302192\n",
      "Epoch 194, loss: 2.301795\n",
      "Epoch 195, loss: 2.301429\n",
      "Epoch 196, loss: 2.301940\n",
      "Epoch 197, loss: 2.301674\n",
      "Epoch 198, loss: 2.301219\n",
      "Epoch 199, loss: 2.302222\n",
      "Epoch 0, loss: 2.303792\n",
      "Epoch 1, loss: 2.303633\n",
      "Epoch 2, loss: 2.302423\n",
      "Epoch 3, loss: 2.303312\n",
      "Epoch 4, loss: 2.302457\n",
      "Epoch 5, loss: 2.303352\n",
      "Epoch 6, loss: 2.302500\n",
      "Epoch 7, loss: 2.302002\n",
      "Epoch 8, loss: 2.303416\n",
      "Epoch 9, loss: 2.303363\n",
      "Epoch 10, loss: 2.302799\n",
      "Epoch 11, loss: 2.302623\n",
      "Epoch 12, loss: 2.302490\n",
      "Epoch 13, loss: 2.303284\n",
      "Epoch 14, loss: 2.302755\n",
      "Epoch 15, loss: 2.302420\n",
      "Epoch 16, loss: 2.302810\n",
      "Epoch 17, loss: 2.302361\n",
      "Epoch 18, loss: 2.302716\n",
      "Epoch 19, loss: 2.303963\n",
      "Epoch 20, loss: 2.301916\n",
      "Epoch 21, loss: 2.302787\n",
      "Epoch 22, loss: 2.302389\n",
      "Epoch 23, loss: 2.303404\n",
      "Epoch 24, loss: 2.302030\n",
      "Epoch 25, loss: 2.302687\n",
      "Epoch 26, loss: 2.302777\n",
      "Epoch 27, loss: 2.302927\n",
      "Epoch 28, loss: 2.302134\n",
      "Epoch 29, loss: 2.302968\n",
      "Epoch 30, loss: 2.302658\n",
      "Epoch 31, loss: 2.302699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, loss: 2.302018\n",
      "Epoch 33, loss: 2.303014\n",
      "Epoch 34, loss: 2.303228\n",
      "Epoch 35, loss: 2.302390\n",
      "Epoch 36, loss: 2.302762\n",
      "Epoch 37, loss: 2.302872\n",
      "Epoch 38, loss: 2.302847\n",
      "Epoch 39, loss: 2.303434\n",
      "Epoch 40, loss: 2.302528\n",
      "Epoch 41, loss: 2.302568\n",
      "Epoch 42, loss: 2.303155\n",
      "Epoch 43, loss: 2.302681\n",
      "Epoch 44, loss: 2.302129\n",
      "Epoch 45, loss: 2.302365\n",
      "Epoch 46, loss: 2.302825\n",
      "Epoch 47, loss: 2.302104\n",
      "Epoch 48, loss: 2.302720\n",
      "Epoch 49, loss: 2.302101\n",
      "Epoch 50, loss: 2.302033\n",
      "Epoch 51, loss: 2.302662\n",
      "Epoch 52, loss: 2.302787\n",
      "Epoch 53, loss: 2.302824\n",
      "Epoch 54, loss: 2.302816\n",
      "Epoch 55, loss: 2.302396\n",
      "Epoch 56, loss: 2.302925\n",
      "Epoch 57, loss: 2.302745\n",
      "Epoch 58, loss: 2.302798\n",
      "Epoch 59, loss: 2.302387\n",
      "Epoch 60, loss: 2.303223\n",
      "Epoch 61, loss: 2.303310\n",
      "Epoch 62, loss: 2.303585\n",
      "Epoch 63, loss: 2.302210\n",
      "Epoch 64, loss: 2.302231\n",
      "Epoch 65, loss: 2.302306\n",
      "Epoch 66, loss: 2.302373\n",
      "Epoch 67, loss: 2.302366\n",
      "Epoch 68, loss: 2.301889\n",
      "Epoch 69, loss: 2.302779\n",
      "Epoch 70, loss: 2.303142\n",
      "Epoch 71, loss: 2.301866\n",
      "Epoch 72, loss: 2.302892\n",
      "Epoch 73, loss: 2.303099\n",
      "Epoch 74, loss: 2.303137\n",
      "Epoch 75, loss: 2.302295\n",
      "Epoch 76, loss: 2.302804\n",
      "Epoch 77, loss: 2.302339\n",
      "Epoch 78, loss: 2.302740\n",
      "Epoch 79, loss: 2.302794\n",
      "Epoch 80, loss: 2.301961\n",
      "Epoch 81, loss: 2.302415\n",
      "Epoch 82, loss: 2.302309\n",
      "Epoch 83, loss: 2.302101\n",
      "Epoch 84, loss: 2.302518\n",
      "Epoch 85, loss: 2.301905\n",
      "Epoch 86, loss: 2.302641\n",
      "Epoch 87, loss: 2.302063\n",
      "Epoch 88, loss: 2.302255\n",
      "Epoch 89, loss: 2.302271\n",
      "Epoch 90, loss: 2.302250\n",
      "Epoch 91, loss: 2.302862\n",
      "Epoch 92, loss: 2.303500\n",
      "Epoch 93, loss: 2.302661\n",
      "Epoch 94, loss: 2.303115\n",
      "Epoch 95, loss: 2.301264\n",
      "Epoch 96, loss: 2.302606\n",
      "Epoch 97, loss: 2.301843\n",
      "Epoch 98, loss: 2.302462\n",
      "Epoch 99, loss: 2.302979\n",
      "Epoch 100, loss: 2.303202\n",
      "Epoch 101, loss: 2.302255\n",
      "Epoch 102, loss: 2.303218\n",
      "Epoch 103, loss: 2.302172\n",
      "Epoch 104, loss: 2.302824\n",
      "Epoch 105, loss: 2.302692\n",
      "Epoch 106, loss: 2.302984\n",
      "Epoch 107, loss: 2.302467\n",
      "Epoch 108, loss: 2.302267\n",
      "Epoch 109, loss: 2.302459\n",
      "Epoch 110, loss: 2.302638\n",
      "Epoch 111, loss: 2.303004\n",
      "Epoch 112, loss: 2.302169\n",
      "Epoch 113, loss: 2.302387\n",
      "Epoch 114, loss: 2.302461\n",
      "Epoch 115, loss: 2.302167\n",
      "Epoch 116, loss: 2.302204\n",
      "Epoch 117, loss: 2.302791\n",
      "Epoch 118, loss: 2.302276\n",
      "Epoch 119, loss: 2.302176\n",
      "Epoch 120, loss: 2.302619\n",
      "Epoch 121, loss: 2.302848\n",
      "Epoch 122, loss: 2.302098\n",
      "Epoch 123, loss: 2.302201\n",
      "Epoch 124, loss: 2.302761\n",
      "Epoch 125, loss: 2.302613\n",
      "Epoch 126, loss: 2.303098\n",
      "Epoch 127, loss: 2.302332\n",
      "Epoch 128, loss: 2.302109\n",
      "Epoch 129, loss: 2.302450\n",
      "Epoch 130, loss: 2.303117\n",
      "Epoch 131, loss: 2.302613\n",
      "Epoch 132, loss: 2.302239\n",
      "Epoch 133, loss: 2.301784\n",
      "Epoch 134, loss: 2.301735\n",
      "Epoch 135, loss: 2.302380\n",
      "Epoch 136, loss: 2.301866\n",
      "Epoch 137, loss: 2.302701\n",
      "Epoch 138, loss: 2.302503\n",
      "Epoch 139, loss: 2.301956\n",
      "Epoch 140, loss: 2.302664\n",
      "Epoch 141, loss: 2.302634\n",
      "Epoch 142, loss: 2.303069\n",
      "Epoch 143, loss: 2.303468\n",
      "Epoch 144, loss: 2.302009\n",
      "Epoch 145, loss: 2.302231\n",
      "Epoch 146, loss: 2.301415\n",
      "Epoch 147, loss: 2.301946\n",
      "Epoch 148, loss: 2.301827\n",
      "Epoch 149, loss: 2.301419\n",
      "Epoch 150, loss: 2.302020\n",
      "Epoch 151, loss: 2.301303\n",
      "Epoch 152, loss: 2.302377\n",
      "Epoch 153, loss: 2.301445\n",
      "Epoch 154, loss: 2.302111\n",
      "Epoch 155, loss: 2.302003\n",
      "Epoch 156, loss: 2.301222\n",
      "Epoch 157, loss: 2.302022\n",
      "Epoch 158, loss: 2.302096\n",
      "Epoch 159, loss: 2.302113\n",
      "Epoch 160, loss: 2.301996\n",
      "Epoch 161, loss: 2.301726\n",
      "Epoch 162, loss: 2.301288\n",
      "Epoch 163, loss: 2.302274\n",
      "Epoch 164, loss: 2.301864\n",
      "Epoch 165, loss: 2.301690\n",
      "Epoch 166, loss: 2.301671\n",
      "Epoch 167, loss: 2.302690\n",
      "Epoch 168, loss: 2.302380\n",
      "Epoch 169, loss: 2.302523\n",
      "Epoch 170, loss: 2.302535\n",
      "Epoch 171, loss: 2.301943\n",
      "Epoch 172, loss: 2.302159\n",
      "Epoch 173, loss: 2.302609\n",
      "Epoch 174, loss: 2.301760\n",
      "Epoch 175, loss: 2.302427\n",
      "Epoch 176, loss: 2.302262\n",
      "Epoch 177, loss: 2.301753\n",
      "Epoch 178, loss: 2.301701\n",
      "Epoch 179, loss: 2.302066\n",
      "Epoch 180, loss: 2.301845\n",
      "Epoch 181, loss: 2.301385\n",
      "Epoch 182, loss: 2.302241\n",
      "Epoch 183, loss: 2.302581\n",
      "Epoch 184, loss: 2.302083\n",
      "Epoch 185, loss: 2.303008\n",
      "Epoch 186, loss: 2.302861\n",
      "Epoch 187, loss: 2.302248\n",
      "Epoch 188, loss: 2.302173\n",
      "Epoch 189, loss: 2.302295\n",
      "Epoch 190, loss: 2.300923\n",
      "Epoch 191, loss: 2.301417\n",
      "Epoch 192, loss: 2.302080\n",
      "Epoch 193, loss: 2.301644\n",
      "Epoch 194, loss: 2.301996\n",
      "Epoch 195, loss: 2.301837\n",
      "Epoch 196, loss: 2.302116\n",
      "Epoch 197, loss: 2.302074\n",
      "Epoch 198, loss: 2.301927\n",
      "Epoch 199, loss: 2.301585\n",
      "Epoch 0, loss: 2.303356\n",
      "Epoch 1, loss: 2.303316\n",
      "Epoch 2, loss: 2.300990\n",
      "Epoch 3, loss: 2.302099\n",
      "Epoch 4, loss: 2.302800\n",
      "Epoch 5, loss: 2.302171\n",
      "Epoch 6, loss: 2.302447\n",
      "Epoch 7, loss: 2.302203\n",
      "Epoch 8, loss: 2.301978\n",
      "Epoch 9, loss: 2.301280\n",
      "Epoch 10, loss: 2.303173\n",
      "Epoch 11, loss: 2.302725\n",
      "Epoch 12, loss: 2.303256\n",
      "Epoch 13, loss: 2.302189\n",
      "Epoch 14, loss: 2.302321\n",
      "Epoch 15, loss: 2.302538\n",
      "Epoch 16, loss: 2.302259\n",
      "Epoch 17, loss: 2.302499\n",
      "Epoch 18, loss: 2.301726\n",
      "Epoch 19, loss: 2.302165\n",
      "Epoch 20, loss: 2.301551\n",
      "Epoch 21, loss: 2.301982\n",
      "Epoch 22, loss: 2.303003\n",
      "Epoch 23, loss: 2.302891\n",
      "Epoch 24, loss: 2.302106\n",
      "Epoch 25, loss: 2.301753\n",
      "Epoch 26, loss: 2.302241\n",
      "Epoch 27, loss: 2.302381\n",
      "Epoch 28, loss: 2.303153\n",
      "Epoch 29, loss: 2.302033\n",
      "Epoch 30, loss: 2.302628\n",
      "Epoch 31, loss: 2.302191\n",
      "Epoch 32, loss: 2.302669\n",
      "Epoch 33, loss: 2.301755\n",
      "Epoch 34, loss: 2.302243\n",
      "Epoch 35, loss: 2.301505\n",
      "Epoch 36, loss: 2.302506\n",
      "Epoch 37, loss: 2.302668\n",
      "Epoch 38, loss: 2.302693\n",
      "Epoch 39, loss: 2.302491\n",
      "Epoch 40, loss: 2.301746\n",
      "Epoch 41, loss: 2.301529\n",
      "Epoch 42, loss: 2.302690\n",
      "Epoch 43, loss: 2.303408\n",
      "Epoch 44, loss: 2.301479\n",
      "Epoch 45, loss: 2.301756\n",
      "Epoch 46, loss: 2.302140\n",
      "Epoch 47, loss: 2.302410\n",
      "Epoch 48, loss: 2.302808\n",
      "Epoch 49, loss: 2.302536\n",
      "Epoch 50, loss: 2.302130\n",
      "Epoch 51, loss: 2.301920\n",
      "Epoch 52, loss: 2.301038\n",
      "Epoch 53, loss: 2.302656\n",
      "Epoch 54, loss: 2.302908\n",
      "Epoch 55, loss: 2.301691\n",
      "Epoch 56, loss: 2.301080\n",
      "Epoch 57, loss: 2.302116\n",
      "Epoch 58, loss: 2.301776\n",
      "Epoch 59, loss: 2.302143\n",
      "Epoch 60, loss: 2.302095\n",
      "Epoch 61, loss: 2.302847\n",
      "Epoch 62, loss: 2.302567\n",
      "Epoch 63, loss: 2.302392\n",
      "Epoch 64, loss: 2.302216\n",
      "Epoch 65, loss: 2.301446\n",
      "Epoch 66, loss: 2.303594\n",
      "Epoch 67, loss: 2.302710\n",
      "Epoch 68, loss: 2.302381\n",
      "Epoch 69, loss: 2.302201\n",
      "Epoch 70, loss: 2.301566\n",
      "Epoch 71, loss: 2.301851\n",
      "Epoch 72, loss: 2.301620\n",
      "Epoch 73, loss: 2.302515\n",
      "Epoch 74, loss: 2.301702\n",
      "Epoch 75, loss: 2.301761\n",
      "Epoch 76, loss: 2.303142\n",
      "Epoch 77, loss: 2.302472\n",
      "Epoch 78, loss: 2.302199\n",
      "Epoch 79, loss: 2.301593\n",
      "Epoch 80, loss: 2.303295\n",
      "Epoch 81, loss: 2.302094\n",
      "Epoch 82, loss: 2.302611\n",
      "Epoch 83, loss: 2.301819\n",
      "Epoch 84, loss: 2.302615\n",
      "Epoch 85, loss: 2.302552\n",
      "Epoch 86, loss: 2.302565\n",
      "Epoch 87, loss: 2.302223\n",
      "Epoch 88, loss: 2.302073\n",
      "Epoch 89, loss: 2.301656\n",
      "Epoch 90, loss: 2.301970\n",
      "Epoch 91, loss: 2.302470\n",
      "Epoch 92, loss: 2.302128\n",
      "Epoch 93, loss: 2.301618\n",
      "Epoch 94, loss: 2.301373\n",
      "Epoch 95, loss: 2.301637\n",
      "Epoch 96, loss: 2.303056\n",
      "Epoch 97, loss: 2.302578\n",
      "Epoch 98, loss: 2.302146\n",
      "Epoch 99, loss: 2.302368\n",
      "Epoch 100, loss: 2.301991\n",
      "Epoch 101, loss: 2.302288\n",
      "Epoch 102, loss: 2.302065\n",
      "Epoch 103, loss: 2.300766\n",
      "Epoch 104, loss: 2.300912\n",
      "Epoch 105, loss: 2.302502\n",
      "Epoch 106, loss: 2.301636\n",
      "Epoch 107, loss: 2.302565\n",
      "Epoch 108, loss: 2.301116\n",
      "Epoch 109, loss: 2.302115\n",
      "Epoch 110, loss: 2.301839\n",
      "Epoch 111, loss: 2.301572\n",
      "Epoch 112, loss: 2.303084\n",
      "Epoch 113, loss: 2.301867\n",
      "Epoch 114, loss: 2.301252\n",
      "Epoch 115, loss: 2.302525\n",
      "Epoch 116, loss: 2.301300\n",
      "Epoch 117, loss: 2.301522\n",
      "Epoch 118, loss: 2.302723\n",
      "Epoch 119, loss: 2.302187\n",
      "Epoch 120, loss: 2.302658\n",
      "Epoch 121, loss: 2.301774\n",
      "Epoch 122, loss: 2.301477\n",
      "Epoch 123, loss: 2.302405\n",
      "Epoch 124, loss: 2.301474\n",
      "Epoch 125, loss: 2.301788\n",
      "Epoch 126, loss: 2.301757\n",
      "Epoch 127, loss: 2.302538\n",
      "Epoch 128, loss: 2.302625\n",
      "Epoch 129, loss: 2.301916\n",
      "Epoch 130, loss: 2.301737\n",
      "Epoch 131, loss: 2.301924\n",
      "Epoch 132, loss: 2.302348\n",
      "Epoch 133, loss: 2.301002\n",
      "Epoch 134, loss: 2.302908\n",
      "Epoch 135, loss: 2.302562\n",
      "Epoch 136, loss: 2.303011\n",
      "Epoch 137, loss: 2.302230\n",
      "Epoch 138, loss: 2.302146\n",
      "Epoch 139, loss: 2.301302\n",
      "Epoch 140, loss: 2.301159\n",
      "Epoch 141, loss: 2.302330\n",
      "Epoch 142, loss: 2.302424\n",
      "Epoch 143, loss: 2.301547\n",
      "Epoch 144, loss: 2.302811\n",
      "Epoch 145, loss: 2.301672\n",
      "Epoch 146, loss: 2.301880\n",
      "Epoch 147, loss: 2.302873\n",
      "Epoch 148, loss: 2.302238\n",
      "Epoch 149, loss: 2.301488\n",
      "Epoch 150, loss: 2.301954\n",
      "Epoch 151, loss: 2.302765\n",
      "Epoch 152, loss: 2.302077\n",
      "Epoch 153, loss: 2.302143\n",
      "Epoch 154, loss: 2.302510\n",
      "Epoch 155, loss: 2.302351\n",
      "Epoch 156, loss: 2.302309\n",
      "Epoch 157, loss: 2.302284\n",
      "Epoch 158, loss: 2.302447\n",
      "Epoch 159, loss: 2.301466\n",
      "Epoch 160, loss: 2.303082\n",
      "Epoch 161, loss: 2.302682\n",
      "Epoch 162, loss: 2.301121\n",
      "Epoch 163, loss: 2.301549\n",
      "Epoch 164, loss: 2.301765\n",
      "Epoch 165, loss: 2.301589\n",
      "Epoch 166, loss: 2.302442\n",
      "Epoch 167, loss: 2.302560\n",
      "Epoch 168, loss: 2.302750\n",
      "Epoch 169, loss: 2.301661\n",
      "Epoch 170, loss: 2.303327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171, loss: 2.302339\n",
      "Epoch 172, loss: 2.302226\n",
      "Epoch 173, loss: 2.300752\n",
      "Epoch 174, loss: 2.301516\n",
      "Epoch 175, loss: 2.301670\n",
      "Epoch 176, loss: 2.301001\n",
      "Epoch 177, loss: 2.301464\n",
      "Epoch 178, loss: 2.301498\n",
      "Epoch 179, loss: 2.302499\n",
      "Epoch 180, loss: 2.301390\n",
      "Epoch 181, loss: 2.301651\n",
      "Epoch 182, loss: 2.301881\n",
      "Epoch 183, loss: 2.301675\n",
      "Epoch 184, loss: 2.302780\n",
      "Epoch 185, loss: 2.302471\n",
      "Epoch 186, loss: 2.302040\n",
      "Epoch 187, loss: 2.302347\n",
      "Epoch 188, loss: 2.301977\n",
      "Epoch 189, loss: 2.302104\n",
      "Epoch 190, loss: 2.301422\n",
      "Epoch 191, loss: 2.301339\n",
      "Epoch 192, loss: 2.300849\n",
      "Epoch 193, loss: 2.301582\n",
      "Epoch 194, loss: 2.302007\n",
      "Epoch 195, loss: 2.301625\n",
      "Epoch 196, loss: 2.301553\n",
      "Epoch 197, loss: 2.301779\n",
      "Epoch 198, loss: 2.301531\n",
      "Epoch 199, loss: 2.301584\n",
      "Best accuracy is 0.19555555555555557 for Linear Classifier with Learning Rate 0.01 and Reg Strength 0.001\n",
      "Linear softmax classifier validation set accuracy: 0.206\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "learning_rates = [3e-2, 1e-2, 1e-3, 1e-4]\n",
    "reg_strengths = [1e-3, 1e-4, 1e-5]\n",
    "\n",
    "# dict for saving accuracy results\n",
    "accuracy_dict = dict()\n",
    "\n",
    "# split array on train and test arrays\n",
    "np.random.seed(17)\n",
    "num_train = train_X.shape[0]\n",
    "shuffled_indices = np.arange(num_train)\n",
    "np.random.shuffle(shuffled_indices)\n",
    "split = int(num_train*0.7)\n",
    "train_indices = shuffled_indices[:split]\n",
    "test_indices = shuffled_indices[split:]\n",
    "\n",
    "for i in list(itertools.product(learning_rates, reg_strengths)):\n",
    "    acc_list = list()\n",
    "    # initialize Linear Classifier\n",
    "    classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "    classifier.fit(train_X[train_indices], train_y[train_indices], epochs=num_epochs, learning_rate=i[0], \n",
    "                   batch_size=batch_size, reg=i[1])\n",
    "    # make a prediction\n",
    "    pred = classifier.predict(train_X[test_indices])\n",
    "    accuracy = multiclass_accuracy(pred, train_y[test_indices])\n",
    "    accuracy_dict[i] = (accuracy, classifier, i[0], i[1])\n",
    "\n",
    "sort_accuracy_dict = {k: v for k, v in sorted(accuracy_dict.items(), key=lambda item: item[1][0], reverse=True)}\n",
    "best_accuracy = list(sort_accuracy_dict.values())[0][0]\n",
    "best_classifier = list(sort_accuracy_dict.values())[0][1]\n",
    "best_learning_rate = list(sort_accuracy_dict.values())[0][2]\n",
    "best_reg_strength = list(sort_accuracy_dict.values())[0][3]\n",
    "\n",
    "print(f'Best accuracy is {best_accuracy} for Linear Classifier with Learning Rate {best_learning_rate} and ' \\\n",
    "      f'Reg Strength {best_reg_strength}')  \n",
    "\n",
    "test_pred = best_classifier.predict(val_X)\n",
    "validation_accuracy = multiclass_accuracy(test_pred, val_y)\n",
    "print(f'Linear softmax classifier validation set accuracy: {validation_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.182\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print(f'Linear softmax classifier test set accuracy: {test_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
